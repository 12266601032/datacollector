
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Spark Evaluator" /><meta name="abstract" content="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. Use the Spark Evaluator processor in standalone pipelines only." /><meta name="description" content="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. Use the Spark Evaluator processor in standalone pipelines only." /><meta name="DC.Relation" scheme="URI" content="../Processors/Processors_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_cpx_1lm_zx" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Spark Evaluator</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../Processors/Processors_title.html" title="Processors">Processors</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../Processors/Processors_title.html" title="Processors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Processors</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_cpx_1lm_zx">
 <h1 class="title topictitle1">Spark Evaluator</h1>

 
 <div class="body conbody"><p class="shortdesc">The Spark Evaluator performs custom processing within a pipeline based on a Spark
        application that you develop. Use the Spark Evaluator processor in standalone pipelines
        only. </p>

  <p class="p">The Spark Evaluator starts a local Spark
            application. The Spark application - or <samp class="ph codeph">SparkContext</samp> - runs as long as
            the pipeline runs. The Spark application submits a job for each batch of records,
            processing the batch and then returning the results and errors back to the pipeline for
            further processing. </p>

        <p class="p">You can configure a parallelism value for the Spark Evaluator. The Spark application
            creates this number of partitions for each batch of records. Each partition is processed
            in parallel. Set the parallelism value based on the number of available processors on
            the <span class="ph">Data
                  Collector</span> machine. Because the Spark Evaluator can use multiple threads to process a batch, the
            processor is especially useful when you need to perform heavy custom processing within a
            pipeline, such as image classification.</p>

        <div class="note note"><span class="notetitle">Note:</span> The stage libraries that include the Spark Evaluator include all dependencies required
            for the processor. The Spark Evaluator does not require any installation
            prerequisites.</div>

        <div class="p">Complete the following tasks to use the Spark Evaluator:<ol class="ol" id="concept_cpx_1lm_zx__ol_pmh_33p_zx">
                <li class="li">Write the Spark application using Java or Scala. Then, package a JAR file
                    containing the application.</li>

                <li class="li">Install the application and its dependencies.</li>

                <li class="li">Configure the Spark Evaluator processor to submit the Spark application.<p class="p">When
                        you configure the processor, you define the parallelism value to use during
                        processing. You also define the name of the custom Spark class that you
                        developed and define the arguments to pass to the <samp class="ph codeph">init</samp>
                        method in the custom Spark class.</p>
</li>

            </ol>
</div>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_lfl_dvd_1y">
 <h2 class="title topictitle2">Developing the Spark Application</h2>

 <div class="body conbody">
  <p class="p">To develop a custom Spark
            application, you write the Spark application and then package a JAR file containing the
            application.</p>

        <p class="p">Use Java or Scala to write a custom Spark class that implements the StreamSets
            SparkTransformer API: <a class="xref" href="https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api" target="_blank">https://github.com/streamsets/datacollector-plugin-api/tree/master/streamsets-datacollector-spark-api</a>.</p>

        <p class="p">You can include the following methods in the custom class:</p>

        <dl class="dl">
            
                <dt class="dt dlterm">init</dt>

                <dd class="dd">Optional. The <samp class="ph codeph">init</samp> method is called once when the pipeline
                    starts to read arguments that you configure in the Spark Evaluator processor.
                    Use the <samp class="ph codeph">init</samp> method to make connections to external systems or
                    to read configuration details or existing data from external systems.</dd>

            
            
                <dt class="dt dlterm">transform</dt>

                <dd class="dd">Required. The <samp class="ph codeph">transform</samp> method is called for each batch of
                    records that the pipeline processes. <span class="ph">Data
                  Collector</span> passes a batch of data to the <samp class="ph codeph">transform</samp> method as a
                    Resilient Distributed Dataset (RDD). The method processes the data according to
                    the custom code.</dd>

            
            
                <dt class="dt dlterm">destroy</dt>

                <dd class="dd">Optional. If you include an <samp class="ph codeph">init</samp> method that makes connections
                    to external systems, you should also include the <samp class="ph codeph">destroy</samp> method
                    to close the connections. The <samp class="ph codeph">destroy</samp> method is called when the
                    pipeline stops.</dd>

            
        </dl>

        <p class="p">When you finish writing the custom Spark class, package a JAR file containing the Spark
            application. Compile against the same stage library version that you use for the Spark
            Evaluator processor. For example, if you are using the Spark Evaluator processor
            included in the stage library for the Cloudera CDH version 5.9 distribution of Hadoop,
            build the application against Spark integrated into Cloudera CDH version 5.9.</p>

        <p class="p">If you use Java, you must build the application so that it is compatible with Java 7. If
            you use Scala, you must build the application so that it is compatible with Scala
            2.10.</p>

 </div>

</div>
<div class="topic task nested1" id="task_dr2_gvd_1y">
    <h2 class="title topictitle2">Installing the Application</h2>

    
    <div class="body taskbody"><p class="shortdesc">Install the Spark application JAR file as an additional library for <span class="ph">Data
                  Collector</span>. If
        your custom Spark application depends on additional libraries other than the
        streamsets-datacollector-api, streamsets-datacollector-spark-api, and spark-core libraries,
        install those libraries in the same location as well. </p>

        <div class="section context">
            <div class="note tip"><span class="tiptitle">Tip:</span> To include all dependent libraries used in your custom Spark
                application, you can use the Apache Maven Dependency Plugin. For more information
                about the Dependency Plugin, see <a class="xref" href="http://maven.apache.org/plugins/maven-dependency-plugin/" target="_blank">http://maven.apache.org/plugins/maven-dependency-plugin/</a>.</div>

        </div>

        <ol class="ol steps"><li class="li step stepexpand">
                <span class="ph cmd">Create a local directory external to the <span class="ph">Data
                  Collector</span> installation directory for the Spark application JAR files. Use an external
                    directory to enable use of the Spark applications after <span class="ph">Data
                  Collector</span> upgrades.</span>
                <div class="itemgroup info">For example, if you installed <span class="ph">Data
                  Collector</span> in the following directory:<pre class="pre codeblock">/opt/sdc/</pre>
<div class="p">you might
                        create the external directory at:
                    <pre class="pre codeblock">/opt/sdc-extras</pre>
</div>
</div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">Create a subdirectory for the stage library that you are using for the Spark
                    Evaluator processor, as follows:</span>
                <div class="itemgroup info">
                    <pre class="pre codeblock">/opt/sdc-extras/&lt;stage library name&gt;/lib/</pre>

                    <div class="p">For example, if you are using the Spark Evaluator processor included in the
                        stage library for the Cloudera CDH version 5.9 distribution of Hadoop,
                        create the following
                        subdirectory:<pre class="pre codeblock">/opt/sdc-extras/streamsets-datacollector-cdh_5_9-lib/lib/</pre>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">Copy the Spark application JAR file and any dependent libraries to the
                    subdirectory.</span>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">In the <span class="ph">Data
                  Collector</span> environment configuration file, <samp class="ph codeph">sdc-env.sh</samp> or
                        <samp class="ph codeph">sdcd-env.sh</samp> located in the
                        <samp class="ph codeph">$SDC_DIST/libexec</samp> directory, add the
                    STREAMSETS_LIBRARIES_EXTRA_DIR environment variable and point it to the external
                    directory, as follows:</span>
                <div class="itemgroup info">
                    <pre class="pre codeblock">export STREAMSETS_LIBRARIES_EXTRA_DIR="&lt;external directory&gt;"</pre>

                    <p class="p">For example: </p>

                    <pre class="pre codeblock">export STREAMSETS_LIBRARIES_EXTRA_DIR="/opt/sdc-extras/"</pre>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">When using the Java Security Manager, which is enabled by default, update the
                        <span class="ph">Data
                  Collector</span> security policy to include the external directory as follows:</span>
                <ol type="a" class="ol substeps" id="task_dr2_gvd_1y__d35045e7622">
                    <li class="li substep substepexpand">
                        <span class="ph cmd">In the <span class="ph">Data
                  Collector</span> configuration directory, open the security policy file,
                                <samp class="ph codeph">$SDC_CONF/sdc-security.policy</samp>.</span>
                    </li>

                    <li class="li substep substepexpand">
                        <span class="ph cmd">Add the following lines to the file:</span>
                        <div class="itemgroup info"><pre class="pre codeblock">// user-defined external directory
grant codebase "file://&lt;external directory&gt;-" {
  permission java.security.AllPermission;
};</pre>
For
                            example:
                            <pre class="pre codeblock">// user-defined external directory
grant codebase "file:///opt/sdc-extras/-" {
  permission java.security.AllPermission;
};</pre>
</div>
                    </li>

                </ol>

            </li>
<li class="li step stepexpand">
                <span class="ph cmd">Restart  <span class="ph">Data
                  Collector</span> to enable the changes.</span>
            </li>
</ol>

    </div>

    <div class="related-links"><div class="relinfo relconcepts"><strong>Related concepts</strong><br />
<div class="related_link"><a class="navheader_parent_path" href="../Install_Config/DCEnvironmentConfig.html#concept_rng_qym_qr" title="You can edit the Data Collector environment configuration file to modify the directories used to store configuration, data, log, and resource files.When you run Data Collector as a service, you must create a system user and group named sdc, or you must edit the values of the SDC_USER and SDC_GROUP environment variables to point to an existing system user or group.You can define the Data Collector Java heap size. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. To enable remote debugging, define debugging options in the SDC_JAVA_OPTS environment variable in the Data Collector environment configuration file.You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.When you use Data Collector with Java 7, Data Collector is configured to use TLS versions 1.1 and 1.2. To connect to a system that uses an earlier version of TLS, modify the Dhttps.protocols option in the SDC_JAVA7_OPTS environment variable in the Data Collector environment configuration file.When you use Data Collector with Java 7, you can define the Java Permanent Generation size, also known as the PermGen size.Data Collector includes a Java Security Manager that is enabled by default. You can edit the Data Collector environment configuration file to configure the path to JAR files to be added to the Data Collector root classloader.">Data Collector Environment Configuration</a></div>
</div>
</div>
    
</div>
<div class="topic task nested1" id="task_g1p_gqn_zx">
    <h2 class="title topictitle2">Configuring a Spark Evaluator</h2>

    <div class="body taskbody">
        <div class="section context">
            <p class="p">Configure a Spark Evaluator
                to process data based on a custom Spark application.</p>

        </div>

        <ol class="ol steps"><li class="li step stepexpand">
                <span class="ph cmd">In the Properties panel, on the <span class="keyword wintitle">General</span> tab, configure the
                    following properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_g1p_gqn_zx__d35045e3900" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d249935e419">General Property</th>

                                    <th class="entry" valign="top" width="70%" id="d249935e422">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Stage name.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">Description</td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Optional description.</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">Stage Library</td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Library version that you want to use. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">Required Fields <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_dnj_bkm_vq" title="A required field is a field that must exist in a record to allow it into the stage for processing. When a record does not include a required field, the record is diverted to the pipeline for error handling. You can define required fields for any processor and most destination stages.">
                                        <img class="image" id="task_g1p_gqn_zx__d35045e3955" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Fields that must include data to be passed into the
                                        stage. <div class="note tip"><span class="tiptitle">Tip:</span> You might include
                                            fields that the stage uses.</div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">Preconditions <a class="xref" href="../Pipeline_Design/DroppingUnwantedRecords.html#concept_msl_yd4_fs" title="Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages.">
                                        <img class="image" id="task_g1p_gqn_zx__d35045e3969" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                        <span class="ph uicontrol">Add</span> to create additional
                                        preconditions. </td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e419 ">On Record Error <a class="xref" href="../Pipeline_Design/ErrorHandling.html#concept_atr_j4y_5r" title="Most stages include error record handling options. When an error occurs when processing a record, Data Collector processes records based on the On Record Error property for the stage.">
                                        <img class="image" id="task_g1p_gqn_zx__d35045e3985" src="../Reusable_Content/../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e422 ">Error record handling for the stage: <ul class="ul" id="task_g1p_gqn_zx__d35045e3989">
                                        <li class="li">Discard - Discards the record.</li>

                                        <li class="li">Send to Error - Sends the record to the pipeline for
                                            error handling.</li>

                                        <li class="li">Stop Pipeline - Stops the pipeline. Not valid for
                                            cluster pipelines.</li>

                                    </ul>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
<li class="li step stepexpand">
                <span class="ph cmd">On the <span class="ph uicontrol">Spark</span> tab, configure the following
                    properties:</span>
                <div class="itemgroup info">
                    
<div class="tablenoborder"><table cellpadding="4" cellspacing="0" summary="" id="task_g1p_gqn_zx__table_ufq_xf4_zx" class="table" frame="border" border="1" rules="all">
                            
                            
                            <thead class="thead" align="left">
                                <tr>
                                    <th class="entry" valign="top" width="30%" id="d249935e551">Spark Property</th>

                                    <th class="entry" valign="top" width="70%" id="d249935e554">Description</th>

                                </tr>

                            </thead>

                            <tbody class="tbody">
                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e551 ">Parallelism</td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e554 ">Number of partitions to create per batch of records. For
                                        example, if set to 4, then the Spark application
                                        simultaneously runs 4 parallel jobs to process the batch.
                                            <p class="p">Set the value based on the number of available
                                            processors on the <span class="ph">Data
                  Collector</span> machine.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e551 ">Application Name</td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e554 ">Name of the Spark application. Spark displays this
                                        application name in the log files. <p class="p">If you run pipelines
                                            that include multiple Spark Evaluator processors, be
                                            sure to use a unique application name for each to make
                                            debugging simpler.</p>
<p class="p">Default is SDC Spark
                                        App.</p>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e551 ">Spark Transformer Class
                                        <a class="xref" href="Spark.html#concept_lfl_dvd_1y">
                                            <img class="image" id="task_g1p_gqn_zx__image_h4p_p5v_yq" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e554 ">Fully qualified name of the custom Spark class that
                                        implements the StreamSets SparkTransformer API. Enter the
                                        class name using the following
                                            format:<pre class="pre codeblock">com.streamsets.spark.&lt;custom class&gt;</pre>
<div class="p">For
                                            example, let's assume that you developed a
                                                <samp class="ph codeph">GetCreditCardType</samp> class that
                                            implemented the SparkTransformer API as
                                            follows:<pre class="pre codeblock">public class GetCreditCardType extends SparkTransformer implements Serializable {
...
}</pre>
</div>
<div class="p">Then
                                            you would enter the class name as
                                            follows:<pre class="pre codeblock">com.streamsets.spark.GetCreditCardType</pre>
</div>
</td>

                                </tr>

                                <tr>
                                    <td class="entry" valign="top" width="30%" headers="d249935e551 ">Init Method Arguments
                                        <a class="xref" href="Spark.html#concept_lfl_dvd_1y">
                                            <img class="image" id="task_g1p_gqn_zx__image_h3p_p6v_yq" src="../Graphics/icon_moreInfo.png" height="12" width="12" /></a></td>

                                    <td class="entry" valign="top" width="70%" headers="d249935e554 ">Arguments to pass to the <samp class="ph codeph">init</samp> method in
                                        the custom Spark class. Enter an argument as required by
                                        your custom Spark class.<p class="p">Click the
                                                <span class="ph uicontrol">Add</span> icon to add additional
                                            arguments.</p>
</td>

                                </tr>

                            </tbody>

                        </table>
</div>

                </div>
            </li>
</ol>

    </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Processors/Processors_title.html" title="Processors"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Processors</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>