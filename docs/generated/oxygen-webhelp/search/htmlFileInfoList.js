fil = new Array();
fil["0"]= "Alerts/Alerts_title.html@@@Alerts@@@...";
fil["1"]= "Apx-GrokPatterns/GrokPatterns.html@@@Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log data. You can use a single pattern or use several patterns to define a larger pattern. You can also use valid sections of patterns to define a custom pattern...";
fil["2"]= "Cluster_Mode/ClusterPipelines.html@@@Cluster Pipelines@@@A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode...";
fil["3"]= "Cluster_Mode/ClusterPipelines_title.html@@@Cluster Pipelines@@@...";
fil["4"]= "Data_Preview/DataCollectorWindow-Preview.html@@@Data Collector Console - Preview Mode@@@You can use the Data Collector console to view how data passes through the pipeline...";
fil["5"]= "Data_Preview/DataPreview.html@@@Data Preview@@@You can preview data to help build or fine-tune a pipeline. When you preview data, the Data Collector passes data through the pipeline and allows you to review how the data passes through each stage...";
fil["6"]= "Destinations/Destinations_overview.html@@@Destinations@@@A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline...";
fil["7"]= "Destinations/KProducer-PartitionStrategy.html@@@Partition Strategy@@@The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically...";
fil["8"]= "Destinations/KProducer-RuntimeResolution.html@@@Runtime Topic Resolution@@@Kafka Producer can write a record to the topic based on an expression. When Kafka Producer evaluates a record, it calculates the expression based on record values and writes the record to the resulting topic...";
fil["9"]= "Destinations/RPCdest-Connections.html@@@RPC Connections@@@In an RPC destination, the RPC connections define where the destinations passes dat...";
fil["10"]= "Expression_Language/RecordFunctions.html@@@Record Functions@@@Use record functions to determine information about a record, such as the stage that created it or whether a field exists in the record...";
fil["11"]= "Getting_Started/DCollector_Window.html@@@Data Collector Console@@@The Data Collector provides a console to configure pipelines, preview data, monitor pipelines, and review snapshots of dat...";
fil["12"]= "Getting_Started/GettingStarted_Title.html@@@Getting Started@@@...";
fil["13"]= "Getting_Started/LoggingIn_CreatingPipeline.html@@@Logging In and Creating a Pipeline@@@After you start the Data Collector, you can log in to the Data Collector console and create your first pipeline...";
fil["14"]= "Getting_Started/What_isa_DataCollector.html@@@What is a Data Collector?@@@Let s walk through it...";
fil["15"]= "Install_Config/JavaHeapSize.html@@@Java Heap Size Configuration@@@You can define the Java heap size used by the Data Collector. By default, the Data Collector Java heap size is 1024 MB...";
fil["16"]= "Origins/BatchSizeWaitTime.html@@@Batch Size and Wait Time@@@For origin stages, the batch size determines the maximum number of records sent through the pipeline at one time. The batch wait time determines the time that the origin waits for data before sending a batch. At the end of the wait time, it sends the batch regardless of how many records the batch contains...";
fil["17"]= "Origins/Origins_overview.html@@@Origins@@@An origin stage represents the source for the pipeline. You can use a single origin stage in a pipeline...";
fil["18"]= "Pipeline_Configuration/ConfiguringAPipeline.html@@@Configuring a Pipeline@@@Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline...";
fil["19"]= "Pipeline_Configuration/DataCollectorWindow-Config.html@@@Data Collector Console - Edit Mode@@@The following image shows the Data Collector console when you configure a pipeline: Area / Icon Name Description 1 Pipeline canvas Displays the pipeline. Use to configure the pipeline data flow. 2...";
fil["20"]= "Pipeline_Configuration/ErrorHandling.html@@@Error Record Handling@@@You can configure error record handling at a stage level and at a pipeline level...";
fil["21"]= "Pipeline_Configuration/PipelineMemory.html@@@Pipeline Memory@@@The Data Collector uses memory when it runs a pipeline. To avoid causing out-of-memory errors on the host machine, you can configure the maximum amount of memory that can be used for the pipeline...";
fil["22"]= "Pipeline_Configuration/Retry.html@@@Retrying the Pipeline@@@By default, when the Data Collector encounters a stage-level error that might cause a standalone pipeline to fail, it retries the pipeline. That is, it waits a period of time, and then tries again to run the pipeline...";
fil["23"]= "Pipeline_Configuration/WildcardsArraysMaps.html@@@Wildcard Use for Arrays and Maps@@@In some processors, you can use the asterisk wildcard (*) as indices in an array or key values in a map. Use a wildcard to help define the field paths for maps and arrays...";
fil["24"]= "Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...";
fil["25"]= "Pipeline_Design/DelimitedDataRecordTypes.html@@@Delimited Data Record Types@@@When reading delimited data, the Data Collector can convert the data in a list or list-map record type. Use the default list-map record type to easily process delimited data. List-Map Record Type...";
fil["26"]= "Pipeline_Design/DeliveryGuarantee.html@@@Delivery Guarantee@@@When you configure a pipeline, you define how you want data to be treated: Do you want to prevent the loss of data or the duplication of dat...";
fil["27"]= "Pipeline_Design/Preconditions.html@@@Preconditions@@@Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages...";
fil["28"]= "Pipeline_Design/RequiredFields.html@@@Required Fields@@@A required field is a field that must exist in a record to allow it into the stage for processing. When a record does not include a required field, the record is diverted to the pipeline for error handling. You can define required fields for any processor and most destination stages...";
fil["29"]= "Pipeline_Design/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that the Data Collector uses to generate error records. The Data Collector can also use the data format to read and write dat...";
fil["30"]= "Pipeline_Design/What_isa_Pipeline.html@@@What is a Pipeline?@@@A pipeline describes the flow of data from the origin system to destination systems and defines how to transform the data along the way...";
fil["31"]= "Pipeline_Monitoring/DataCollectorWindow-Monitor.html@@@Data Collector Console - Monitor Mode@@@In Monitor mode, you can use the Data Collector console to view data as it passes through the pipeline...";
fil["32"]= "Pipeline_Monitoring/PipelineMonitoring.html@@@Pipeline Monitoring@@@When the Data Collector runs a pipeline, you can view real-time statistics about the pipeline, examine a sample of the data being processed, and create rules and alerts...";
fil["33"]= "Processors/Processors_overview.html@@@Processors@@@A processor stage represents a type of data processing that you want to perform. You can use as many processors in a pipeline as you need...";
fil["34"]= "RPC_Pipelines/EnableSSL.html@@@Enable SSL@@@You can enable the RPC pipelines to transfer data securely using SSL. To use SSL, enable SSL for both the RPC destination and the RPC origin...";
