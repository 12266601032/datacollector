fil = new Array();
fil["0"]= "oxy_ex-1/Advanced_Config/RuntimeResources.html@@@Runtime Resources@@@Similar to runtime properties, runtime resources are values that you define in a file local to the Data Collector and call from within a pipeline. But with runtime resources, you can restrict the permissions for the files to secure sensitive information. Use runtime properties to load sensitive information from files at runtime...";
fil["1"]= "oxy_ex-1/Apx-GrokPatterns/GrokPatterns.html@@@Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log data. You can use a single pattern or use several patterns to define a larger pattern. You can also use valid sections of patterns to define a custom pattern...";
fil["2"]= "oxy_ex-1/Cluster_Mode/ClusterPipelines.html@@@Cluster Pipelines@@@A cluster pipeline is a pipeline that runs in cluster execution mode. You can run a pipeline in standalone execution mode or cluster execution mode...";
fil["3"]= "oxy_ex-1/Destinations/KProducer-PartitionStrategy.html@@@Partition Strategy@@@The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically...";
fil["4"]= "oxy_ex-1/Destinations/KProducer-RuntimeResolution.html@@@Runtime Topic Resolution@@@Kafka Producer can write a record to the topic based on an expression. When Kafka Producer evaluates a record, it calculates the expression based on record values and writes the record to the resulting topic...";
fil["5"]= "oxy_ex-1/Install_Config/Kerberos.html@@@Enabling Kerberos Authentication@@@You can use Kerberos authentication to connect to origin and destination systems, as well as YARN clusters...";
fil["6"]= "oxy_ex-1/Origins/Directory.html@@@Directory@@@Configure a Directory origin to read data from files in a directory...";
fil["7"]= "oxy_ex-1/Origins/FileTail.html@@@File Tail@@@File Tail can process log and text data that includes multiple lines. You might use multiple line processing to include stack traces with log data, or to process MySQL multiline logs...";
fil["8"]= "oxy_ex-1/Origins/HTTPClient.html@@@HTTP Client@@@The HTTP Client origin reads JSON data from an HTTP resource URL...";
fil["9"]= "oxy_ex-1/Origins/HadoopFS-origin.html@@@Hadoop FS@@@Configure a Hadoop FS origin to read data from HDFS...";
fil["10"]= "oxy_ex-1/Origins/JDBCConsumer.html@@@JDBC Consumer@@@JDBC Consumer uses offset values in the offset column to determine where to continue processing after a deliberate or unexpected stop. To ensure seamless recovery, use a primary key or indexed column as the offset column...";
fil["11"]= "oxy_ex-1/Origins/KConsumer.html@@@Kafka Consumer@@@Configure a Kafka Consumer to read data from a Kafka cluster...";
fil["12"]= "oxy_ex-1/Origins/KinConsumer.html@@@Kinesis Consumer@@@You can configure the read interval for the Kinesis Consumer. The read interval determines how long Kinesis Consumer waits before requesting additional data from Kinesis shards. By default, the Kinesis Consumer waits one second between requests...";
fil["13"]= "oxy_ex-1/Origins/MongoDB.html@@@MongoDB@@@You can configure the read preference that the MongoDB origin uses. The read preference determines how the origin reads data from different members of the MongoDB replica set...";
fil["14"]= "oxy_ex-1/Origins/Omniture.html@@@Omniture@@@The Omniture origin processes JSON website usage reports generated by the Omniture reporting APIs. Omniture is also known as the Adobe Marketing Cloud...";
fil["15"]= "oxy_ex-1/Origins/Origins_overview.html@@@Origins@@@You can preview raw source data for Directory, File Tail, and Kafka Consumer origins. Preview raw source data when reviewing the data might help with origin configuration...";
fil["16"]= "oxy_ex-1/Origins/Origins_title.html@@@Origins@@@...";
fil["17"]= "oxy_ex-1/Origins/RPC-Configuring.html@@@Configuring an RPC Origin@@@Configure an RPC origin to process data from an RPC destination...";
fil["18"]= "oxy_ex-1/Origins/RPC-EnabingSecurity.html@@@Enabling SSL@@@You can use HTTPS to securely transfer data to the RPC origin. To use HTTPS you need a valid keystore file and password...";
fil["19"]= "oxy_ex-1/Origins/RPC.html@@@RPC@@@The RPC origin reads data passed from an RPC destination over the HTTP or HTTPS. Use the RPC origin as part of an RPC pipeline that reads data from another RPC pipeline...";
fil["20"]= "oxy_ex-1/Origins/UDP.html@@@UDP Source@@@The UDP Source origin reads messages from one or more UDP ports. UDP Source generates a record for every message. UDP Source can read syslog messages, Netflow messages from NetFlow Version 5, and collectd messages...";
fil["21"]= "oxy_ex-1/Pipeline_Configuration/WildcardsArraysMaps.html@@@Wildcard Use for Arrays and Maps@@@In some processors, you can use the asterisk wildcard (*) as indices in an array or key values in a map. Use a wildcard to help define the field paths for maps and arrays...";
fil["22"]= "oxy_ex-1/Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...";
fil["23"]= "oxy_ex-1/Pipeline_Design/DelimitedDataRecordStructure.html@@@Delimited Data Record Structure@@@When reading delimited data, the Data Collector converts each row of delimited data to a record in List data format. To perform additional processing on the record, you can use the general expression language with your knowledge of the resulting record format or you can use delimited data functions...";
fil["24"]= "oxy_ex-1/Pipeline_Design/Preconditions.html@@@Preconditions@@@Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages...";
fil["25"]= "oxy_ex-1/Pipeline_Design/RequiredFields.html@@@Required Fields@@@A required field is a field that must exist in a record to allow it into the stage for processing. When a record does not include a required field, the record is diverted to the pipeline for error handling. You can define required fields for any processor and most destination stages...";
fil["26"]= "oxy_ex-1/Pipeline_Design/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that the Data Collector uses to generate error records. The Data Collector can also use the data format to read and write dat...";
fil["27"]= "oxy_ex-1/RPC_Pipelines/RPCpipeline.html@@@RPC Pipelines@@@Remote Protocol Call pipelines, a.k.a. RPC pipelines, are a set of pipelines that pass data from one pipeline to another over the WAN without writing to an intermediary system. Use RPC pipelines to securely transmit data across data centers...";
