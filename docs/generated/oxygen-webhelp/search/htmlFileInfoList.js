fil = new Array();
fil["0"]= "Administration/Administration_title.html@@@Administration@@@...";
fil["1"]= "Administration/CLI-Overview.html@@@Command Line Interface@@@Use the help command to view additional information for the specified command...";
fil["2"]= "Administration/RESTResponse.html@@@REST Response@@@You can view REST response JSON data for different aspects of the Data Collector, such as pipeline configuration information or monitoring details...";
fil["3"]= "Administration/Restarting.html@@@Restarting Data Collector@@@You can restart Data Collector to apply changes to the Data Collector configuration file, environment configuration file, or user logins. During the restart process, Data Collector shuts down and then...";
fil["4"]= "Administration/ShuttingDown.html@@@Shutting Down Data Collector@@@You can shut down and then manually launch Data Collector to apply changes to the Data Collector configuration file, environment configuration file, or user logins. To use the command line for...";
fil["5"]= "Administration/ViewingDCConfigs.html@@@Viewing Data Collector Configuration Properties@@@To view Data Collector configuration properties, click Administration &gt; Configuration . For details about the configuration properties or to edit the configuration file, see Configuring Data Collector...";
fil["6"]= "Administration/ViewingDirectories.html@@@Viewing Data Collector Directories@@@You can view the directories that the Data Collector uses. You might check the directories being used to access a file in the directory or to increase the amount of available space for a directory...";
fil["7"]= "Administration/ViewingJVMMetrics.html@@@Viewing JVM Metrics@@@The Data Collector provides JVM metrics for the Data Collector...";
fil["8"]= "Administration/ViewingLogData.html@@@Viewing Data Collector Logs@@@You can view and download log data. When you download log data, you can select the file to download...";
fil["9"]= "Alerts/DataAlerts.html@@@Data Rules and Alerts@@@Data rules define the information that you want to see about the data that passes between stages. You can create data rules based on any link in the pipeline. You can also enable metrics and create alerts for data rules...";
fil["10"]= "Alerts/DataDriftAlerts.html@@@Data Drift Rules and Alerts@@@Data drift alerts trigger when a change of the specified type occurs from record to record...";
fil["11"]= "Alerts/EmailforAlerts-Configuring.html@@@Configuring Email for Alerts@@@You can define the email addresses to receive metric and data alerts. When an alert triggers an email, the Data Collector sends an email to every address in the list...";
fil["12"]= "Alerts/MetricAlerts.html@@@Metric Rules and Alerts@@@Create a custom metric rule to receive alerts when a real-time statistic reaches a certain threshold. You can create metric rules and alerts when you configure or monitor a pipeline. You can edit or delete metric rules when they are not enabled...";
fil["13"]= "Alerts/RulesAlerts-Overview.html@@@Rules and Alerts@@@Define rules to enable the Data Collector to capture information about a running pipeline. Enable the alert associated with a rule to be notified when the specified condition occurs...";
fil["14"]= "Alerts/RulesAlerts_title.html@@@Rules and Alerts@@@...";
fil["15"]= "Apx-DataFormats/DataFormat_Overview.html@@@Data Format Support@@@The following table lists the data formats supported by each destination...";
fil["16"]= "Apx-DataFormats/DataFormat_Title.html@@@Data Formats by Stage@@@...";
fil["17"]= "Apx-GrokPatterns/DateTimePatterns.html@@@Date and Time Grok Patterns@@@You can use the following date and time grok patterns to define the structure of log data: MONTH...";
fil["18"]= "Apx-GrokPatterns/GeneralPatterns.html@@@General Grok Patterns@@@You can use the following general grok patterns to define the structure of log data: USER %{USERNAME} USERNAME [a-zA-Z0-9._-]+ BASE10NUM (?&lt;![0-9.+-])(?&gt;[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9...";
fil["19"]= "Apx-GrokPatterns/GrokPatterns.html@@@Defining Grok Patterns@@@You can use the grok patterns in this appendix to define the structure of log dat...";
fil["20"]= "Apx-GrokPatterns/GrokPatterns_title.html@@@Grok Patterns@@@...";
fil["21"]= "Apx-GrokPatterns/JavaPatterns.html@@@Java Grok Patterns@@@You can use the following Java-related grok patterns to define the structure of log data: JAVACLASS (?:[a-zA-Z$_][a-zA-Z$_0-9]*\\.)*[a-zA-Z$_][a-zA-Z$_0-9]* JAVAFILE (?:[A-Za-z0-9_. -]+) A space...";
fil["22"]= "Apx-GrokPatterns/LogPatterns.html@@@Log Grok Patterns@@@You can use the following log-related grok patterns to define the structure of log data: SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME} PROG (?:[\\w._/%-]+) SYSLOGPROG...";
fil["23"]= "Apx-GrokPatterns/NetworkingPatterns.html@@@Networking Grok Patterns@@@You can use the following networking-related grok patterns to define the structure of log data: MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC}) CISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4...";
fil["24"]= "Apx-GrokPatterns/PathPatterns.html@@@Path Grok Patterns@@@You can use the following path grok patterns to define the structure of log data: PATH (?:%{UNIXPATH}|%{WINPATH}) UNIXPATH (?&gt;/(?&gt;[\\w_%!$@:.,~-]+|\\\\.)*)+ TTY...";
fil["25"]= "Apx-RegEx/Examples.html@@@Regex Examples@@@Masking credit card numbers, except for one group You can use the following regular expression in the Field Masker to mask all numbers in a credit or debit card except for the last 4 digits...";
fil["26"]= "Apx-RegEx/QuickReference.html@@@Quick Reference@@@The following table includes some details you might find helpful when creating a regular expression: Character Description Examples [ ] Use brackets to define character classes. [0-9][0-9][0-9...";
fil["27"]= "Apx-RegEx/RegEx-Overview.html@@@Regular Expressions@@@A regular expression, also known as regex, describes a pattern for a string...";
fil["28"]= "Apx-RegEx/RegEx-Title.html@@@Regular Expressions@@@...";
fil["29"]= "Apx-RegEx/Regex-inthePipeline.html@@@Regular Expressions in the Pipeline@@@Though generally not required, you can use Java-based regular expressions at various locations within a pipeline to define, search for, or manipulate strings...";
fil["30"]= "Cluster_Mode/ClusterPipelines.html@@@Cluster Pipelines@@@You can configure Data Collector to use HTTP or HTTPS when you run cluster pipelines. By default Data Collector uses HTTP...";
fil["31"]= "Cluster_Mode/ClusterPipelines_title.html@@@Cluster Pipelines@@@...";
fil["32"]= "Cluster_Mode/HDFSRequirements.html@@@HDFS Requirements@@@Cluster mode pipelines that read from HDFS require the Cloudera distribution of Hadoop (CDH) or Hortonworks Data Platform (HDP). Complete the following steps to configure a cluster mode pipeline to...";
fil["33"]= "Cluster_Mode/KafkaRequirements.html@@@Kafka Requirements@@@Cluster mode pipelines that read from a Kafka cluster have the following minimum requirements: Component Minimum Requirement Spark Streaming Spark version 1.3 through 1.6 Apache Kafka Spark Streaming...";
fil["34"]= "Cluster_Mode/MapRRequirements.html@@@MapR Requirements@@@Complete the following steps to configure a cluster pipeline to read from MapR in cluster streaming mode...";
fil["35"]= "DPM/AggregatedStatistics.html@@@Aggregated Statistics for Pipelines@@@You can configure a pipeline to aggregate statistics after the Data Collector has been registered with DPM...";
fil["36"]= "DPM/DPM.html@@@Meet Dataflow Performance Manager@@@StreamSets Dataflow Performance Manager (DPMTM) is a management console for data in motion. DPM lets you map, measure, and master complex dataflows within your organization. With DPM, you can map multiple dataflows in a single visual topology and track changes to the dataflows over time. You can view real-time statistics to measure dataflow performance across each topology, from end-to-end or point-to-point. You can master your day-to-day operations by performing release and configuration management, and monitoring alerts to ensure incoming data meets business requirements for availability and accuracy...";
fil["37"]= "DPM/DPMConfiguration.html@@@DPM Configuration@@@You can customize how a Data Collector works with DPM by editing the DPM configuration file, dpm.properties...";
fil["38"]= "DPM/DPM_title.html@@@Dataflow Performance Manager@@@...";
fil["39"]= "DPM/OrgUserAccount.html@@@Request a DPM Organization and User Account@@@To register a Data Collector with DPM, you must have a DPM user account within an organization...";
fil["40"]= "DPM/PipelineManagement.html@@@Pipeline Management with DPM@@@After you register a Data Collector with DPM, you can manage how the pipelines work with DPM...";
fil["41"]= "DPM/RegisterSDCwithDPM.html@@@Register Data Collector with DPM@@@You must register a Data Collector to work with DPM. When you register a Data Collector, Data Collector generates an authentication token that it uses to issue authenticated requests to DPM...";
fil["42"]= "DPM/UnregisterSDCwithDPM.html@@@Unregister Data Collector from DPM@@@You can unregister a Data Collector from DPM when you no longer want to use that Data Collector installation with DPM. When you restart an unregistered Data Collector, you use your Data Collector user account to log in...";
fil["43"]= "DPM/WorkingWithDPM.html@@@Working with DPM@@@Before you can work with DPM, you must have a DPM organization and user account...";
fil["44"]= "Data_Preview/DataCollectorWindow-Preview.html@@@Data Collector Console - Preview Mode@@@You can use the Data Collector console to view how data passes through the pipeline...";
fil["45"]= "Data_Preview/DataPreview.html@@@Data Preview@@@You can preview complete and incomplete pipelines. The Data Preview icon becomes active when data preview is available...";
fil["46"]= "Data_Preview/DataPreview_Title.html@@@Data Preview@@@...";
fil["47"]= "Data_Preview/EditingPreviewData.html@@@Editing Preview Data@@@You can edit preview data to view how a stage or group of stages processes the changed data. Edit preview data to test for data conditions that might not appear in the preview data set...";
fil["48"]= "Data_Preview/EditingProperties.html@@@Editing Properties@@@In data preview, you can edit stage properties to see how the changes affect preview data. For example, you might edit the expression in an Expression Evaluator to see how the expression alters dat...";
fil["49"]= "Data_Preview/PreviewingMultipleStages.html@@@Previewing Multiple Stages@@@You can preview data for a group of linked stages within a pipeline...";
fil["50"]= "Data_Preview/PreviewingaSingleStage.html@@@Previewing a Single Stage@@@You can preview data for a single stage. In the Preview panel, you can review the values for each record to determine if the stage transforms data as expected. Above the pipeline canvas, click the...";
fil["51"]= "Destinations/AmazonS3.html@@@Amazon S3@@@You can configure the destination to use Amazon Web Services server-side encryption (SSE) to protect data written to Amazon S3. When configured for server-side encryption, the destination passes required server-side encryption configuration values to Amazon S3. Amazon S3 uses the values to encrypt the data as it is written to Amazon S3...";
fil["52"]= "Destinations/Bigtable.html@@@Google Bigtable@@@The time basis determines the timestamp value added for each column written to Google Cloud Bigtable...";
fil["53"]= "Destinations/Cassandra.html@@@Cassandra@@@The Cassandra destination writes data to a Cassandra cluster...";
fil["54"]= "Destinations/DataLakeStore.html@@@Azure Data Lake Store@@@The Azure Data Lake Store destination writes data to the Microsoft Azure Data Lake Store. Use the Azure Data Lake Store destination in standalone pipelines only at this time. Before you use the...";
fil["55"]= "Destinations/Destinations-title.html@@@Destinations@@@...";
fil["56"]= "Destinations/Destinations_overview.html@@@Destinations@@@A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline...";
fil["57"]= "Destinations/Elasticsearch.html@@@Elasticsearch@@@When appropriate, you can specify the expression that defines the document ID. When you do not specify an expression, Elasticsearch generates IDs for each document...";
fil["58"]= "Destinations/Flume.html@@@Flume@@@The Flume destination writes data to a Flume source. When you write data to Flume, you pass data to a Flume client. The Flume client passes data to hosts based on client configuration properties...";
fil["59"]= "Destinations/HBase.html@@@HBase@@@The time basis determines the timestamp value added for each column written to HBase...";
fil["60"]= "Destinations/HadoopFS-destination.html@@@Hadoop FS@@@You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS...";
fil["61"]= "Destinations/Hive.html@@@Hive Streaming@@@The Hive Streaming destination writes data to Hive tables stored in the ORC (Optimized Row Columnar) file format...";
fil["62"]= "Destinations/HiveMetastore.html@@@Hive Metastore@@@You must configure Hive Metastore to use Hive and Hadoop configuration files and individual properties...";
fil["63"]= "Destinations/InfluxDB.html@@@InfluxDB@@@The InfluxDB destination writes data to an InfluxDB database...";
fil["64"]= "Destinations/JDBCProducer.html@@@JDBC Producer@@@Configure the JDBC Producer to use JDBC to write data to a database table...";
fil["65"]= "Destinations/KProducer.html@@@Kafka Producer@@@The partition strategy determines how to write data to Kafka partitions. You can use a partition strategy to balance the work load or to write data semantically...";
fil["66"]= "Destinations/KinFirehose.html@@@Kinesis Firehose@@@The Kinesis Firehose destination writes data to an existing delivery stream in Amazon Kinesis Firehose. Before using the Kinesis Firehose destination, use the AWS Management Console to create a delivery stream to an Amazon S3 bucket or Amazon Redshift table...";
fil["67"]= "Destinations/KinProducer.html@@@Kinesis Producer@@@The Kinesis Producer destination writes data to Amazon Kinesis Streams. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon S3, use the Amazon S3 destination...";
fil["68"]= "Destinations/Kudu.html@@@Kudu@@@To write to Kudu, you configure the destination to perform one of the following write operations: insert, update, delete, or upsert. You define the default operation for the destination. You can also define the operation in a record header attribute...";
fil["69"]= "Destinations/LocalFS.html@@@Local FS@@@Use the Local FS destination to write records to files in a local file system. When you configure a Local FS destination, you can define a directory template and time basis to determine the output...";
fil["70"]= "Destinations/MapRDB.html@@@MapR DB@@@You can configure the MapR DB destination to use an HBase user to write data to MapR DB...";
fil["71"]= "Destinations/MapRFS.html@@@MapR FS@@@You can configure the MapR FS destination to use an HDFS user to write data to MapR FS...";
fil["72"]= "Destinations/MapRStreamsProd.html@@@MapR Streams Producer@@@The MapR Streams Producer destination writes messages to MapR Streams...";
fil["73"]= "Destinations/MongoDB.html@@@MongoDB@@@Configure a MongoDB destination to write to MongoDB...";
fil["74"]= "Destinations/RabbitMQ.html@@@RabbitMQ Producer@@@RabbitMQ Producer writes AMQP messages to a single RabbitMQ queue...";
fil["75"]= "Destinations/Redis.html@@@Redis@@@The Redis destination writes data to Redis...";
fil["76"]= "Destinations/SDC_RPCdest.html@@@SDC RPC@@@The SDC RPC destination compresses data by default when passing data to an SDC RPC origin. When necessary, you can disable compression in the destination...";
fil["77"]= "Destinations/Salesforce.html@@@Salesforce@@@The Salesforce destination writes data to Salesforce objects...";
fil["78"]= "Destinations/Solr.html@@@Solr@@@The index mode determines how the Solr destination indexes records when writing to Solr. Index mode also determines how the destination handles errors...";
fil["79"]= "Destinations/ToError.html@@@To Error@@@The To Error destination passes records to the pipeline for error handling. Use the To Error destination to send a stream of records to pipeline error handling...";
fil["80"]= "Destinations/Trash.html@@@Trash@@@The Trash destination discards records. Use the Trash destination as a visual representation of records discarded from the pipeline. Or, you might use the Trash destination during development as a temporary placeholder...";
fil["81"]= "Destinations/WaveAnalytics.html@@@Wave Analytics@@@The Wave Analytics destination typically creates multiple datasets, based on the configured dataset wait time. You can optionally configure the destination to use a Wave Analytics dataflow to combine multiple datasets together...";
fil["82"]= "Event_Handling/CaseStudy-EventStorage.html@@@Case Study: Event Storage@@@Store event records to preserve an audit trail of the events that occur. You can store event records from any event-generating stage. For this case study, say you want to keep a log of the files...";
fil["83"]= "Event_Handling/CaseStudy-FileManagement.html@@@Case Study: Output File Management@@@By default, the Hadoop FS destination creates a complex set of directories for output files and late record files, keeping files open for writing based on stage configuration. That s great, but once...";
fil["84"]= "Event_Handling/CaseStudy-Impala.html@@@Case Study: Impala Metadata Updates for HDS@@@You love the Hive Drift Solution (HDS) because it automatically updates the Hive metastore when needed. But if you ve been using it with Impala, you ve been trying to time the Invalidate Metadat...";
fil["85"]= "Event_Handling/CaseStudy-Parquet.html@@@Case Study: Parquet Conversion@@@Say you want to store data on HDFS using the columnar format, Parquet. But Data Collector doesn t have a Parquet data format. How do you do it? The event framework was created for exactly this...";
fil["86"]= "Event_Handling/DataPreview_Monitor.html@@@Event Records in Data Preview, Monitor, and Snapshot@@@When generated, event records display in data preview, Monitor mode, and snapshot as event records. Once a record leaves the event-generating stage, it is treated like a standard record. In dat...";
fil["87"]= "Event_Handling/EventFramework-Overview.html@@@Event Framework@@@The event framework enables the pipeline to trigger tasks in external systems based on actions that occur in the pipeline, such as running a MapReduce job after the pipeline writes a file to HDFS. You...";
fil["88"]= "Event_Handling/EventFramework-Summary.html@@@Event Framework Summary@@@To summarize the key points of the event framework: You can use the event framework to any pipeline that includes a stage that generates events. The event-generating stage generates events at logical...";
fil["89"]= "Event_Handling/EventFramework-Title.html@@@Event Framework@@@...";
fil["90"]= "Event_Handling/EventRecords.html@@@Event Records@@@Event records are records created by a stage when a stage-related event occurs, like when an origin starts reading a new file or a destination closes an output file. Most event records pass general...";
fil["91"]= "Event_Handling/EventStreams.html@@@Event Streams@@@You can use the event framework in any pipeline where the event handling logic suits your needs. When configuring the event stream, you can add additional stages as needed, but you cannot merge the...";
fil["92"]= "Executors/Executors-overview.html@@@Executors@@@An executor stage executes a task in an external system when it receives an event - it does not write or store events. Use executor stages as part of an event stream to perform event-driven...";
fil["93"]= "Executors/Executors-title.html@@@Executors@@@...";
fil["94"]= "Executors/HDFSMetadata.html@@@HDFS File Metadata Executor@@@The HDFS File Metadata executor changes file metadata for closed files in HDFS, MapR FS, or a local file system each time it receives a file closure event record. Use the HDFS File Metadata executor...";
fil["95"]= "Executors/HiveQuery.html@@@Hive Query Executor@@@The Hive Query executor connects to Hive or Impala and performs a user-defined Hive or Impala query each time it receives an event record. Use the Hive Query executor as part of an event stream to...";
fil["96"]= "Executors/JDBCQuery.html@@@JDBC Query Executor@@@The JDBC Query executor connects through JDBC to a database and performs a user-defined SQL query each time it receives an event record. Use the JDBC Query executor as part of an event stream in the...";
fil["97"]= "Executors/MapReduce.html@@@MapReduce Executor@@@The MapReduce executor starts a MapReduce job in HDFS or MapR FS each time it receives an event record. Use the MapReduce executor as part of an event stream. You can use the MapReduce executor to...";
fil["98"]= "Expression_Language/Constants.html@@@Constants@@@The expression language provides constants for use in expressions. In a pipeline, you can also create constants for use within the pipeline...";
fil["99"]= "Expression_Language/DateTimeVariables.html@@@Datetime Variables@@@The expression language provides datetime variables for use in expressions...";
fil["100"]= "Expression_Language/ExpressionLanguage_overview.html@@@Expression Language@@@The StreamSets expression language enables you to create expressions that evaluate or modify data. The StreamSets expression language is based on the JSP 2.0 expression language. Use the expression...";
fil["101"]= "Expression_Language/ExpressionLanguage_title.html@@@Expression Language@@@...";
fil["102"]= "Expression_Language/Functions.html@@@Functions@@@Use time functions to return the current time or to transform datetime dat...";
fil["103"]= "Expression_Language/Literals.html@@@Literals@@@The expression language includes the following literals...";
fil["104"]= "Expression_Language/Operators.html@@@Operators@@@The precedence of operators highest to lowest, left to right is as follows...";
fil["105"]= "Expression_Language/ReservedWords.html@@@Reserved Words@@@The following words are reserved for the expression language and should not be used as identifiers...";
fil["106"]= "Getting_Started/DCollector_Window.html@@@Data Collector Console@@@Data Collector provides a console to configure pipelines, preview data, monitor pipelines, and review snapshots of dat...";
fil["107"]= "Getting_Started/GettingStarted_Title.html@@@Getting Started@@@...";
fil["108"]= "Getting_Started/LoggingIn_CreatingPipeline.html@@@Logging In and Creating a Pipeline@@@After you start Data Collector, you can log in to the Data Collector console and create your first pipeline...";
fil["109"]= "Getting_Started/What_is_DataCollector.html@@@What is StreamSets Data Collector?@@@Let s walk through it...";
fil["110"]= "Glossary/GlossaryOfTerms.html@@@Glossary of Terms@@@batch A set of records that passes through a pipeline. Data Collector processes data in batches. cluster execution mode Pipeline execution mode that allows you to process large volumes of data from...";
fil["111"]= "Glossary/Glossary_title.html@@@Glossary@@@...";
fil["112"]= "Hive_Drift_Solution/BasicImplementation.html@@@Basic Implementation@@@You can use the Hive Metadata processor, Hive Metastore destination for metadata processing, and Hadoop FS or MapR FS destination for data processing in any pipeline where the logic is appropriate...";
fil["113"]= "Hive_Drift_Solution/CaseStudy.html@@@Case Study@@@Now what happens when you start the pipeline...";
fil["114"]= "Hive_Drift_Solution/HiveDataTypes.html@@@Hive Data Types@@@The following table lists the Data Collector data types and the corresponding Hive data types. The Hive Metadata processor uses these conversions when generating metadata records. The Hive Metadat...";
fil["115"]= "Hive_Drift_Solution/HiveDrift-Overview.html@@@Hive Drift Solution: Ingesting Drifting Data into Hive@@@The Hive Drift Solution detects drift in incoming data and updates corresponding Hive tables. The solution enables creating and updating Hive tables based on record requirements and writing data to...";
fil["116"]= "Hive_Drift_Solution/HiveDriftSolution_title.html@@@Hive Drift Solution@@@...";
fil["117"]= "Hive_Drift_Solution/Implementation.html@@@Implementation Steps@@@To implement the Hive Drift Solution, perform the following steps: Configure the origin and any additional processors that you want to use. If using the JDBC Consumer as the origin, enable the...";
fil["118"]= "Install_Config/AdditionalDrivers.html@@@Additional Drivers@@@You can install additional drivers for stages. Before you use the following stages, you need to install drivers for the implementation that you want to use: JDBC Consumer origin JMS Consumer origin...";
fil["119"]= "Install_Config/Authentication.html@@@User Authentication@@@Data Collector can authenticate user accounts based on LDAP or files. Best practice is to use LDAP if your organization has it. By default, Data Collector uses file-based authentication...";
fil["120"]= "Install_Config/CMInstall-Overview.html@@@Installation with Cloudera Manager@@@You can use Cloudera Manager to easily install Data Collector across the cluster as an add-on service. To install Data Collector through Cloudera Manager, perform the following steps: Install the...";
fil["121"]= "Install_Config/CoreInstall_Overview.html@@@Core Installation@@@You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space...";
fil["122"]= "Install_Config/CreateAnotherDC.html@@@Creating Another Data Collector Instance@@@You can create another instance of a Data Collector tarball or RPM installation on the same machine with the create-dc command. The additional Data Collector instance uses the same configuration as the original Data Collector instance. You can modify the configuration properties as needed...";
fil["123"]= "Install_Config/CustomStageLibraries.html@@@Custom Stage Libraries@@@If you develop custom stages, store the stage libraries in a local directory external to the Data Collector installation directory. Use an external directory to enable use of the custom stage libraries after Data Collector upgrades...";
fil["124"]= "Install_Config/DCConfig.html@@@Data Collector Configuration@@@You can use Kerberos authentication to connect to external systems as well as YARN clusters...";
fil["125"]= "Install_Config/DCEnvironmentConfig.html@@@Data Collector Environment Configuration@@@Data Collector provides two environment configuration files: $SDC_DIST/libexec/sdc-env.sh - Used when you start Data Collector manually from the command line. $SDC_DIST/libexec/sdcd-env.sh - Used when...";
fil["126"]= "Install_Config/FullInstall_ServiceStart.html@@@Full Installation and Launch (Service Start)@@@To install the full Data Collector as a service, you can download the Data Collector RPM package or the Data Collector tarball from the StreamSets website. When you install from the RPM package, Dat...";
fil["127"]= "Install_Config/Install_Config_title.html@@@Installation and Configuration@@@...";
fil["128"]= "Install_Config/InstallationAndConfig.html@@@Installation@@@You can install Data Collector and start it manually or run it as a service...";
fil["129"]= "Install_Config/Installing_the_DC-Docker.html@@@Run Data Collector from Docker@@@You can run the Data Collector image from Docker Hub...";
fil["130"]= "Install_Config/Installing_the_DC.html@@@Full Installation and Launch (Manual Start)@@@To install the full Data Collector and start it manually, download the full Data Collector tarball...";
fil["131"]= "Install_Config/JMXMetrics-EnableExternalTools.html@@@Enabling External JMX Tools@@@Data Collector uses JMX metrics to generate the graphical display of the status of a running pipeline. You can provide the same JMX metrics to external tools if desired...";
fil["132"]= "Install_Config/LogLevel.html@@@Modifying the Log Level@@@If the Data Collector logs do not provide enough troubleshooting information, you can modify the log level to display messages at another severity level...";
fil["133"]= "Install_Config/MapR-Prerequisites.html@@@MapR Prerequisites@@@Due to licensing restrictions, StreamSets cannot distribute MapR libraries with Data Collector. As a result, you must perform additional steps to enable the Data Collector machine to connect to MapR...";
fil["134"]= "Install_Config/RuntimeProperties.html@@@Using Runtime Properties@@@Runtime properties are properties that you define in a file local to the Data Collector and call from within a pipeline. With runtime properties, you can define different sets of values for different Data Collectors...";
fil["135"]= "Install_Config/RuntimeResources.html@@@Using Runtime Resources@@@Similar to runtime properties, runtime resources are values that you define in a file local to the Data Collector and call from within a pipeline. But with runtime resources, you can restrict the permissions for the files to secure sensitive information. Use runtime resources to load sensitive information from files at runtime...";
fil["136"]= "Install_Config/Vault-Overview.html@@@Accessing Hashicorp Vault Secrets@@@Data Collector can access information, a.k.a. secrets, stored in Hashicorp Vault...";
fil["137"]= "Origins/AmazonS3.html@@@Amazon S3@@@The Amazon S3 origin uses a buffer to read objects into memory to produce records. The size of the buffer determines the maximum size of the record that can be processed...";
fil["138"]= "Origins/Directory.html@@@Directory@@@Configure a Directory origin to read data from files in a directory...";
fil["139"]= "Origins/FileTail.html@@@File Tail@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["140"]= "Origins/HTTPClient.html@@@HTTP Client@@@The HTTP Client origin processes data differently based on the data format. The origin processes the following types of dat...";
fil["141"]= "Origins/HadoopFS-origin.html@@@Hadoop FS@@@You can configure the Hadoop FS origin to use a Hadoop user to read data from HDFS...";
fil["142"]= "Origins/JDBCConsumer.html@@@JDBC Consumer@@@When reading from Microsoft SQL Server, JDBC Consumer can group row updates from the same transaction when reading from a change log table. This maintains consistency when performing change data capture...";
fil["143"]= "Origins/JMS.html@@@JMS Consumer@@@Configure a JMS Consumer origin to read JMS messages...";
fil["144"]= "Origins/KConsumer.html@@@Kafka Consumer@@@Configure a Kafka Consumer to read data from a Kafka cluster...";
fil["145"]= "Origins/KinConsumer.html@@@Kinesis Consumer@@@You can configure the read interval for the Kinesis Consumer. The read interval determines how long Kinesis Consumer waits before requesting additional data from Kinesis shards. By default, the Kinesis Consumer waits one second between requests...";
fil["146"]= "Origins/MapRFS.html@@@MapR FS@@@You can configure the MapR FS origin to use a Hadoop user to read files from MapR FS...";
fil["147"]= "Origins/MapRStreamsCons.html@@@MapR Streams Consumer@@@Configure a MapR Streams Consumer to read messages from MapR Streams...";
fil["148"]= "Origins/MongoDB.html@@@MongoDB@@@You can configure the read preference that the MongoDB origin uses. The read preference determines how the origin reads data from different members of the MongoDB replica set...";
fil["149"]= "Origins/MySQLBinaryLog.html@@@MySQL Binary Log@@@Configure a MySQL Binary Log origin to process change data capture (CDC) information provided by MySQL binary logs...";
fil["150"]= "Origins/Omniture.html@@@Omniture@@@The Omniture origin processes JSON website usage reports generated by the Omniture reporting APIs. Omniture is also known as the Adobe Marketing Cloud...";
fil["151"]= "Origins/OracleCDC.html@@@Oracle CDC Client@@@The Oracle CDC Client processes change data capture (CDC) information provided by Oracle LogMiner redo logs. The Oracle CDC Client origin produces records with a map of fields and record header...";
fil["152"]= "Origins/Origins_overview.html@@@Origins@@@You can preview raw source data for Directory, File Tail, and Kafka Consumer origins. Preview raw source data when reviewing the data might help with origin configuration...";
fil["153"]= "Origins/Origins_title.html@@@Origins@@@...";
fil["154"]= "Origins/RabbitMQ.html@@@RabbitMQ Consumer@@@Configure a RabbitMQ Consumer to read messages from a RabbitMQ queue...";
fil["155"]= "Origins/Redis.html@@@Redis Consumer@@@When you use an origin to read log data, you define the format of the log files to be read...";
fil["156"]= "Origins/SDCRPCtoKafka.html@@@SDC RPC to Kafka@@@Configure the SDC RPC to Kafka maximum batch request size and message size properties in relationship to each other and to the Kafka configuration for maximum message size...";
fil["157"]= "Origins/SDC_RPCorigin.html@@@SDC RPC@@@The SDC RPC origin enables connectivity between two SDC RPC pipelines. The SDC RPC origin reads data passed from an SDC RPC destination. Use the SDC RPC origin as part of an SDC RPC destination pipeline...";
fil["158"]= "Origins/SFTP.html@@@SFTP/FTP Client@@@If the remote server requires authentication, configure the authentication method that the origin must use to log in to the remote server...";
fil["159"]= "Origins/Salesforce.html@@@Salesforce@@@The Salesforce origin generates Salesforce header attributes that provide additional information about each record, such as the source objects for the record. The origin receives these details from Salesforce...";
fil["160"]= "Origins/UDP.html@@@UDP Source@@@The UDP Source origin reads messages from one or more UDP ports. UDP Source generates a record for every message. UDP Source can read collectd messages, Netflow messages from NetFlow Version 5, and...";
fil["161"]= "Origins/UDPtoKafka.html@@@UDP to Kafka@@@When you use a UDP to Kafka origin in a pipeline, connect the origin to a Trash destination...";
fil["162"]= "Pipeline_Configuration/ConfiguringAPipeline.html@@@Configuring a Pipeline@@@Configure a pipeline to define the stream of data. After you configure the pipeline, you can start the pipeline...";
fil["163"]= "Pipeline_Configuration/DataCollectorWindow-Config.html@@@Data Collector Console - Edit Mode@@@The following image shows the Data Collector console when you configure a pipeline: Area / Icon Name Description 1 Pipeline canvas Displays the pipeline. Use to configure the pipeline data flow. 2...";
fil["164"]= "Pipeline_Configuration/Expressions.html@@@Expression Configuration@@@Use the following information and tips when you invoke expression completion...";
fil["165"]= "Pipeline_Configuration/PipelineConfiguration_title.html@@@Pipeline Configuration@@@...";
fil["166"]= "Pipeline_Configuration/PipelineConstants.html@@@Pipeline Constants@@@A pipeline constant is a constant that you define for the pipeline and that you can use in any stage in the pipeline. Define a pipeline constant when you have a constant that you want to update easily or to use more than once...";
fil["167"]= "Pipeline_Configuration/PipelineMemory.html@@@Pipeline Memory@@@Data Collector uses memory when it runs a pipeline. To avoid causing out-of-memory errors on the host machine, you can configure the maximum amount of memory that can be used for the pipeline...";
fil["168"]= "Pipeline_Configuration/PipelineRateLimit.html@@@Rate Limit@@@You can limit the rate at which a pipeline processes records by defining the maximum number of records that the pipeline can read in a second...";
fil["169"]= "Pipeline_Configuration/Retry.html@@@Retrying the Pipeline@@@By default, when Data Collector encounters a stage-level error that might cause a standalone pipeline to fail, it retries the pipeline. That is, it waits a period of time, and then tries again to run the pipeline...";
fil["170"]= "Pipeline_Configuration/Validation.html@@@Implicit and Explicit Validation@@@Data Collector performs two types of validation: Implicit validation Implicit validation occurs by default as the Data Collector console saves your changes. Implicit validation lists missing or...";
fil["171"]= "Pipeline_Design/ControlCharacters.html@@@Control Character Removal@@@You can use several stages to remove control characters - such as escape or end-of-transmission characters - from data. Remove control characters to avoid creating invalid records...";
fil["172"]= "Pipeline_Design/DatainMotion.html@@@Data in Motion@@@When you configure a pipeline, you define how you want data to be treated: Do you want to prevent the loss of data or the duplication of dat...";
fil["173"]= "Pipeline_Design/DelimitedDataRootFieldTypes.html@@@Delimited Data Root Field Type@@@When reading delimited data, Data Collector can create records that use the list or list-map root field type. When Data Collector creates records for delimited data, it creates a single root field of...";
fil["174"]= "Pipeline_Design/DesigningDataFlow.html@@@Designing the Data Flow@@@When you connect a stage to multiple stages, all data passes to all connected stages. You can configure required fields for a stage to discard records before they enter the stage, but by default all records are passed...";
fil["175"]= "Pipeline_Design/DevStages.html@@@Development Stages@@@The Data Collector provides several development stages that you can use to help develop or test pipelines. Note: Do not use development stages in production pipelines. You can use the following...";
fil["176"]= "Pipeline_Design/DroppingUnwantedRecords.html@@@Dropping Unwanted Records@@@Preconditions are conditions that a record must satisfy to enter the stage for processing. Like required fields, if a record does not meet a precondition, it is diverted to the pipeline for error handling. You can define preconditions for any processor and most destination stages...";
fil["177"]= "Pipeline_Design/ErrorHandling.html@@@Error Record Handling@@@You can configure error record handling at a stage level and at a pipeline level...";
fil["178"]= "Pipeline_Design/PipelineDesign_title.html@@@Pipeline Concepts and Design@@@...";
fil["179"]= "Pipeline_Design/Protobuf-Prerequisites.html@@@Protobuf Data Format Prerequisites@@@Perform the following prerequisites before reading or writing protobuf dat...";
fil["180"]= "Pipeline_Design/SDCRecordFormat.html@@@SDC Record Data Format@@@SDC Record is a proprietary data format that Data Collector uses to generate error records. Data Collector can also use the data format to read and write dat...";
fil["181"]= "Pipeline_Design/TextCDelim.html@@@Text Data Format with Custom Delimiters@@@By default, the text data format creates records based on line breaks, creating a record for each line of text. You can configure origins to create records based on custom delimiters. Use custom...";
fil["182"]= "Pipeline_Design/What_isa_Pipeline.html@@@What is a Pipeline?@@@A pipeline describes the flow of data from the origin system to destination systems and defines how to transform the data along the way...";
fil["183"]= "Pipeline_Design/WholeFile.html@@@Whole File Data Format@@@You can use the whole file data format to move entire files from an origin system to a destination system. With the whole file data format, you can transfer any type of file. You cannot perform...";
fil["184"]= "Pipeline_Maintenance/AddingLabelsPipelines.html@@@Adding Labels to Pipelines@@@You can add labels to pipelines to group similar pipelines. For example, you might want to group pipelines by database schema or by the test or production environment. You can add labels to pipelines...";
fil["185"]= "Pipeline_Maintenance/DataCollectorWindow_AllPipelines.html@@@Data Collector Console - All Pipelines on the Home Page@@@You can display all pipelines in the Data Collector console when you click the Home icon or the Pipelines link in the pipeline path when configuring or monitoring a pipeline. View all pipelines on the Home page to perform pipeline maintenance...";
fil["186"]= "Pipeline_Maintenance/DeletingAPipeline.html@@@Deleting Pipelines@@@You can delete pipelines when you no longer need them. Deleting pipelines is permanent. To keep backups, export the pipelines before you delete them. From the Home page, select pipelines in the list...";
fil["187"]= "Pipeline_Maintenance/DuplicatingAPipeline.html@@@Duplicating a Pipeline@@@Duplicate a pipeline when you want to keep the existing version of a pipeline while continuing to configure a duplicate version. A duplicate is an exact copy of the original pipeline...";
fil["188"]= "Pipeline_Maintenance/ExportingPipelines.html@@@Exporting Pipelines@@@Export pipelines to create backups or to use the pipelines with another Data Collector...";
fil["189"]= "Pipeline_Maintenance/ImportingPipelines.html@@@Importing Pipelines@@@Import pipelines to use pipelines developed on a different Data Collector or to restore backup files. Import pipelines from pipeline files. Pipeline files are JSON files exported from a Data Collector...";
fil["190"]= "Pipeline_Maintenance/PipelineMaintenance_title.html@@@Pipeline Maintenance@@@...";
fil["191"]= "Pipeline_Maintenance/StartingPipelines.html@@@Starting Pipelines@@@You can start pipelines when they are valid. When you start a pipeline, Data Collector runs the pipeline until you stop the pipeline or shut down Data Collector...";
fil["192"]= "Pipeline_Maintenance/StoppingPipelines.html@@@Stopping Pipelines@@@Stop pipelines when you want Data Collector to stop processing data for the pipelines...";
fil["193"]= "Pipeline_Monitoring/DataCollectorWindow-Monitor.html@@@Data Collector Console - Monitor Mode@@@In Monitor mode, you can use the Data Collector console to view data as it passes through the pipeline...";
fil["194"]= "Pipeline_Monitoring/MonitoringErrors.html@@@Monitoring Errors@@@You can view the errors related to each stage. Stage-related errors include the error records that the stage produces and other errors encountered by the stage...";
fil["195"]= "Pipeline_Monitoring/PipelineMonitoring.html@@@Pipeline Monitoring@@@When the Data Collector runs a pipeline, you can view real-time statistics about the pipeline, examine a sample of the data being processed, and create rules and alerts...";
fil["196"]= "Pipeline_Monitoring/PipelineMonitoring_title.html@@@Pipeline Monitoring@@@...";
fil["197"]= "Pipeline_Monitoring/Snapshots.html@@@Snapshots@@@A snapshot is a set of data captured as it moves through a running pipeline. You can capture snapshots when you monitor a pipeline...";
fil["198"]= "Pipeline_Monitoring/ViewingPipelineStageStatistics.html@@@Viewing Pipeline and Stage Statistics@@@When you monitor a pipeline, you can view real-time summary and error statistics for the pipeline and for stages in the pipeline...";
fil["199"]= "Pipeline_Monitoring/ViewingtheRunHistory.html@@@Viewing the Run History@@@You can view a run summary for each run of the pipeline when you view the pipeline history...";
fil["200"]= "Processors/Base64Decoder.html@@@Base64 Field Decoder@@@The Base64 Field Decoder decodes Base64 encoded data to binary data. Use the processor to decode Base64 encoded data before evaluating data in the field...";
fil["201"]= "Processors/Base64Encoder.html@@@Base64 Field Encoder@@@The Base64 Field Encoder encodes binary data using Base64. Use the processor to encode binary data that must be sent over channels that expect ASCII dat...";
fil["202"]= "Processors/Expression.html@@@Expression Evaluator@@@You can use expressions to add or modify header attributes for a record...";
fil["203"]= "Processors/FieldFlattener.html@@@Field Flattener@@@The Field Flattener flattens list and map fields. The processor can flatten the entire record to produce a record with no nested fields. Or it can flatten specific list or map fields. Use the Field...";
fil["204"]= "Processors/FieldHasher.html@@@Field Hasher@@@Field Hasher provides several methods to hash data. When you hash a field more than once, Field Hasher uses the existing hash when generating the next hash...";
fil["205"]= "Processors/FieldMasker.html@@@Field Masker@@@You can use the following mask types to mask dat...";
fil["206"]= "Processors/FieldMerger.html@@@Field Merger@@@The Field Merger merges one or more fields in a record to a different location in the record. Use only for records with a list or map structure...";
fil["207"]= "Processors/FieldRemover.html@@@Field Remover@@@The Field Remover removes fields from records. Use the Field Remover to discard field data that you do not need in the pipeline. You configure the Field Remover to complete one of the following...";
fil["208"]= "Processors/FieldRenamer.html@@@Field Renamer@@@Use the Field Renamer to rename fields in a record. You can specify individual fields to rename or use regular expressions to rename sets of fields...";
fil["209"]= "Processors/FieldSplitter.html@@@Field Splitter@@@The Field Splitter splits string data based on a regular expression and passes the separated data to new fields. Use the Field Splitter to split complex string values into logical components...";
fil["210"]= "Processors/FieldTypeConverter.html@@@Field Type Converter@@@You can use the Field Type Converter to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you d like to decrease the scale to 4 so that the value is 12345.6789...";
fil["211"]= "Processors/GeoIP.html@@@Geo IP@@@The Geo IP processor is a lookup processor that can return geolocation and IP intelligence information for a specified IP address. The Geo IP processor uses MaxMind GeoIP2 database files for the...";
fil["212"]= "Processors/Groovy.html@@@Groovy Evaluator@@@In scripts that process list-map data, treat the data as maps...";
fil["213"]= "Processors/HBaseLookup.html@@@HBase Lookup@@@Configure an HBase Lookup processor to perform key-value lookups in HBase...";
fil["214"]= "Processors/HTTPClient.html@@@HTTP Client@@@Configure an HTTP Client processor to perform requests against a resource URL...";
fil["215"]= "Processors/HiveMetadata.html@@@Hive Metadata@@@You must configure Hive Metadata to use Hive and Hadoop configuration files and individual properties...";
fil["216"]= "Processors/JDBCLookup.html@@@JDBC Lookup@@@The JDBC Lookup processor uses a JDBC connection to perform lookups in a database table and pass the lookup values to fields. Use the JDBC Lookup to enrich records with additional dat...";
fil["217"]= "Processors/JDBCTee.html@@@JDBC Tee@@@The JDBC Tee processor uses a JDBC connection to write data to a database table, and then pass generated database column values to fields. Use the JDBC Tee to write some or all record fields to a database table and then enrich records with additional dat...";
fil["218"]= "Processors/JSONParser.html@@@JSON Parser@@@Configure a JSON Parser to parse a JSON object in a String field...";
fil["219"]= "Processors/JavaScript.html@@@JavaScript Evaluator@@@In scripts that process list-map data, treat the data as maps...";
fil["220"]= "Processors/Jython.html@@@Jython Evaluator@@@In scripts that process list-map data, treat the data as maps...";
fil["221"]= "Processors/ListPivoter.html@@@Field Pivoter@@@Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field...";
fil["222"]= "Processors/LogParser.html@@@Log Parser@@@When you use Log Parser to parse log data, you define the format of the log files to be read...";
fil["223"]= "Processors/Processors_overview.html@@@Processors@@@A processor stage represents a type of data processing that you want to perform. You can use as many processors in a pipeline as you need...";
fil["224"]= "Processors/Processors_title.html@@@Processors@@@...";
fil["225"]= "Processors/RDeduplicator.html@@@Record Deduplicator@@@The Record Deduplicator caches record information for comparison until it reaches a specified number of records. Then, it discards the information in the cache and starts over...";
fil["226"]= "Processors/RedisLookup.html@@@Redis Lookup@@@Configure a Redis Lookup processor to perform key-value lookups in Redis...";
fil["227"]= "Processors/Spark.html@@@Spark Evaluator@@@The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. Use the Spark Evaluator processor in standalone pipelines only...";
fil["228"]= "Processors/StaticLookup.html@@@Static Lookup@@@Configure a Static Lookup processor to perform key-value lookups in memory...";
fil["229"]= "Processors/StreamSelector.html@@@Stream Selector@@@A condition defines the data that passes to the associated stream. All records that meet the condition pass to the stream. Use the expression language to define conditions...";
fil["230"]= "Processors/ValueReplacer.html@@@Value Replacer@@@The Value Replacer replaces values in fields. You can use the Value Replacer to replace null values in specified fields with constants, replace existing field values with nulls, and replace specified...";
fil["231"]= "Processors/XMLFlattener.html@@@XML Flattener@@@Configure an XML Flattener to flatten XML data embedded in a string field...";
fil["232"]= "Processors/XMLParser.html@@@XML Parser@@@Configure an XML Parser to parse XML data in a string field...";
fil["233"]= "RPC_Pipelines/ConfiguringDeliveryGuarantee.html@@@Configuring the Delivery Guarantee@@@The delivery guarantee determines when a pipeline commits the offset. When configuring the delivery guarantee for SDC RPC pipelines, use the same option in origin and destination pipelines...";
fil["234"]= "RPC_Pipelines/ConfiguringSDCRPCPipelines.html@@@Configuration Guidelines for SDC RPC Pipelines@@@To create a valid set of SDC RPC pipelines, some configuration options must be aligned. Use the following guidelines to configure SDC RPC pipelines: origin pipeline In the origin pipeline, configure...";
fil["235"]= "RPC_Pipelines/Deployment_Architecture.html@@@Deployment Architecture@@@When using SDC RPC pipelines, consider your needs and environment carefully as you design the deployment architecture...";
fil["236"]= "RPC_Pipelines/EnablingEncryption.html@@@Enabling Encryption@@@You can enable SDC RPC pipelines to transfer data securely using TLS. To use TLS, enable TLS in both the SDC RPC destination and the SDC RPC origin...";
fil["237"]= "RPC_Pipelines/RPC_ID.html@@@Defining the RPC ID@@@The RPC ID is a user-defined identifier that allows an SDC RPC origin and SDC RPC destination to recognize each other...";
fil["238"]= "RPC_Pipelines/SDC_RPCpipeline.html@@@SDC RPC Pipelines@@@Data Collector Remote Protocol Call pipelines, a.k.a. SDC RPC pipelines, are a set of StreamSets pipelines that pass data from one pipeline to another without writing to an intermediary system...";
fil["239"]= "RPC_Pipelines/SDC_RPCpipelines_title.html@@@SDC RPC Pipelines@@@...";
fil["240"]= "Reusable_Content/Reusable_Topics/Kafka-EnablingSecurity.html@@@Enabling Security@@@When using Kafka version 0.9.0.0 or later , you can configure the Kafka Producer to connect securely through SSL, Kerberos, or both. These versions provide features to support secure connections...";
fil["241"]= "Troubleshooting/AccessingErrorMessages.html@@@Accessing Error Messages@@@Informational and error messages display in different locations based on the type of information: Pipeline configuration issues The Data Collector console provides guidance and error details in the...";
fil["242"]= "Troubleshooting/ClusterMode.html@@@Cluster Execution Mode@@@Use the following tips for help with pipelines in cluster mode: I got the following validation error when configuring a cluster pipeline. What does it mean? Validation_0071 - Stage  &lt;stage id&gt;  does...";
fil["243"]= "Troubleshooting/Destinations.html@@@Destinations@@@Use the following tips for help with destination stages and systems. Why is the pipeline failing entire batches when only a few records have a problem? Due to Cassandra requirements, when you write to...";
fil["244"]= "Troubleshooting/Monitoring.html@@@Monitoring@@@Use the following tips for help with monitoring the pipeline...";
fil["245"]= "Troubleshooting/Origins.html@@@Origins@@@Use the following tips for help with origin stages and systems. Why isn t the Directory origin reading all of my files? Directory reads a set of files based on the configured file name pattern, read...";
fil["246"]= "Troubleshooting/Performance.html@@@Performance@@@Use the following tips for help with performance: Why is my batch size only 1000 records when I configured my origin for larger batches? The Data Collector maximum batch size overrides the maximum...";
fil["247"]= "Troubleshooting/PipelineBasics.html@@@Pipeline Basics@@@Use the following tips for help with pipeline basics: When I go to the Data Collector console, I get a  Webpage not available  error message. The Data Collector is not running. Start the Dat...";
fil["248"]= "Troubleshooting/Troubleshooting_title.html@@@Troubleshooting@@@...";
fil["249"]= "Tutorial/BasicTutorial.html@@@Basic Tutorial@@@Now before we run the basic pipeline, let s add a data rule and alert. Data rules are user-defined rules used to inspect data moving between two stages. They are a powerful way to look for outliers and anomalous dat...";
fil["250"]= "Tutorial/BeforeYouBegin.html@@@Before You Begin@@@Before you start this tutorial, you ll need to do a few things: Download sample data. You can download sample data from the following location...";
fil["251"]= "Tutorial/ExtendedTutorial.html@@@Extended Tutorial@@@Now that the extended pipeline is complete, let s reset the origin and run the pipeline again...";
fil["252"]= "Tutorial/Overview.html@@@Tutorial Overview@@@This tutorial walks through creating and running a pipeline. You can download sample data so you can perform data preview, run the completed pipeline, and monitor the results...";
fil["253"]= "Tutorial/Tutorial-title.html@@@Tutorial@@@...";
fil["254"]= "Upgrade/CMUpgrade.html@@@Upgrade an Installation with Cloudera Manager@@@After you add the StreamSets repository to Cloudera Manager, you can download and distribute the new StreamSets parcel across the cluster. Stop the StreamSets service and deactivate the previous parcel before you activate the new parcel...";
fil["255"]= "Upgrade/PostUpgrade.html@@@Post Upgrade Tasks@@@In some situations, you must complete tasks within Data Collector after you upgrade...";
fil["256"]= "Upgrade/RPM.html@@@Upgrade an Installation from the RPM Package@@@If you installed the core RPM package, install the individual stage libraries that the upgraded pipelines require...";
fil["257"]= "Upgrade/Tarball.html@@@Upgrade an Installation from the Tarball@@@Install the new version of the tarball...";
fil["258"]= "Upgrade/Upgrade.html@@@Upgrade@@@You can upgrade a previous version of Data Collector to a new version. You can upgrade an installation from the tarball, an installation from the RPM package, or an installation with Cloudera Manager...";
fil["259"]= "Upgrade/Upgrade_title.html@@@Upgrade@@@...";
