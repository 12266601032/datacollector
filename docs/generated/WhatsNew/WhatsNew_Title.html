
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="What's New" /><meta name="abstract" content="" /><meta name="description" content="Data Collector version 2.2.1.0 includes the following new features and enhancements: Processors New Field Zip processor - Merges two List fields or two List-Map fields in the same record. New ..." /><meta name="DC.Relation" scheme="URI" content="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" /><meta name="DC.Relation" scheme="URI" content="../Install_Config/Install_Config_title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hz3_5fk_fy" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>What's New</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Install_Config/Install_Config_title.html" title="Installation and Configuration"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation and Configuration</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_hz3_5fk_fy">
 <h1 class="title topictitle1">What's New</h1>

 
 <div class="body conbody"><p class="shortdesc"></p>

  <p class="p"></p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_wbf_dgk_fy">
 <h2 class="title topictitle2">What's New in 2.2.1.0</h2>

 <div class="body conbody">
        <div class="p"><span class="ph">Data
                  Collector</span>
            version 2.2.1.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_y1c_1pm_3y">
                            <li class="li">New <a class="xref" href="../Processors/FieldZip.html#concept_o3b_t1k_yx">Field Zip processor</a> - Merges two List fields or two
                                List-Map fields in the same record.</li>

                            <li class="li">New <a class="xref" href="../Processors/SalesforceLookup.html#concept_k23_3rk_yx" title="The Salesforce Lookup processor performs lookups in a Salesforce object and passes the lookup values to fields. Use the Salesforce Lookup to enrich records with additional data.">Salesforce Lookup processor</a> - Performs lookups in a
                                Salesforce object and passes the lookup values to fields. Use the
                                Salesforce Lookup to enrich records with additional data.</li>

                            <li class="li"><a class="xref" href="../Processors/ValueReplacer.html#concept_ppg_ztk_3y">Value Replacer</a> enhancement - You can now replace field
                                values with nulls using a condition.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_wbf_dgk_fy__ul_vlq_dpm_3y">
                            <li class="li"><a class="xref" href="../Destinations/DataLakeStore.html#concept_wsz_qj4_zx">Whole file support in the Azure Data Lake Store
                                    destination</a> - You can now use the whole file data format
                                to stream whole files to Azure Data Lake Store. </li>

                        </ul>

                    </dd>

                
            </dl>
</div>

    </div>

</div>
<div class="topic concept nested1" id="concept_oyv_zfk_fy">
 <h2 class="title topictitle2">What's New in 2.2.0.0</h2>

 <div class="body conbody">
  <div class="p"><span class="ph">Data
                  Collector</span> version
            2.2.0.0 includes the following new features and enhancements:<dl class="dl">
                
                    <dt class="dt dlterm">Event Framework</dt>

                    <dd class="dd">The Data Collector event framework enables the pipeline to trigger tasks in
                        external systems based on actions that occur in the pipeline, such as
                        running a MapReduce job after the pipeline writes a file to HDFS. You can
                        also use the event framework to store event information, such as when an
                        origin starts or completes reading a file. </dd>

                    <dd class="dd">For details, see the <a class="xref" href="../Event_Handling/EventFramework-Title.html#concept_cph_5h4_lx">Event Framework chapter</a>. </dd>

                    <dd class="dd">The event framework includes the following new features and enhancements:<ul class="ul" id="concept_oyv_zfk_fy__ul_ojf_xgk_fy">
                            <li class="li"><a class="xref" href="../Executors/Executors-overview.html#concept_stt_2lk_fx">New executor stages</a>. A new type of stage that performs
                                tasks in external systems upon receiving an event. This release
                                includes the following executors:<ul class="ul" id="concept_oyv_zfk_fy__ul_bn4_ygk_fy">
                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_wgj_slk_fx">HDFS File Metadata executor</a> - Changes file
                                        metadata such as the name, location, permissions, and ACLs. </li>

                                    <li class="li"><a class="xref" href="../Executors/HiveQuery.html#concept_kjw_llk_fx">Hive Query executor</a> - Runs a Hive or Impala
                                        query. </li>

                                    <li class="li"><a class="xref" href="../Executors/JDBCQuery.html#concept_j3r_gcv_sx">JDBC Query executor</a> - Runs a SQL query. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_bj2_zlk_fx">MapReduce executor</a> - Runs a custom MapReduce job
                                        or an Avro to Parquet MapReduce job. </li>

                                </ul>
</li>

                            <li class="li">Event generation. The following stages now generate events that you
                                can use in a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_sxd_bhk_fy">
                                    <li class="li"><a class="xref" href="../Origins/Directory.html#concept_ttg_vgn_qx">Directory</a> and <a class="xref" href="../Origins/FileTail.html#concept_gwn_c32_px">File Tail</a> origins - Generate events when they
                                        start and complete reading a file.</li>

                                    <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#concept_aqq_tt2_px">Amazon S3 destination</a> - Generates events when it
                                        completes writing to an object or streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Destinations/HadoopFS-destination.html#concept_bvb_rxj_px">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_in1_fcm_px">Local FS</a>, and <a class="xref" href="../Destinations/MapRFS.html#concept_bqd_3qb_rx">MapR FS</a> destinations - Generate events when they
                                        close an output file or complete streaming a whole file. </li>

                                    <li class="li"><a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors - Can run scripts
                                        that generate events. </li>

                                    <li class="li"><a class="xref" href="../Executors/HDFSMetadata.html#concept_vhl_mfj_rx">HDFS File Metadata executor</a> - Generates events
                                        when it changes file metadata. </li>

                                    <li class="li"><a class="xref" href="../Executors/MapReduce.html#concept_e1s_sm5_sx">MapReduce executor</a> - Generates events when it
                                        starts a MapReduce job.</li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Dev stages</a>. You can use the following stages to develop
                                and test event handling: <ul class="ul" id="concept_oyv_zfk_fy__ul_ysc_2hk_fy">
                                    <li class="li">Dev Data Generator enhancement - You can now configure the
                                        Dev Data Generator to generate event records as well as data
                                        records. You can also specify the number of records in a
                                        batch. </li>

                                    <li class="li">To Event - Generates event records using the incoming record
                                        as the body of the event record.</li>

                                </ul>
</li>

                        </ul>
</dd>

                
                
                    <dt class="dt dlterm">Installation</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_q3x_pvm_3y">
                            <li class="li"><a class="xref" href="../Install_Config/InstallationAndConfig.html#concept_vzg_n2p_kq" title="Install Data Collector on a machine that meets the following minimum requirements. To run pipelines in cluster execution mode, each node in the cluster must meet the minimum requirements.">Installation requirements</a>:<ul class="ul" id="concept_oyv_zfk_fy__ul_zh5_qvm_3y">
                                    <li class="li">Java requirement - Oracle Java 7 is supported but now
                                        deprecated. Oracle announced the end of public updates for
                                        Java 7 in April 2015. StreamSets recommends migrating to
                                        Java 8, as Java 7 support will be removed in a future Data
                                        Collector release. </li>

                                    <li class="li">File descriptors requirement - Data Collector now requires a
                                        minimum of 32,768 open file descriptors. </li>

                                </ul>
</li>

                        </ul>

                        <ul class="ul" id="concept_oyv_zfk_fy__ul_jtf_ghk_fy">
                            <li class="li"><a class="xref" href="../Install_Config/CoreInstall_Overview.html#concept_vvw_p3m_s5" title="You can download and install a core version of Data Collector, and then install individual stage libraries as needed. Use the core installation to install only the stage libraries that you want to use. The core installation allows Data Collector to use less disk space.">Core installation</a> includes the basic stage library only
                                - The core RPM and tarball installations now include the basic stage
                                library only, to allow Data Collector to use less disk space.
                                Install additional stage libraries using the Package Manager for
                                tarball installations or the command line for RPM and tarball
                                installations. <p class="p">Previously, the core installation also included
                                    the Groovy, Jython, and statistics stage libraries.</p>
</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Configuration</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5w_mhk_fy">
                            <li class="li">New <a class="xref" href="../Install_Config/AdditionalStageLibraries.html#concept_zhl_k4f_cr" title="The Data Collector includes a wide range of stages for pipeline development, but you might need additional functionality. You can install stage libraries to provide access to additional stages.">stage libraries</a>. Data Collector now supports the
                                following stage libraries: <ul class="ul" id="concept_oyv_zfk_fy__ul_oxv_nhk_fy">
                                    <li class="li">Apache Kudu version 1.0.x - Earlier Kudu versions are no
                                        longer supported. </li>

                                    <li class="li">Cloudera CDH version 5.9 distribution of Apache Hadoop. </li>

                                    <li class="li">Cloudera version 5.9 distribution of Apache Kafka 2.0. </li>

                                    <li class="li">Elasticsearch version 5.0.x. </li>

                                    <li class="li">Google Cloud Bigtable. </li>

                                    <li class="li">Hortonworks HDP version 2.5 distribution of Apache Hadoop. </li>

                                    <li class="li">MySQL Binary Log. </li>

                                    <li class="li">Salesforce. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Install_Config/Authentication.html#concept_x2j_5ts_g5" title="If your organization uses LDAP, configure Data Collector to use LDAP authentication. After you configure LDAP authentication, users log in to Data Collector using their LDAP username and password.">LDAP authentication</a> - If you use LDAP authentication,
                                you can now configure Data Collector to connect to multiple LDAP
                                servers. You can also configure Data Collector to support an LDAP
                                deployment where members are defined by uid or by full DN. </li>

                            <li class="li"><a class="xref" href="../Install_Config/DCEnvironmentConfig.html#concept_kqh_lj3_vx" title="You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.">Java garbage collector</a> - Data Collector now uses the
                                Concurrent Mark Sweep (CMS) garbage collector by default. You can
                                configure Data Collector to use a different garbage collector by
                                modifying Java configuration options in the Data Collector
                                environment configuration file.</li>

                            <li class="li">Environment variables for <a class="xref" href="../Install_Config/DCEnvironmentConfig.html#concept_vrx_4fg_qr" title="You can define the Data Collector Java heap size. By default, the Java heap size is 1024 MB. You can enable remote debugging to debug a Data Collector instance running on a remote machine. To enable remote debugging, define debugging options in the SDC_JAVA_OPTS environment variable in the Data Collector environment configuration file.You can define the Java garbage collector that Data Collector uses. By default, Data Collector uses the Concurrent Mark Sweep (CMS) garbage collector.When you use Data Collector with Java 7, Data Collector is configured to use TLS versions 1.1 and 1.2. To connect to a system that uses an earlier version of TLS, modify the Dhttps.protocols option in the SDC_JAVA7_OPTS environment variable in the Data Collector environment configuration file.When you use Data Collector with Java 7, you can define the Java Permanent Generation size, also known as the PermGen size.">Java configuration options</a>. Data Collector now uses
                                three environment variables to define Java configuration options:
                                    <ul class="ul" id="concept_oyv_zfk_fy__ul_wym_thk_fy">
                                    <li class="li">SDC_JAVA_OPTS - Includes configuration options for all Java
                                        versions. SDC_JAVA7_OPTS - Includes configuration options
                                        used only when Data Collector is running Java 7.</li>

                                    <li class="li">SDC_JAVA8_OPTS - Includes configuration options used only
                                        when Data Collector is running Java 8. </li>

                                </ul>
</li>

                            <li class="li"><a class="xref" href="../Getting_Started/GettingStarted_Title.html#task_r3q_fnx_pr">New time zone property</a> - You can configure the Data
                                Collector console to use UTC, the browser time zone, or the Data
                                Collector time zone. The time zone property affects how dates and
                                times display in the UI. The default is the browser time zone.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Origins</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_kq1_whk_fy">
                            <li class="li">New <a class="xref" href="../Origins/MySQLBinaryLog.html#concept_kqg_1yh_xx" title="The MySQL Binary Log origin processes change data capture (CDC) information provided by MySQL server in binary logs. The origin produces records with a map of fields of CDC information.">MySQL Binary Log origin</a> - Reads MySQL binary logs to
                                generate records with change data capture information. </li>

                            <li class="li">New <a class="xref" href="../Origins/Salesforce.html#concept_odf_vr3_rx" title="The Salesforce origin reads data from Salesforce.">Salesforce origin </a>- Reads data from Salesforce. The
                                origin can execute a SOQL query to read existing data from
                                Salesforce. The origin can also subscribe to the Force.com Streaming
                                API to receive notifications for changes to Salesforce data. </li>

                            <li class="li"><a class="xref" href="../Origins/Directory.html#concept_qpt_rg3_cy">Directory origin</a> enhancement - You can configure the
                                Directory origin to read files from all subdirectories when using
                                the last-modified timestamp for the read order. </li>

                            <li class="li"><a class="xref" href="../Origins/JDBCConsumer.html#task_ryz_tkr_bs">JDBC Consumer</a> and <a class="xref" href="../Origins/OracleCDC.html#task_ehh_mjj_tw">Oracle CDC Client</a> origin enhancement - You can now
                                configure the transaction isolation level that the JDBC Consumer and
                                Oracle CDC Client origins use to connect to the database.
                                Previously, the origins used the default transaction isolation level
                                configured for the database.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Processors</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_wfg_13k_fy">
                            <li class="li">New <a class="xref" href="../Processors/Spark.html#concept_cpx_1lm_zx" title="The Spark Evaluator performs custom processing within a pipeline based on a Spark application that you develop. Use the Spark Evaluator processor in standalone pipelines only.">Spark
                                    Evaluator processor</a> - Processes data based on a Spark
                                application that you develop. Use the Spark Evaluator processor to
                                develop a Spark application that performs custom processing within a
                                pipeline. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldFlattener.html#concept_vpx_zc1_xx">Field Flattener processor</a> enhancements - In addition to
                                flattening the entire record, you can also now use the Field
                                Flattener processor to flatten specific list or map fields in the
                                record. </li>

                            <li class="li"><a class="xref" href="../Processors/FieldTypeConverter.html#concept_sym_c4g_xx" title="You can use the Field Type Converter to change the scale of decimal fields. For example, you might have a decimal field with the value 12345.6789115, and you'd like to decrease the scale to 4 so that the value is 12345.6789.">Field Type Converter processor</a> enhancements - You can
                                now use the Field Type Converter processor to change the scale of a
                                decimal field. Or, if you convert a field with another data type to
                                the Decimal data type, you can configure the scale to use in the
                                conversion. </li>

                            <li class="li"><a class="xref" href="../Processors/ListPivoter.html#concept_ekg_313_qw" title="Configure a Field Pivoter to pivot data in a list, map, or list-map field and generate a record for each item in the field.">Field
                                    Pivoter processor</a> enhancements - The List Pivoter
                                processor has been renamed to the Field Pivoter processor. You can
                                now use the processor to pivot data in a list, map, or list-map
                                field. You can also use the processor to save the field name of the
                                first-level item in the pivoted field. </li>

                            <li class="li"><a class="xref" href="../Processors/JDBCLookup.html#task_kbr_2cy_hw">JDBC Lookup</a> and <a class="xref" href="../Processors/JDBCTee.html#task_qpj_ncy_hw">JDBC Tee</a> processor enhancement - You can now configure
                                the transaction isolation level that the JDBC Lookup and JDBC Tee
                                processors use to connect to the database. Previously, the origins
                                used the default transaction isolation level configured for the
                                database. </li>

                            <li class="li">Scripting processor enhancements - The <a class="xref" href="../Processors/Groovy.html#concept_qcz_ssq_1y">Groovy Evaluator</a>, <a class="xref" href="../Processors/JavaScript.html#concept_mkv_wgh_cy">JavaScript Evaluator</a>, and <a class="xref" href="../Processors/Jython.html#concept_zhd_chh_cy">Jython Evaluator</a> processors can generate event records
                                and work with record header attributes. The sample scripts now
                                include examples of both and a new tip for generating unique record
                                IDs. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLFlattener.html#task_pmb_l55_sv" title="Configure an XML Flattener to flatten XML data embedded in a string field.">XML Flattener processor</a> enhancement - You can now
                                configure the XML Flattener processor to write the flattened data to
                                a new output field. Previously, the processor wrote the flattened
                                data to the same field. </li>

                            <li class="li"><a class="xref" href="../Processors/XMLParser.html#concept_dtt_q5q_k5" title="Configure an XML Parser to parse XML data in a string field.">XML
                                    Parser processor</a> enhancement. You can now generate
                                records from XML documents using simplified XPath expressions. This
                                enables reading records from deeper within XML documents. </li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Destination</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_v5j_l3k_fy">
                            <li class="li">New <a class="xref" href="../Destinations/DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store destination</a> - Writes data to
                                Microsoft Azure Data Lake Store. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Bigtable.html#concept_pl5_tmq_tx" title="The Google Bigtable destination writes data to Google Cloud Bigtable.">Google Bigtable destination</a> - Writes data to Google
                                Cloud Bigtable. </li>

                            <li class="li">New <a class="xref" href="../Destinations/Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce destination</a> - Writes data to Salesforce. New
                                Wave Analytics destination. Writes data to Salesforce Wave
                                Analytics. The destination creates a dataset with external data. </li>

                            <li class="li"><a class="xref" href="../Destinations/AmazonS3.html#task_pxb_j3r_rt">Amazon S3 destination</a> change - The AWS KMS Key ID
                                property has been renamed AWS KMS Key ARN. Data Collector upgrades
                                existing pipelines seamlessly. </li>

                            <li class="li">File suffix enhancement. You can now configure a file suffix, such
                                as txt or json, for output files generated by <a class="xref" href="../Destinations/HadoopFS-destination.html#concept_awl_4km_zq" title="You can use Kerberos authentication to connect to HDFS. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to HDFS. You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS.">Hadoop FS</a>, <a class="xref" href="../Destinations/LocalFS.html#concept_zvc_bv5_1r">Local
                                    FS</a>, <a class="xref" href="../Destinations/MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR
                                    FS</a>, and the <a class="xref" href="../Destinations/AmazonS3.html#concept_avx_bnq_rt" title="The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination.">Amazon
                                    S3</a> destinations. </li>

                            <li class="li"><a class="xref" href="../Destinations/JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer</a> destination enhancement - You can now
                                configure the transaction isolation level that the JDBC Producer
                                destination uses to connect to the database. Previously, the
                                destination used the default transaction isolation level configured
                                for the database. </li>

                            <li class="li"><a class="xref" href="../Destinations/Kudu.html#concept_dvg_vvj_wx" title="To write to Kudu, you configure the destination to perform one of the following write operations: insert, update, delete, or upsert. You define the default operation for the destination. You can also define the operation in a record header attribute.">Kudu destination</a> enhancement - You can now configure the
                                destination to perform one of the following write operations:
                                insert, update, delete, or upsert.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Data Formats</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_r3d_dkk_fy">
                            <li class="li"><a class="xref" href="../Pipeline_Design/XMLDFormat.html#concept_lty_42b_dy">XML processing</a> enhancement - You can now generate
                                records from XML documents using <a class="xref" href="../Pipeline_Design/XMLDFormat.html#concept_zw2_mfk_dy">simplified XPath expressions</a> with origins that process
                                XML data and the XML Parser processor. This enables reading records
                                from deeper within XML documents. </li>

                            <li class="li">Consolidated data format properties - You now configure the data
                                format and related properties on a new Data Format tab. Previously,
                                data formats had individual configuration tabs, e.g., Avro,
                                Delimited, Log. <p class="p">Related properties, such as Charset, Compression
                                    Format, and Ignore Control Characters now appear on the Data
                                    Format tab as well. </p>
</li>

                            <li class="li"><a class="xref" href="../Pipeline_Design/WholeFile.html#concept_ojv_sr4_vx">Checksum generation for whole files</a> - Destinations that
                                stream whole files can now generate checksums for the files so you
                                can confirm the accurate transmission of the file.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Pipeline Maintenance</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_h24_3kk_fy">
                            <li class="li">Add labels to pipelines from the Home page - You can now add labels
                                to multiple pipelines from the Data Collector Home page. Use labels
                                to group similar pipelines. For example, you might want to group
                                pipelines by database schema or by the test or production
                                environment. </li>

                            <li class="li">Reset the origin for multiple pipelines from the Home page - You can
                                now reset the origin for multiple pipelines at the same time from
                                the Data Collector Home page.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Rules and Alerts</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_sfk_lkk_fy">
                            <li class="li"><a class="xref" href="../Alerts/RulesAlerts_title.html#concept_ky2_g4f_qv" title="The gauge metric type provides alerts based on the number of input, output, or error records for the last processed batch. It also provides alerts on the age of the current batch, the amount of time a stage takes to process a batch, or the time that Data Collector last received a record from the origin.">Metric
                                    rules and alerts</a> enhancements - The gauge metric type can
                                now provide alerts based on the number of input, output, or error
                                records for the last processed batch.</li>

                        </ul>

                    </dd>

                
                
                    <dt class="dt dlterm">Expression Language Functions</dt>

                    <dd class="dd">
                        <ul class="ul" id="concept_oyv_zfk_fy__ul_bwr_nkk_fy">
                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_kxj_nyl_5x">file functions </a>- You can use the following new file
                                functions to work with file paths:<ul class="ul" id="concept_oyv_zfk_fy__ul_xdq_4kk_fy">
                                    <li class="li">file:fileExtension(&lt;filepath&gt;) - Returns the file
                                        extension from a path. </li>

                                    <li class="li">file:fileName(&lt;filepath&gt;) - Returns a file name from a
                                        path. </li>

                                    <li class="li">file:parentPath(&lt;filepath&gt;) - Returns the parent path of
                                        the specified file or directory. </li>

                                    <li class="li">file:pathElement(&lt;filepath&gt;, &lt;integer&gt;) - Returns the
                                        portion of the file path specified by a positive or negative
                                        integer. </li>

                                    <li class="li">file:removeExtension(&lt;filepath&gt;) - Removes the file
                                        extension from a path. </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_dvg_nqn_wx" title="Use pipeline functions to determine information about a pipeline, such as the pipeline name or the pipeline version.">pipeline functions</a> - You can use the following new
                                pipeline functions to determine information about a pipeline: <ul class="ul" id="concept_oyv_zfk_fy__ul_lc5_rkk_fy">
                                    <li class="li">pipeline:name() - Returns the pipeline name. </li>

                                    <li class="li">pipeline:version() - Returns the pipeline version when the
                                        pipeline has been published to Dataflow Performance Manager
                                        (DPM). </li>

                                </ul>
</li>

                            <li class="li">New <a class="xref" href="../Expression_Language/Functions.html#concept_qkr_trf_sw" title="Use time functions to return the current time or to transform datetime data.">time functions</a> - You can use the following new time
                                functions to transform datetime data:<ul class="ul" id="concept_oyv_zfk_fy__ul_znz_skk_fy">
                                    <li class="li"> time:extractLongFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a long value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:extractStringFromDate(&lt;Date object&gt;, &lt;string&gt;) -
                                        Extracts a string value from a Date object, based on the
                                        specified date format. </li>

                                    <li class="li">time:millisecondsToDateTime(&lt;long&gt;) - Converts an epoch
                                        or UNIX time in milliseconds to a Date object. </li>

                                </ul>
</li>

                        </ul>

                    </dd>

                
            </dl>
</div>

 </div>

</div>
</div>
<div class="navfooter"><!---->
<span class="navprev"><a class="link" href="../Getting_Started/GettingStarted_Title.html#concept_htw_ghg_jq" title="Getting Started"><span class="navheader_label">Previous topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Getting Started</span></a></span>  
<span class="navnext"><a class="link" href="../Install_Config/Install_Config_title.html" title="Installation and Configuration"><span class="navheader_label">Next topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Installation and Configuration</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>