
<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />        
      <meta name="copyright" content="(C) Copyright 2005" /><meta name="DC.rights.owner" content="(C) Copyright 2005" /><meta name="DC.Type" content="concept" /><meta name="DC.Title" content="Destinations" /><meta name="abstract" content="A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline." /><meta name="description" content="A destination stage represents the target for a pipeline. You can use one or more destinations in a pipeline." /><meta name="DC.Relation" scheme="URI" content="../Destinations/Destinations-title.html" /><meta name="DC.Format" content="XHTML" /><meta name="DC.Identifier" content="concept_hpr_twm_jq" /><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/commonltr.css"><!----></link><title>Destinations</title><!--  Generated with Oxygen version 18.1, build number 2016112217.  --><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/css/webhelp_topic.css"><!----></link><link rel="stylesheet" type="text/css" href="../oxygen-webhelp/resources/skins/skin.css" /><link rel="stylesheet" type="text/css" href="../skin.css" /><script type="text/javascript"><!--
            
            var prefix = "../index.html";
            
            --></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-1.11.3.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.cookie.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery-ui.custom.min.js"><!----></script><script type="text/javascript" src="../oxygen-webhelp/resources/js/jquery.highlight-3.js"><!----></script><script type="text/javascript" charset="utf-8" src="../oxygen-webhelp/resources/js/webhelp_topic.js"><!----></script>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
</head>
<body onload="highlightSearchTerm()" class="frmBody">
<table class="nav"><tbody><tr><td colspan="2"><div id="printlink"><a href="javascript:window.print();" title="Print this page"></a></div><div id="permalink"><a href="#" title="Link to this page"></a></div></td></tr><tr><td style="width:75%;"><span class="topic_breadcrumb_links"><span class="topic_breadcrumb_link"><a class="navheader_parent_path" href="../Destinations/Destinations-title.html" title="Destinations">Destinations</a></span></span></td><td><span id="topic_navigation_links" class="navheader">
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </span></td></tr></tbody></table>
<div class="nested0" id="concept_hpr_twm_jq">
 <h1 class="title topictitle1">Destinations</h1>

 
 <div class="body conbody"><p class="shortdesc">A destination stage represents the target for a pipeline. You can use one or more
    destinations in a pipeline.</p>

  <div class="p">You can use the following
      types of destinations in a pipeline: <ul class="ul" id="concept_hpr_twm_jq__ul_mxz_jxm_jq">
        <li class="li"><a class="xref" href="AmazonS3.html#concept_avx_bnq_rt" title="The Amazon S3 destination writes data to Amazon S3. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon Kinesis Streams, use the Kinesis Producer destination.">Amazon S3</a> - Writes data to Amazon
          S3. </li>

        <li class="li"><a class="xref" href="DataLakeStore.html#concept_jzm_kf4_zx">Azure Data Lake Store</a> - Writes
          data to the Azure Data Lake Store.</li>

        <li class="li"><a class="xref" href="Cassandra.html#concept_hfy_mfd_sr" title="The Cassandra destination writes data to a Cassandra cluster.">Cassandra</a> - Writes data to a
          Cassandra cluster.</li>

        <li class="li"><a class="xref" href="Elasticsearch.html#concept_u5t_vpv_4r" title="The Elasticsearch destination writes data to an Elasticsearch cluster. It can also write to Elastic Cloud clusters (formerly Found clusters) and Shield-enabled clusters. The Elasticsearch destination writes each record to Elasticsearch as a document.">Elasticsearch</a> - Writes data to
          an Elasticsearch cluster.</li>

        <li class="li"><a class="xref" href="Flume.html#concept_pzn_hl4_yr" title="The Flume destination writes data to a Flume source. When you write data to Flume, you pass data to a Flume client. The Flume client passes data to hosts based on client configuration properties.">Flume</a> - Writes data to a Flume
          source.</li>

        <li class="li"><a class="xref" href="Bigtable.html#concept_pl5_tmq_tx" title="The Google Bigtable destination writes data to Google Cloud Bigtable.">Google Bigtable</a> - Writes data to
          Google Cloud Bigtable.</li>

        <li class="li"><a class="xref" href="HadoopFS-destination.html#concept_awl_4km_zq" title="You can use Kerberos authentication to connect to HDFS. When you use Kerberos authentication, Data Collector uses the Kerberos principal and keytab to connect to HDFS. You can configure the Hadoop FS destination to use an HDFS user to write data to HDFS.">Hadoop FS</a> - Writes data
          to the Hadoop Distributed File System (HDFS).</li>

        <li class="li"><a class="xref" href="HBase.html#concept_wsz_5t5_vr" title="The HBase destination writes data to an HBase cluster. The destination can write data to HBase as text, binary data, or JSON strings. You can define the data format for each column written to HBase.">HBase</a> - Writes data to an HBase
          cluster.</li>

        <li class="li"><a class="xref" href="HiveMetastore.html#concept_gcr_z2t_zv" title="The Hive Metastore destination works with the Hive Metadata processor and the Hadoop FS or MapR FS destination as part of the Hive Drift Solution.">Hive Metastore</a> - Creates and
          updates Hive tables as needed.</li>

        <li class="li"><a class="xref" href="Hive.html#concept_kvs_3hh_ht" title="The Hive Streaming destination writes data to Hive tables stored in the ORC (Optimized Row Columnar) file format.">Hive Streaming</a> - Writes data to
          Hive.</li>

        <li class="li"><a class="xref" href="InfluxDB.html#concept_inf_db_sr" title="The InfluxDB destination writes data to an InfluxDB database.">InfluxDB</a> - Writes data to
          InfluxDB.</li>

        <li class="li"><a class="xref" href="JDBCProducer.html#concept_kvs_3hh_ht" title="The JDBC Producer destination uses a JDBC connection to write data to a database table. You can also use the JDBC Producer to write change capture data from a Microsoft SQL Server change log.">JDBC Producer</a> - Writes data to
          JDBC.</li>

        <li class="li"><a class="xref" href="KProducer.html#concept_oq2_5jl_zq" title="The Kafka Producer destination writes data to a Kafka cluster.">Kafka Producer</a> - Writes data to a
          Kafka cluster.</li>

        <li class="li"><a class="xref" href="KinFirehose.html#concept_bjv_dpk_kv" title="The Kinesis Firehose destination writes data to an Amazon Kinesis Firehose delivery stream. Firehose automatically delivers the data to the Amazon S3 bucket or Amazon Redshift table that you specify in the delivery stream.">Kinesis Firehose</a> - Writes data
          to a Kinesis Firehose delivery stream.</li>

        <li class="li"><a class="xref" href="KinProducer.html#concept_swk_h1j_yr" title="The Kinesis Producer destination writes data to Amazon Kinesis Streams. To write data to an Amazon Kinesis Firehose delivery system, use the Kinesis Firehose destination. To write data to Amazon S3, use the Amazon S3 destination.">Kinesis Producer</a> - Writes data
          to Kinesis Streams.</li>

        <li class="li"><a class="xref" href="Kudu.html#concept_chy_xxg_4v" title="The Kudu destination writes data to a Kudu cluster.">Kudu</a> - Writes data to Kudu.</li>

        <li class="li"><a class="xref" href="LocalFS.html#concept_zvc_bv5_1r">Local FS</a> - Writes data to a local
          file system. </li>

        <li class="li"><a class="xref" href="MapRDB.html#concept_vxg_w2z_yv" title="The MapR DB destination writes data to MapR DB. The destination can write data to MapR DB as text, binary data, or JSON strings. You can define the data format for each column written to MapR DB.">MapR DB</a> - Writes data to MapR
          DB.</li>

        <li class="li"><a class="xref" href="MapRFS.html#concept_spv_xlc_fv" title="The MapR FS destination writes files to MapR FS. You can write the data to MapR as flat files or Hadoop sequence files.">MapR FS</a> - Writes data to MapR
          FS.</li>

        <li class="li"><a class="xref" href="MapRStreamsProd.html#concept_cfj_qbn_2v" title="The MapR Streams Producer destination writes messages to MapR Streams.">MapR Streams Producer</a> -
          Writes data to MapR Streams.</li>

        <li class="li"><a class="xref" href="MongoDB.html#concept_eth_k5n_4v" title="The MongoDB destination writes data to MongoDB. To write data to MongoDB, you must define an operation record header attribute in the pipeline. The operation header attribute indicates the operation to perform for each record: insert, upsert, or delete.">MongoDB</a> - Writes data to
          MongoDB.</li>

        <li class="li"><a class="xref" href="MongoDB.html#concept_eth_k5n_4v" title="The MongoDB destination writes data to MongoDB. To write data to MongoDB, you must define an operation record header attribute in the pipeline. The operation header attribute indicates the operation to perform for each record: insert, upsert, or delete.">Rabbit MQ Producer</a> - Writes data to
          RabbitMQ.</li>

        <li class="li"><a class="xref" href="Redis.html#concept_ktc_gw2_gw" title="The Redis destination writes data to Redis.">Redis</a> - Writes data to Redis.</li>

        <li class="li"><a class="xref" href="Salesforce.html#concept_rlb_rt3_rx" title="The Salesforce destination writes data to Salesforce objects.">Salesforce</a> - Writes data to
          Salesforce.</li>

        <li class="li"><a class="xref" href="SDC_RPCdest.html#concept_lfk_hx2_ct" title="The SDC RPC destination enables connectivity between two SDC RPC pipelines. The SDC RPC destination passes data to one or more SDC RPC origins. Use the SDC RPC destination as part of an SDC RPC origin pipeline.">SDC RPC</a> - Passes data to an SDC
          RPC origin in an SDC RPC pipeline.</li>

        <li class="li"><a class="xref" href="Solr.html#concept_lfk_hx2_ct">Solr</a> - Writes data to a Solr node or
          cluster.</li>

        <li class="li"><a class="xref" href="ToError.html#concept_ryn_v3z_lr" title="The To Error destination passes records to the pipeline for error handling. Use the To Error destination to send a stream of records to pipeline error handling.">To Error</a> - Passes records to the
          pipeline for error handling.</li>

        <li class="li"><a class="xref" href="Trash.html#concept_htf_ydj_wq" title="The Trash destination discards records. Use the Trash destination as a visual representation of records discarded from the pipeline. Or, you might use the Trash destination during development as a temporary placeholder.">Trash</a> - Removes records from the
          pipeline.</li>

        <li class="li"><a class="xref" href="WaveAnalytics.html#concept_hlx_r53_rx" title="The Wave Analytics destination writes data to Salesforce Wave Analytics. The destination connects to Wave Analytics to create a dataset with external data.">Wave Analytics</a> - Writes data
          to Salesforce Wave Analytics.</li>

      </ul>
To help create or test pipelines, you can use the following development destination:<ul class="ul" id="concept_hpr_twm_jq__ul_wvk_p3f_lx">
        <li class="li">To Event</li>

      </ul>
</div>

    <p class="p">For more information, see <a class="xref" href="../Pipeline_Design/DevStages.html#concept_czx_ktn_ht">Development Stages</a>.</p>

 </div>

<div class="related-links"></div>
<div class="topic concept nested1" id="concept_lmn_gdc_1w">
    <h2 class="title topictitle2">Record Header Attributes for Record-Based Writes</h2>

    <div class="body conbody">
        <p class="p">Destinations can use information in record header
            attributes to write data. Destinations that write Avro data can use Avro schemas in the
            record header. The Hadoop FS and MapR FS destinations can use record header attributes
            to determine the directory to write to and when to roll a file as part of the Hive Drift
            Solution. For more information, see <a class="xref" href="../Hive_Drift_Solution/HiveDriftSolution_title.html#concept_phk_bdf_2w">Hive Drift Solution: Ingesting Drifting Data into Hive</a>.</p>

        <p class="p">To use a record header attribute, configure the destination to use the header attribute
            and ensure that the records include the header attribute.</p>

        <p class="p"><span class="ph">The Hive Metadata processor automatically generates record
                        header attributes for Hadoop FS and MapR FS to use as part of the Hive Drift
                        Solution. For all other destinations, you can use the Expression Evaluator
                        to add record header attributes.</span>
        </p>

        <div class="p">You can use the following record header attributes in destinations:
            <dl class="dl">
                
                    <dt class="dt dlterm">targetDirectory attribute in the Hadoop FS, Local FS, and MapR FS
                        destinations</dt>

                    <dd class="dd">The targetDirectory record header attribute defines the directory where the
                        record is written. If the directory does not exist, the destination creates
                        the directory. The targetDirectory header attribute replaces the Directory
                        Template property in the destination.</dd>

                    <dd class="dd">When you use targetDirectory to provide the directory, the time basis
                        configured for the destination is used only for determining whether a record
                        is late. Time basis is not used to determine the output directories to
                        create or to write records to directories.</dd>

                    <dd class="dd">To use the targetDirectory header attribute, on the
                            <span class="keyword wintitle">Output</span> tab, select <span class="ph uicontrol">Directory in
                            Header</span>.</dd>

                
                
                    <dt class="dt dlterm">avroSchema attribute in destinations that write Avro data</dt>

                    <dd class="dd">The avroSchema header attribute defines the Avro schema for the record. When
                        you use this header attribute, you cannot define an Avro schema to use in
                        the destination. </dd>

                    <dd class="dd">To use the avroSchema header attribute, on the <span class="keyword wintitle">Data
                            Format</span> tab, select the <span class="ph uicontrol">Avro</span> data
                        format, and then for the <span class="ph uicontrol">Avro Schema Location</span>
                        property, select <span class="ph uicontrol">In Record Header</span>.</dd>

                
                
                    <dt class="dt dlterm">roll attribute in the Hadoop FS, Local FS, and MapR FS destinations</dt>

                    <dd class="dd">The roll attribute, when present in the record header, triggers a roll of
                        the file. </dd>

                    <dd class="dd">You can define the name of the roll header attribute. When you use the Hive
                        Metadata processor to generate the roll header attribute, use the default
                        "roll" attribute name. When you use an Expression Evaluator, use the name of
                        the roll attribute that you defined in the processor.</dd>

                    <dd class="dd">To use a roll header attribute, on the <span class="keyword wintitle">Output</span> tab,
                        select <span class="ph uicontrol">Use Roll Attribute</span> and define the name of the
                        attribute. </dd>

                
            </dl>
</div>

    </div>

<div class="topic concept nested2" id="concept_th5_3zj_mw">
    <h3 class="title topictitle3">Generating Record Header Attributes</h3>

    <div class="body conbody">
        <p class="p">You can
            use the Hive Metadata processor or the Expression Evaluator to generate record header
            attributes for record-based writes. <span class="ph">The Hive Metadata processor automatically generates record
                        header attributes for Hadoop FS and MapR FS to use as part of the Hive Drift
                        Solution. For all other destinations, you can use the Expression Evaluator
                        to add record header attributes.</span></p>

        <div class="p">To use the Expression Evaluator, you must generate record header attributes as expected
            by the destination. Use the following guidelines to generate record header attributes
            with the Expression Evaluator:<dl class="dl">
                
                    <dt class="dt dlterm">Generating the target directory with the Expression Evaluator</dt>

                    <dd class="dd">When using the Expression Evaluator to generate the target directory, note
                        the following details:</dd>

                    <dd class="dd">
                        <ul class="ul" id="concept_th5_3zj_mw__ul_i1l_xyj_mw">
                            <li class="li">The destination expects the directory in a header attribute named
                                "targetDirectory".</li>

                            <li class="li">
                                <p class="p">The destination uses the directory exactly as written in the
                                    targetDirectory header attribute. Unlike directory templates,
                                    the directory specified in the targetDirectory attribute should
                                    not include any components that require evaluation, such as
                                    constants, variables, or runtime properties. </p>

                            </li>

                            <li class="li">
                                <p class="p">When you define the expression that evaluates to a directory, you
                                    can use any valid component, including expressions that evaluate
                                    data in the record. </p>

                            </li>

                        </ul>

                    </dd>

                    <dd class="dd">
                        <p class="p">For example, you want to write records to different directories based on
                            the <span class="ph">Data
                  Collector</span> that runs the pipeline, and the region and store ID where the
                            transaction took place. You can set up a runtime resource named DIR that
                            defines the base for the directory and define DIR for each <span class="ph">Data
                  Collector</span> that runs the pipeline. Then, you can use the following expression in
                            the Expression Evaluator to define the targetDirectory attribute: </p>

                        <pre class="pre">${runtime:conf('DIR')/transactions/${record.value('/region')}/${record.value('/storeID')}</pre>

                    </dd>

                
                
                    <dt class="dt dlterm">Generating the Avro schema with the Expression Evaluator</dt>

                    <dd class="dd">When using the Expression Evaluator to generate the Avro schema, note the
                        following details: <ul class="ul" id="concept_th5_3zj_mw__ul_kx1_4ql_nw">
                            <li class="li">The destination expects the Avro schema in a header attribute named
                                "avroSchema".</li>

                            <li class="li">Use the standard Avro schema format, for example:
                                <pre class="pre codeblock">{"type":"record","name":"table_name","namespace":"database_name",
"fields":[{"name":"int_val","type":["null","int"],"default":null},
{"name":"str_val","type":["null","string"],"default":null}]}</pre>
</li>

                            <li class="li">The database name and table name must be included in the Avro
                                schema.</li>

                        </ul>
</dd>

                    <dd class="dd">
                        <div class="note tip"><span class="tiptitle">Tip:</span> You might use an Avro schema generator to help generate the
                            Avro schema. </div>

                    </dd>

                
                
                    <dt class="dt dlterm">Generating the roll attribute with the Expression Evaluator</dt>

                    <dd class="dd">When using the Expression Evaluator to generate the roll attribute, note the
                        following details: <ul class="ul" id="concept_th5_3zj_mw__ul_a23_hnc_pw">
                            <li class="li">Use any name for the attribute and specify the attribute name in the
                                destination. </li>

                            <li class="li">Configure an expression that defines when to roll files.</li>

                        </ul>
</dd>

                
            </dl>
</div>

        <div class="p">To define a record header attribute in the Expression Evaluator, perform the following
                steps:<ol class="ol" id="concept_th5_3zj_mw__ol_jww_k31_cw">
                <li class="li">On the <span class="keyword wintitle">Expressions</span> tab of the Expression Evaluator, specify
                    the <span class="ph uicontrol">Header Attribute</span> name. <p class="p">To generate a target
                        directory, use <samp class="ph codeph">targetDirectory</samp>. </p>
<p class="p">To generate an Avro
                        schema, use <samp class="ph codeph">avroSchema</samp>.</p>
<p class="p">You can use any name for a
                        roll indicator header attribute.</p>
</li>

                <li class="li">For the <span class="ph uicontrol">Header Attribute Expression</span>, define the
                    expression that evaluates to the information you want the destination to use.
                </li>

            </ol>
</div>

    </div>

</div>
</div>
</div>
<div class="navfooter"><!---->
<span class="navparent"><a class="link" href="../Destinations/Destinations-title.html" title="Destinations"><span class="navheader_label">Parent topic</span><span class="navheader_separator">: </span><span class="navheader_linktext">Destinations</span></a></span>  </div><div class="footer" id="webhelp_copyright_information"><!-- SDC google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60917135-3', 'auto');
  ga('send', 'pageview');
</script></div>
</body>
</html>