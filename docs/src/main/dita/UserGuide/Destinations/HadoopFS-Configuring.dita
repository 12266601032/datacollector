<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_m2m_skm_zq">
    <title>Configuring a Hadoop FS Destination</title>
    <taskbody>
        <context>
            <p><indexterm>Hadoop FS
                destination<indexterm>configuring</indexterm></indexterm>Configure a Hadoop FS
                destination to write data to HDFS.</p>
        </context>
        <steps id="steps_ljw_44d_br">
            <step
                conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/1stStep-StageLib-ReqField-EHandling">
                <cmd/>
            </step>
            
            <step>
                <cmd>On the <wintitle>Hadoop FS</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_rst_t4d_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3*"/>
                            <thead>
                                <row>
                                    <entry>Hadoop FS Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Hadoop FS URI</entry>
                                    <entry>HDFS URI.</entry>
                                </row>
                                <row>
                                    <entry>HDFS User <xref href="HadoopFS-HDFSUser-dest.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_byg_yqg_xs"
                                        /></xref></entry>
                                    <entry>The HDFS user to use to connect to HDFS. When using this
                                        property, make sure HDFS is configured appropriately.<p>By
                                            default, the pipeline uses the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> user to connect to HDFS.</p></entry>
                                </row>
                                <row>
                                    <entry>Kerberos Authentication<xref
                                            href="HadoopFS-Kerberos.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_a5x_jzn_vs"
                                        /></xref></entry>
                                    <entry>Uses Kerberos credentials to connect to HDFS. <p>When
                                            selected, uses the Kerberos principal and keytab defined
                                            in the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> configuration file. </p></entry>
                                </row>
                                <row>
                                    <entry>Hadoop FS Configuration Directory <xref
                                            href="HadoopFS-HadoopProperties-dest.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_br4_fgs_5r"/></xref></entry>
                                    <entry>Location of the HDFS configuration files.<p>Use a
                                            directory or symlink within the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory.</p><p>You can use the following
                                            files with the Hadoop FS destination:<ul
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/HDFSfiles_HDFSdest"
                                                id="ul_qnc_jtt_bt">
                                                <li/>
                                            </ul></p><note>Properties in the configuration files are
                                            overridden by individual properties defined in the
                                            stage.</note></entry>
                                </row>
                                <row>
                                    <entry>Hadoop FS Configuration</entry>
                                    <entry>Additional HDFS properties to use. <p>To add properties,
                                            click <uicontrol>Add</uicontrol> and define the property
                                            name and value. Use the property names and values as
                                            expected by HDFS.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="FS-OutputFiles">
                <cmd>On the <wintitle>Output Files</wintitle> tab, configure the following
                    options:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_byd_xpd_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Output Files Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>File Type</entry>
                                    <entry>Output file type:<ul id="ul_lgf_j3g_br">
                                            <li>Text files</li>
                                            <li>Sequence files</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Data Format</entry>
                                    <entry>Format of data to be written. Use one of the following
                                            options:<ul id="ul_un2_cqd_br">
                                            <li>Text</li>
                                            <li>JSON</li>
                                            <li>Delimited</li>
                                            <li>Avro</li>
                                            <li>Protobuf</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Files Prefix</entry>
                                    <entry>Prefix to use for output files. Use when writing to a
                                        directory that receives files from other sources.<p>Uses the
                                            prefix sdc-${sdc:id()} by default. The prefix evaluates
                                            to sdc-&lt;Data Collector ID>. </p><p>The Data Collector
                                            ID is stored in the following file:
                                                <filepath>&lt;SDCinstalldir>/data/sdc.id</filepath>.
                                        </p></entry>
                                </row>
                                <row
                                    conref="../Reusable_Content/ReusableTables.dita#concept_wfr_rnw_yq/D-CHARSET-file">
                                    <entry/>
                                </row>
                                <row>
                                    <entry>Directory Template <xref
                                            href="../Destinations/HadoopFS-DirectoryTemplates.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_mb4_fsw_45"/></xref></entry>
                                    <entry>Template for creating output directories. You can use
                                        constants, field values, and datetime variables. <p>Output
                                            directories are created based on the smallest datetime
                                            variable in the template.</p></entry>
                                </row>
                                <row>
                                    <entry>Data Time Zone</entry>
                                    <entry>Time zone to use to create directories and evaluate where
                                        records are written.</entry>
                                </row>
                                <row>
                                    <entry>Time Basis <xref
                                            href="../Destinations/HadoopFS-TimeBasis.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_a4r_rkw_45"/></xref></entry>
                                    <entry>Time basis to use for creating output directories and
                                        writing records to the directories. Use one of the following
                                            expressions:<ul id="ul_ggs_43g_br">
                                            <li>${time:now()} - Uses the processing time as the time
                                                basis. </li>
                                            <li>${record:value("/&lt;date field>")} - Uses the time
                                                associated with the record as the time basis.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Max Records in a File</entry>
                                    <entry>Maximum number of records to be written to an output
                                        file. Additional records are written to a new file. <p>Use 0
                                            to opt out of this property.</p></entry>
                                </row>
                                <row>
                                    <entry>Max File Size (MB)</entry>
                                    <entry>Maximum size of an output file. Additional records are
                                        written to a new file. <p>Use 0 to opt out of this
                                            property.</p></entry>
                                </row>
                                <row>
                                    <entry>Compression Codec</entry>
                                    <entry>Compression type for output files:<ul id="ul_ltx_djg_br">
                                            <li>None </li>
                                            <li>gzip</li>
                                            <li>bzip2</li>
                                            <li>Snappy</li>
                                            <li>Other - use for LZO or other compression types.
                                            </li>
                                        </ul><p>LZO compression require additional configuration.
                                            For more information, see <xref
                                                href="../Destinations/HadoopFS-Comp-LZO.dita"
                                        />.</p></entry>
                                </row>
                                <row>
                                    <entry>Compression Codec Class</entry>
                                    <entry>Full class name of the other compression codec that you
                                        want to use. </entry>
                                </row>
                                <row>
                                    <entry>Sequence File Key</entry>
                                    <entry>Record key for creating Hadoop sequence files. Use one of
                                        the following options:<ul id="ul_xzr_vkg_br">
                                            <li>${record:value("/&lt;field name>")}</li>
                                            <li>${uuid()}</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Compression Type</entry>
                                    <entry>Compression type for sequence files when using a
                                        compression codec:<ul id="ul_etm_1lg_br">
                                            <li>Block Compression</li>
                                            <li>Record Compression</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/FS-LateRecords">
                <cmd/>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/TextProps">
                <cmd/>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/JSONProps">
                <cmd/>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/DelimProps">
                <cmd/>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/D-AVRO-File">
                <cmd/>
            </step>
            <step conref="../Reusable_Content/ReusableSteps.dita#task_kzs_5vz_sq/D-PROTO-props">
                <cmd/>
            </step>
        </steps>
    </taskbody>
</task>
