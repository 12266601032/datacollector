<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cpx_1lm_zx">
 <title>Spark Evaluator</title>
 <shortdesc>The Spark Evaluator performs custom processing within a pipeline based on a Spark
        application that you develop. Use the Spark Evaluator processor in standalone pipelines
        only. </shortdesc>
 <conbody>
  <p><indexterm>Spark Evaluator
                    processor<indexterm>overview</indexterm></indexterm><indexterm>processors<indexterm>Spark
                    Evaluator</indexterm></indexterm>The Spark Evaluator starts a local Spark
            application. The Spark application - or <codeph>SparkContext</codeph> - runs as long as
            the pipeline runs. The Spark application submits a job for each batch of records,
            processing the batch and then returning the results and errors back to the pipeline for
            further processing. </p>
        <p>You can configure a parallelism value for the Spark Evaluator. The Spark application
            creates this number of partitions for each batch of records. Each partition is processed
            in parallel. Set the parallelism value based on the number of available processors on
            the <ph conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
            /> machine. Because the Spark Evaluator can use multiple threads to process a batch, the
            processor is especially useful when you need to perform heavy custom processing within a
            pipeline, such as image classification.</p>
        <note>The stage libraries that include the Spark Evaluator include all dependencies required
            for the processor. The Spark Evaluator does not require any installation
            prerequisites.</note>
        <p>Complete the following tasks to use the Spark Evaluator:<ol id="ol_pmh_33p_zx">
                <li>Write the Spark application using Java or Scala. Then, package a JAR file
                    containing the application.</li>
                <li>Install the application and its dependencies.</li>
                <li>Configure the Spark Evaluator processor to submit the Spark application.<p>When
                        you configure the processor, you define the parallelism value to use during
                        processing. You also define the name of the custom Spark class that you
                        developed and define the arguments to pass to the <codeph>init</codeph>
                        method in the custom Spark class.</p></li>
            </ol></p>
 </conbody>
</concept>
