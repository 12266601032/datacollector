<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA General Task//EN" "generalTask.dtd">
<task id="task_kzs_5vz_sq">
    <title>Reusable Steps</title>
    <shortdesc>You can conref these steps. Don't change anything in this file without checking where
        it is used.</shortdesc>
    <taskbody>
        <context/>
        <steps id="steps_pf2_wvz_sq">
            <step>
                <cmd id="PIPE_PROPS">
                    <draft-comment author="Loretta"><uicontrol>PIPELINE
                        PROPERTIES</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following two step are used in Configuring a
                        Pipeline and the tutorial pipeline config topic.</draft-comment>
                </cmd>
            </step>
            <step id="CreatePipeline1">
                <cmd>From the <wintitle>Home</wintitle> page or <wintitle>Getting Started</wintitle>
                    page, click <uicontrol>Create New Pipeline</uicontrol>. </cmd>
                <info>If you are working on another pipeline in the <ph
                        conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> console, use
                    the <uicontrol>Toggle Library Pane</uicontrol> icon to display the pipeline
                    library, and click the <uicontrol>Create New Pipeline</uicontrol> icon.</info>
            </step>
            <step id="CreatePipeline2">
                <cmd>In the <wintitle>New Pipeline</wintitle> window, enter a pipeline name and
                    optional description, and click <uicontrol>Save</uicontrol>.</cmd>
                <stepresult>The pipeline canvas displays pipeline name and an error icon. The error
                    icon indicates that you need to configure error handling for the pipeline. The
                    Properties panel displays the pipeline properties. </stepresult>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DATA
                        PREVIEW</uicontrol></draft-comment>
                    <draft-comment author="Loretta">The steps below are for data preview. They are
                        used in "Previewing a Single Stage" and "Previewing Multiple
                        Stages."</draft-comment>
                </cmd>
            </step>
            <step id="StartPreview">
                <cmd>Above the pipeline canvas, click the <uicontrol>Preview</uicontrol> icon:
                        <image href="../Graphics/icon_Preview.png" id="image_tfg_tf4_zs" scale="70"
                    />.</cmd>
                <info>If the Preview icon is disabled, check the Issues list for unconnected stages
                    and required properties that are not defined.</info>
            </step>
            <step id="Preview-Source">
                <cmd>In the <wintitle>Preview Configuration</wintitle> dialog box, configure the
                    following properties, then click <uicontrol>Run Preview</uicontrol>.</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_mjq_mfm_cs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Preview Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Preview Source</entry>
                                    <entry>Source data for the preview:<ul id="ul_db2_4fm_cs">
                                            <li>Configured Source - Provides data from the origin
                                                system.</li>
                                            <li>Snapshot Data - Uses available snapshot data.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Preview Batch Size</entry>
                                    <entry>Number of records to use in the preview. Honors values up
                                        to the <ph
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> preview batch size. <p>Default is 10. The <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> default is 10.</p></entry>
                                </row>
                                <row>
                                    <entry>Preview Timeout</entry>
                                    <entry>Milliseconds to wait for preview data. Use to limit the
                                        time data preview waits for data to arrive at the origin.
                                        Relevant for transient origins only. </entry>
                                </row>
                                <row>
                                    <entry>Write to Destinations</entry>
                                    <entry>Determines whether the preview writes data to pipeline
                                        destinations. <p>By default, does not write data to
                                            destinations. </p></entry>
                                </row>
                                <row>
                                    <entry>Show Record Header</entry>
                                    <entry>Displays record metadata in List view. Headers do not
                                        display in Table view.</entry>
                                </row>
                                <row>
                                    <entry>Show Field Type</entry>
                                    <entry>Displays the data type for fields in List view. Field
                                        types do not display in Table view.</entry>
                                </row>
                                <row>
                                    <entry>Snapshot Data</entry>
                                    <entry>When using a snapshot for source data, select the
                                        snapshot to use. </entry>
                                </row>
                                <row>
                                    <entry>Remember the Configuration</entry>
                                    <entry>Stores the current preview configuration for use every
                                        time you request a preview for this pipeline. <p>After you
                                            run data preview, you can change this option in the
                                            Preview panel by selecting the Preview Configuration
                                            icon (<image
                                                href="../Graphics/icon_PrevPreviewConfig.png"
                                                id="image_qzd_tnf_vs" scale="80"/>) and clearing the
                                            option. The change takes effect the next time you run
                                            data preview.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
                <stepresult>The Preview panel highlights the origin stage and displays preview data
                    in table view. Since this is the origin of the pipeline, no input data displays.
                        <p>To view preview data in list view, click the <uicontrol>List
                            View</uicontrol> icon: <image href="../Graphics/icon_PrevListView.png"
                            id="image_ids_rc4_xs" scale="80"/>.</p></stepresult>
            </step>
            <step id="DeletePreviewRecord">
                <cmd>To delete a record that you do not want to use, click the
                        <uicontrol>Delete</uicontrol> icon.</cmd>
            </step>
            <step id="RefreshPreview">
                <cmd>To refresh the preview, click the <uicontrol>Refresh Preview</uicontrol> icon:
                        <image href="../Graphics/icon_PrevRefresh.png" id="image_nys_3ff_vs"
                        scale="90"/>.</cmd>
                <info>Based on the origin, refreshing the preview either provides a new set of data
                    or reverts any changes to the existing data.</info>
            </step>
            <step id="RevertRefreshClose2">
                <cmd>To exit data preview and return to pipeline configuration, click
                        <uicontrol>Close Preview</uicontrol>.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following step is used in "Previewing a
                        Single Stage" and "Reviewing Snapshot Data"</draft-comment>
                </cmd>
            </step>
            <step id="NextStage">
                <cmd>To view data for the next stage, click the <uicontrol>Next Stage</uicontrol>
                    icon. Or, to view data for a different stage, select the stage in the pipeline
                    canvas.</cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol id="ORIGIN_INFO"
                        >ORIGINS</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStepErrorHandling</uicontrol> -
                        Use this step for origins that are not cluster origins. Standalone
                        only</draft-comment>
                </cmd>
            </step>
            <step id="1stStepErrorHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_vlh_bgh_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_mmh_bgh_hr">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. </li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStep-ClusterOrigin</uicontrol> -
                        Use for origins that ARE cluster origins - points out stop pipeline is not
                        valid.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ClusterOrigin">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_u44_lsn_xs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_mp4_lsn_xs">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"
                            ><uicontrol>1stStep-StageLib-EHandling</uicontrol> - Use this step for
                        origins with stage libraries.</draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_rv1_q3d_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_kw1_q3d_3t">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-AVROFILE</uicontrol>  - The
                        following step is used by Directory, Hadoop FS - origins that read avro from
                        files.</draft-comment>
                </cmd>
            </step>
            <step id="O-AVRO-FILE">
                <cmd>For Avro data, on the <wintitle>Avro</wintitle> tab, optionally configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_igl_bq2_2t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition to use when processing data.
                                        Overrides any existing schema definitions associated with
                                        the data. Defining a schema can improve performance.<p>When
                                            not used, the origin uses the schema defined in the
                                            file.</p><p>You can optionally use the
                                            runtime:loadResource function to use a schema definition
                                            stored in a runtime resource file. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-AVRO-Mess</uicontrol> - The
                        following step is used by JMS, Kafka, Kinesis - origins that read Avro data
                        from messages.</draft-comment>
                </cmd>
            </step>
            <step id="O-AVRO-Mess">
                <cmd>For Avro data, on the <wintitle>Avro</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_acd_2qd_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Message includes Schema</entry>
                                    <entry>Indicates that the message includes an Avro schema.
                                    </entry>
                                </row>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Avro schema definition to use when processing data.
                                        Overrides any existing schema definitions associated with
                                        the data. Defining a schema can improve performance.<p>When
                                            not used, the origin uses the schema defined in the
                                            file.</p><p>You can optionally use the
                                            runtime:loadResource function to use a schema definition
                                            stored in a runtime resource file. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DELIMITED data</uicontrol> -
                        Directory, Hadoop FS, JMS Consumer, Kafka Consumer, Kinesis
                        Consumer</draft-comment>
                </cmd>
            </step>
            <step id="DelimFILE">
                <cmd>For delimited data, on the <uicontrol>Delimited</uicontrol> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_DelimitedData">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Delimited Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Delimiter Format Type</entry>
                                    <entry>Delimiter format type. Use one of the following options:
                                            <ul
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul_delFileTypes"
                                            id="ul_s5z_b3z_3r">
                                            <li/>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Header Line</entry>
                                    <entry>Indicates whether a file contains a header line, and
                                        whether to use the header line.</entry>
                                </row>
                                <row>
                                    <entry>Max Record Length (chars)</entry>
                                    <entry>Maximum length of a record in characters. Longer records
                                        are not read. </entry>
                                </row>
                                <row>
                                    <entry>Delimiter Character</entry>
                                    <entry>Delimiter character for a custom file type.</entry>
                                </row>
                                <row>
                                    <entry>Escape Character</entry>
                                    <entry>Escape character for a custom file type.</entry>
                                </row>
                                <row>
                                    <entry>Quote Character</entry>
                                    <entry>Quote character for a custom file type.</entry>
                                </row>
                                <row>
                                    <entry>Root Field Type <xref
                                            href="../Pipeline_Design/DelimitedDataRootFieldTypes.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_exx_41q_ft"/></xref></entry>
                                    <entry>Root field type to use:<ul id="ul_izr_p1q_ft">
                                            <li>List-Map - Generates an indexed list of data.
                                                Enables you to use standard functions to process
                                                data. Use for new pipelines.</li>
                                            <li>List - Generates a record with an indexed list with
                                                a map for header and value. Requires the use of
                                                delimited data functions to process data. Use only
                                                to maintain pipelines created before 1.1.0.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Lines to Skip</entry>
                                    <entry>Lines to skip before reading data. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><b>JSON-2props </b> - Kafka Consumer, JMS
                        Consumer, Hadoop FS, Kinesis all use them. HTTP Client, and probably others
                        use the 2nd row.</draft-comment>
                </cmd>
            </step>
            <step id="JSON-2props">
                <cmd>For JSON data, on the <wintitle>JSON</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_JSONdata">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>JSON Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>JSON Content</entry>
                                    <entry>Type of JSON content. Use one of the following options: <p>
                                            <ul id="ul_atw_krl_5q">
                                                <li>Array of Objects </li>
                                                <li>Multiple Objects</li>
                                            </ul>
                                        </p></entry>
                                </row>
                                <row id="ROW-MaxObject">
                                    <entry>Maximum Object Length (chars)</entry>
                                    <entry>Maximum number of characters in a JSON object. <p>Longer
                                            objects are diverted to the pipeline for error handling.
                                        </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>LOG data </uicontrol> - Used in JMS
                        Consumer, Kafka Consumer, Hadoop FS, probably Directory. Use the following
                        for origins that use the Log data format. UL in first row is also being
                        conrefed in Log Parser. </draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Also - For any changes to the bullets below this
                        first table, update Log Parser as well. Everything except the Log4j
                        table/bullet point is copied from here. </draft-comment>
                </cmd>
            </step>
            <step id="LogData_Log4j">
                <cmd>For log data, click the <wintitle>Log</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ihc_3fs_sr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Log Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Log Format</entry>
                                    <entry>Format of the log files. Use one of the following
                                            options:<ul id="ul-LogFormatList">
                                            <li>Common Log Format</li>
                                            <li>Combined Log Format</li>
                                            <li>Apache Error Log Format</li>
                                            <li>Apache Access Log Custom Format</li>
                                            <li>Regular Expression</li>
                                            <li>Grok Pattern</li>
                                            <li>Log4j</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Max Line Length</entry>
                                    <entry>Maximum length of a log line. The origin truncates longer
                                        lines. </entry>
                                </row>
                                <row>
                                    <entry>Retain Original Line</entry>
                                    <entry>Determines how to treat the original log line. Select to
                                        include the original log line as a field in the resulting
                                            record.<p>By default, the original line is
                                            discarded.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                    <ul id="ul_mdk_djs_sr">
                        <li>When you select <uicontrol>Apache Access Log Custom Format</uicontrol>,
                            use Apache log format strings to define the <uicontrol>Custom Log
                                Format</uicontrol>.</li>
                        <li>When you select <uicontrol>Regular Expression</uicontrol>, enter the
                            regular expression that describes the log format, and then map the
                            fields that you want to include to each regular expression group.</li>
                        <li>When you select <uicontrol>Grok Pattern</uicontrol>, you can use the
                                <uicontrol>Grok Pattern Definition</uicontrol> field to define
                            custom grok patterns. You can define a pattern on each line. <p>In the
                                    <uicontrol>Grok Pattern</uicontrol> field, enter the pattern to
                                use to parse the log. You can use a predefined grok patterns or
                                create a custom grok pattern using patterns defined in
                                    <uicontrol>Grok Pattern Definition</uicontrol>.</p><p>For more
                                information about defining grok patterns and supported grok
                                patterns, see <xref
                                    href="../Apx-GrokPatterns/GrokPatterns.dita#concept_vdk_xjb_wr"
                                />.</p></li>
                        <li>When you select <uicontrol>Log4j</uicontrol>, define the following properties:<p>
                                <table frame="all" rowsep="1" colsep="1" id="table_pzm_rbt_sr">
                                    <tgroup cols="2">
                                        <colspec colname="c1" colnum="1" colwidth="1*"/>
                                        <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                                        <thead>
                                            <row>
                                                <entry>Log4j Property</entry>
                                                <entry>Description</entry>
                                            </row>
                                        </thead>
                                        <tbody>
                                            <row>
                                                <entry>On Parse Error</entry>
                                                <entry>Determines how to handle information that
                                                  cannot be parsed:<ul id="ul_bvm_5bt_sr">
                                                  <li>Skip and Log Error - Skips reading the line
                                                  and logs a stage error.</li>
                                                  <li>Skip, No Error - Skips reading the line and
                                                  does not log an error.</li>
                                                  <li>Include as Stack Trace - Includes information
                                                  that cannot be parsed as a stack trace to the
                                                  previously-read log line. The information is added
                                                  to the message field for the last valid log
                                                  line.</li>
                                                  </ul></entry>
                                            </row>
                                            <row>
                                                <entry>Use Custom Log Format</entry>
                                                <entry>Allows you to define a custom log
                                                  format.</entry>
                                            </row>
                                            <row>
                                                <entry>Custom Format</entry>
                                                <entry>Use log4j variables to define a custom log
                                                  format. </entry>
                                            </row>
                                        </tbody>
                                    </tgroup>
                                </table>
                            </p></li>
                    </ul>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>TEXT data</uicontrol> - Directory,
                        Kafka Consumer, JMS Consumer, Hadoop FS, Kinesis Consumer</draft-comment>
                </cmd>
            </step>
            <step id="Text">
                <cmd>For text data, on the <uicontrol>Text</uicontrol> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_LogData">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Text Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Max Line Length</entry>
                                    <entry>Maximum number of characters allowed for a line. Longer
                                        lines are truncated.<p>Adds a boolean field to the record to
                                            indicate if it was truncated. The field name is
                                            Truncated. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>XML data </uicontrol>- step used by
                        message and file based origins: JMS Consumer, Kafka Consumer, Hadoop FS,
                        Kinesis Consumer, Directory, S3, HTTP Client, maybe more.</draft-comment>
                </cmd>
            </step>
            <step id="XMLprops">
                <cmd>For XML data, on the <wintitle>XML</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_nxh_bc2_2t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>XML Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Delimiter Element</entry>
                                    <entry>
                                        <p>XML element that acts as a delimiter. Omit a delimiter to
                                            treat the entire XML document as one record.</p>
                                    </entry>
                                </row>
                                <row>
                                    <entry>Max Record Length (chars)</entry>
                                    <entry>
                                        <p>The maximum number of characters in a record. Longer
                                            records are diverted to the pipeline for error handling.
                                        </p>
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-Binary </uicontrol>. Used by Kafka
                        and Kinesis Consumer</draft-comment>
                </cmd>
            </step>
            <step id="O-Binary">
                <cmd>For binary data, click the <wintitle>Binary</wintitle> tab and configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_dpx_kdm_35">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Binary Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Max Data Size</entry>
                                    <entry>Maximum number of bytes in the message. Larger messages
                                        cannot be processed or written to error. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-PROTO-Mess</uicontrol>. Step used
                        by message-reading stages except for Kafka. Kafka conrefs the first two rows
                        of table:</draft-comment>
                </cmd>
            </step>
            <step id="O-PROTO-Mess">
                <cmd>For protobuf data, click the <wintitle>Protobuf</wintitle> tab and configure
                    the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_s3c_mz4_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-DescFile">
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory: &lt;SDCinstalldir>/resources. <p>For
                                            information about generating the descriptor file, see
                                                <xref
                                                href="../Pipeline_Design/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row id="row-MessageType">
                                    <entry>Message Type</entry>
                                    <entry>Message type to use to read data. Enter a message type
                                        defined in the descriptor file.</entry>
                                </row>
                                <row>
                                    <entry>Delimited Messages</entry>
                                    <entry>Indicates if a message might include more than one
                                        protobuf message.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>O-Proto-File</uicontrol>. Step used
                        by file-based proto</draft-comment>
                </cmd>
            </step>
            <step id="O-PROTO-File">
                <cmd>For protobuf data, click the <wintitle>Protobuf</wintitle> tab and configure
                    the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_f4r_4cp_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory: &lt;SDCinstalldir>/resources. <p>For
                                            information about generating the descriptor file, see
                                                <xref
                                                href="../Pipeline_Design/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Message Type</entry>
                                    <entry>Message type to use to read data. Enter a message type
                                        defined in the descriptor file.</entry>
                                </row>
                                <row>
                                    <entry>Delimited Messages</entry>
                                    <entry>Indicates if a file might include more than one protobuf
                                        message.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="PROCESSORS">
                <cmd>
                    <draft-comment author="Loretta"
                        ><uicontrol>PROCESSORS</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Use this step for processors that have error
                        handling - should be almost all. </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ReqField-ErrorHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_blh_n2h_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_h4p_p5v_yq"/></xref></entry>
                                    <entry>Fields that must include data to be passed into the
                                        stage. <note outputclass="" type="tip">You might include
                                            fields that the stage uses.</note></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_f3b_khp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_swp_lfh_hr">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline. Not valid for
                                                cluster pipelines.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">Use the following step for processors without
                        record handling - Field Remover, Record Deduplicator.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-ReqField-noEHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_wjk_pfh_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_nkk_pfh_hr"/></xref></entry>
                                    <entry>Fields that must include data to be passed into the
                                        stage. <note outputclass="" type="tip">You might include
                                            fields that the stage uses.</note></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_pbl_thp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">The following steps are used in JDBC Consumer
                        &amp; Producer config topics.</draft-comment>
                </cmd>
            </step>
            <step id="JDBC-Credentials">
                <cmd>When using JDBC credentials, on the <uicontrol>Credentials</uicontrol> tab,
                    configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ybf_v1w_ht">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Credentials Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Username</entry>
                                    <entry>User name for the JDBC connection.</entry>
                                </row>
                                <row>
                                    <entry>Password</entry>
                                    <entry>Password for the JDBC account</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="JDBC-Legacy">
                <cmd>When using JDBC versions older than 4.0, on the <uicontrol>Legacy
                        Drivers</uicontrol> tab, optionally configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_ojq_d4s_bs">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Legacy Driver Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>JDBC Class Driver Name</entry>
                                    <entry>Class name for the JDBC driver. Required for JDBC
                                        versions older than version 4.0.</entry>
                                </row>
                                <row>
                                    <entry>Connection Health Test Query</entry>
                                    <entry>Optional query to test the health of a connection.
                                        Recommended only when the JDBC version is older than 4.0.
                                    </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step id="DEST_INFO">
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DESTINATION -
                        General</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>1stStep-NoStageLib</uicontrol> - Use
                        for destinations without stage libs, e.g. Local FS. </draft-comment>
                </cmd>
            </step>
            <step id="1stStep-NoStageLib">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_fvw_4kg_j5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_lww_4kg_j5"/></xref></entry>
                                    <entry>Fields that must include data to be passed into the
                                        stage. <note outputclass="" type="tip">You might include
                                            fields that the stage uses.</note></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_sww_4kg_j5"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_fxw_4kg_j5">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta" id="1stStageLib-ReqField-ErrorH"
                            ><uicontrol>1stStep-StageLib-ReqField-EHandling</uicontrol> - Use this
                        step for destinations with stage library, req fields, and error
                        handling.</draft-comment>
                </cmd>
            </step>
            <step id="1stStep-StageLib-ReqField-EHandling">
                <cmd>In the Properties panel, on the <wintitle>General</wintitle> tab, configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_tvy_43h_hr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>General Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Name</entry>
                                    <entry>Stage name.</entry>
                                </row>
                                <row>
                                    <entry>Description</entry>
                                    <entry>Optional description.</entry>
                                </row>
                                <row>
                                    <entry>Stage Library</entry>
                                    <entry>Library version that you want to use. </entry>
                                </row>
                                <row>
                                    <entry>Required Fields <xref
                                            href="../Pipeline_Design/RequiredFields.dita#concept_dnj_bkm_vq">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_hwy_43h_hr"/></xref></entry>
                                    <entry>Fields that must include data to be passed into the
                                        stage. <note outputclass="" type="tip">You might include
                                            fields that the stage uses.</note></entry>
                                </row>
                                <row>
                                    <entry>Preconditions <xref
                                            href="../Pipeline_Design/Preconditions.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_rfz_thp_fs"/></xref></entry>
                                    <entry>Conditions that must evaluate to TRUE to allow a record
                                        to enter the stage for processing. Click
                                            <uicontrol>Add</uicontrol> to create additional
                                        preconditions. </entry>
                                </row>
                                <row>
                                    <entry>On Record Error</entry>
                                    <entry>Error record handling for the stage: <ul
                                            id="ul_fqw_4fy_kt">
                                            <li>Discard - Discards the record.</li>
                                            <li>Send to Error - Sends the record to the pipeline for
                                                error handling.</li>
                                            <li>Stop Pipeline - Stops the pipeline.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-Binary</uicontrol> - Used by Kafka
                        Consumer and Amazon S3 dest.</draft-comment>
                </cmd>
            </step>
            <step id="D-Binary">
                <cmd>For binary data, click the <wintitle>Binary</wintitle> tab and configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_xct_mbm_gt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Binary Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Binary Field Path</entry>
                                    <entry>Field that contains the binary data.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-Event</uicontrol> - Used in
                        Flume. Flume doesn't have other compression types, so it's not getting that
                        note. </draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-Event">
                <cmd>For Avro data, click the <wintitle>Avro</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_igm_vtd_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Schema definition to use when writing data. </entry>
                                </row>
                                <row>
                                    <entry>Include Schema</entry>
                                    <entry>Includes the schema in each event. <note>Omitting the
                                            schema definition can improve performance, but requires
                                            the appropriate schema management to avoid losing track
                                            of the schema associated with the data.</note></entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. <draft-comment author="Loretta"
                                                >should I remove this? Is there Flume
                                                compression?</draft-comment></p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-File</uicontrol> - used in
                        Hadoop FS and Amazon S3.</draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-File">
                <cmd>For Avro data, click the <wintitle>Avro</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_fpg_rx3_ks">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Schema definition to use when writing data. The
                                        destination includes the schema definition in each generated
                                        file. </entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>D-AVRO-Mess</uicontrol> - Used in
                        KProducer.</draft-comment>
                </cmd>
            </step>
            <step id="D-AVRO-Mess">
                <cmd>For Avro data, click the <wintitle>Avro</wintitle> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_o2j_4nd_3t">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Avro Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Avro Schema</entry>
                                    <entry>Schema definition to use when writing data. </entry>
                                </row>
                                <row>
                                    <entry>Include Schema</entry>
                                    <entry>Includes the schema in each message. <note>Omitting the
                                            schema definition can improve performance, but requires
                                            the appropriate schema management to avoid losing track
                                            of the schema associated with the data.</note></entry>
                                </row>
                                <row>
                                    <entry>Avro Compression Codec</entry>
                                    <entry>The Avro compression type to use. <p>When using Avro
                                            compression, do not enable other compression available
                                            in the destination. </p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DelimProps:</uicontrol> Use for the
                        appropriate delimited destinations - currently Amazon S3, Flume, Hadoop FS
                        and Kafka Producer:</draft-comment>
                </cmd>
            </step>
            <step id="DelimProps">
                <cmd>For delimited data, click the <wintitle>Delimited</wintitle> tab and configure
                    the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_wb3_2kg_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Delimited Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Delimiter Format</entry>
                                    <entry>Format for delimited data:<ul
                                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul_delFileTypes"
                                            id="ul_k3j_vvf_jr">
                                            <li/>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Header Line</entry>
                                    <entry>Indicates whether to create a header line.</entry>
                                </row>
                                <row>
                                    <entry>Remove New Line Characters</entry>
                                    <entry>Removes new line characters from within a record.
                                            <p>Recommended when writing data as a single line of
                                            text.</p></entry>
                                </row>
                                <row>
                                    <entry>Delimiter Character</entry>
                                    <entry>Delimiter character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p>Default is the pipe character ( |
                                        ).</p></entry>
                                </row>
                                <row>
                                    <entry>Escape Character </entry>
                                    <entry>Escape character for a custom delimiter format. Select
                                        one of the available options or use Other to enter a custom
                                        character. <p>Default is the backslash character ( \
                                        ).</p></entry>
                                </row>
                                <row>
                                    <entry>Quote Character</entry>
                                    <entry>Quote character for a custom delimiter format. Select one
                                        of the available options or use Other to enter a custom
                                        character. <p>Default is the quotation mark character ( "
                                            ).</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>JSONProps</uicontrol> - Used for
                        Hadoop FS, Kafka Producer, and Amazon S3.</draft-comment>
                </cmd>
            </step>
            <step id="JSONProps">
                <cmd>For JSON data, click the <uicontrol>JSON</uicontrol> tab and configure the
                    following property:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_lgq_53c_wr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>JSON Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>JSON Content</entry>
                                    <entry>Determines how JSON data is written:<ul
                                            id="ul_mss_w3c_wr">
                                            <li>JSON Array of Objects - Each file includes a single
                                                array. In the array, each element is a JSON
                                                representation of each record.</li>
                                            <li>Multiple JSON Objects - Each file includes multiple
                                                JSON objects. Each object is a JSON representation
                                                of a record.</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>TextProps</uicontrol> - Using for
                        Hadoop FS, Kafka Producer, and Amazon S3.</draft-comment>
                </cmd>
            </step>
            <step id="TextProps">
                <cmd>For text data, click the <uicontrol>Text</uicontrol> tab and configure the
                    following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_egv_3df_jr">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Text Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Text Field Path</entry>
                                    <entry>Field that contains the text data to be written. All data
                                        must be incorporated into the specified field. </entry>
                                </row>
                                <row>
                                    <entry>Empty Line If No Text</entry>
                                    <entry>Creates an empty line when a record does not include the
                                        text field specified above. <p>When not selected, records
                                            without the specified text field are
                                        discarded.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><b>D-PROTO-props</b> - used by all protobuf
                        destinations</draft-comment>
                </cmd>
            </step>
            <step id="D-PROTO-props">
                <cmd>For protobuf data, click the <wintitle>Protobuf</wintitle> tab and configure
                    the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_vmt_tdp_45">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Protobuf Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Protobuf Descriptor File </entry>
                                    <entry>Descriptor file (.desc) to use. The descriptor file must
                                        be in the <ph
                                            conref="ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                        /> resources directory: &lt;SDCinstalldir>/resources. <p>For
                                            information about generating the descriptor file, see
                                                <xref
                                                href="../Pipeline_Design/Protobuf-Prerequisites.dita"
                                            />.</p></entry>
                                </row>
                                <row>
                                    <entry>Message Type</entry>
                                    <entry>Message type to use to write data. Enter a message type
                                        defined in the descriptor file.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>DESTINATION -
                        Specific</uicontrol></draft-comment>
                </cmd>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta">This step is used by Amazon S3 origin and
                        destination</draft-comment>
                </cmd>
            </step>
            <step id="S3-AdvancedProps">
                <cmd>On the <wintitle>Advanced</wintitle> tab, optionally configure proxy
                    information:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_xmc_smx_r5">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Advanced Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Use Proxy</entry>
                                    <entry>Specifies whether to use a proxy to connect to Amazon
                                        S3.</entry>
                                </row>
                                <row>
                                    <entry>Proxy Host</entry>
                                    <entry>Proxy host.</entry>
                                </row>
                                <row>
                                    <entry>Proxy Port</entry>
                                    <entry>Proxy port.</entry>
                                </row>
                                <row>
                                    <entry>Proxy User</entry>
                                    <entry>User name for proxy credentials.</entry>
                                </row>
                                <row>
                                    <entry>Proxy Password</entry>
                                    <entry>Password for proxy credentials. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>FS-OutputFiles</uicontrol> - for
                        Hadoop FS and Local FS</draft-comment>
                </cmd>
            </step>
            <step id="FS-OutputFiles">
                <cmd>On the <wintitle>Output Files</wintitle> tab, configure the following
                    options:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_byd_xpd_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                            <thead>
                                <row>
                                    <entry>Output Files Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>File Type</entry>
                                    <entry>Output file type:<ul id="ul_lgf_j3g_br">
                                            <li>Text files</li>
                                            <li>Sequence files</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Data Format</entry>
                                    <entry>Format of data to be written. Use one of the following
                                            options:<ul id="ul_un2_cqd_br">
                                            <li>Text</li>
                                            <li>JSON</li>
                                            <li>Delimited</li>
                                            <li>Avro</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Files Prefix</entry>
                                    <entry>Prefix to use for output files. Use when writing to a
                                        directory that receives files from other sources.<p>Uses the
                                            prefix sdc-${sdc:id()} by default. The prefix evaluates
                                            to sdc-&lt;Data Collector ID>. </p><p>The Data Collector
                                            ID is stored in the following file:
                                                <filepath>&lt;SDCinstalldir>/data/sdc.id</filepath>.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Data Charset</entry>
                                    <entry>Character encoding to use when writing data. <p>Not used
                                            for the SDC Record data format.</p></entry>
                                </row>
                                <row>
                                    <entry>Directory Template <xref
                                            href="../Destinations/HadoopFS-DirectoryTemplates.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"/></xref></entry>
                                    <entry>Template for creating output directories. You can use
                                        constants, field values, and datetime variables. <p>Output
                                            directories are created based on the smallest datetime
                                            variable in the template.</p></entry>
                                </row>
                                <row>
                                    <entry>Data Time Zone</entry>
                                    <entry>Time zone to use to create directories and evaluate where
                                        records are written.</entry>
                                </row>
                                <row>
                                    <entry>Time Basis <xref
                                            href="../Destinations/HadoopFS-TimeBasis.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_a4r_rkw_45"/></xref></entry>
                                    <entry>Time basis to use for creating output directories and
                                        writing records to the directories. Use one of the following
                                            expressions:<ul id="ul_ggs_43g_br">
                                            <li>${time:now()} - Uses the processing time as the time
                                                basis. </li>
                                            <li>${record:value("/&lt;date field>")} - Uses the time
                                                associated with the record as the time basis.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Max Records in a File</entry>
                                    <entry>Maximum number of records to be written to an output
                                        file. Additional records are written to a new file. <p>Use 0
                                            to opt out of this property.</p></entry>
                                </row>
                                <row>
                                    <entry>Max File Size (MB)</entry>
                                    <entry>Maximum size of an output file. Additional records are
                                        written to a new file. <p>Use 0 to opt out of this
                                            property.</p></entry>
                                </row>
                                <row>
                                    <entry>Compression Codec</entry>
                                    <entry>Program to use to compress output files:<ul
                                            id="ul_ltx_djg_br">
                                            <li>None </li>
                                            <li>gzip</li>
                                            <li>bzip2</li>
                                            <li>Snappy</li>
                                            <li>Other - Use for LZO or other compression types. </li>
                                        </ul><p>LZO compression require additional configuration.
                                            For more information, see <xref
                                                href="../Destinations/HadoopFS-Comp-LZO.dita"
                                        />.</p></entry>
                                </row>
                                <row>
                                    <entry>Compression Codec Class</entry>
                                    <entry>Full class name of the other compression codec that you
                                        want to use. </entry>
                                </row>
                                <row>
                                    <entry>Sequence File Key</entry>
                                    <entry>Record key for creating Hadoop sequence files. Use one of
                                        the following options:<ul id="ul_xzr_vkg_br">
                                            <li>${record:value("/&lt;field name>")}</li>
                                            <li>${uuid()}</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Compression Type</entry>
                                    <entry>Compression type for sequence files when using a
                                        compression codec:<ul id="ul_etm_1lg_br">
                                            <li>Block Compression</li>
                                            <li>Record Compression</li>
                                        </ul></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>FS-LateRecords</uicontrol> - for
                        Hadoop FS and Local FS</draft-comment>
                </cmd>
            </step>
            <step id="FS-LateRecords">
                <cmd>On the <wintitle>Late Records</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <note>These properties are only relevant for a time basis based on the time of a
                        record.</note>
                    <table frame="all" rowsep="1" colsep="1" id="table_wv3_xzd_br">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3*"/>
                            <thead>
                                <row>
                                    <entry>Late Records Property <xref
                                            href="../Destinations/HadoopFS-LateRecordHandling.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_skv_3kw_45"/>
                                        </xref>
                                    </entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Late Record Time Limit (secs)</entry>
                                    <entry>Time limit for output directories to accept data. <p>You
                                            can enter a time in seconds, or use the expression to
                                            enter a time in hours. You can also use MINUTES in the
                                            default expression to define the time in minutes.
                                        </p></entry>
                                </row>
                                <row>
                                    <entry>Late Record Handling</entry>
                                    <entry>Determines how to handle late records:<ul
                                            id="ul_gx4_c12_br">
                                            <li>Send to error - Sends the record to the stage for
                                                error handling. </li>
                                            <li>Send to late records file - Sends the record to a
                                                late records file.</li>
                                        </ul></entry>
                                </row>
                                <row>
                                    <entry>Late Record Directory Template <xref
                                            href="../Destinations/HadoopFS-DirectoryTemplates.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                id="image_blv_3kw_45"/></xref></entry>
                                    <entry>Template for creating late record directories. You can
                                        use constants, field values, and datetime variables.
                                            <p>Output directories are created based on the smallest
                                            datetime variable in the template.</p></entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>KafkaConfig</uicontrol> - used for
                        the Kafka Producer - Kafka tab properties. Rows in the table are used for
                        Configuring a Pipeline, error handling > Write to Kafka.</draft-comment>
                </cmd>
            </step>
            <step id="KafkaConfig">
                <cmd>On the <wintitle>Kafka</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <p>
                        <table frame="all" rowsep="1" colsep="1" id="KafkaTableProperties">
                            <tgroup cols="2">
                                <colspec colname="c1" colnum="1" colwidth="1.25*"/>
                                <colspec colname="c2" colnum="2" colwidth="3.25*"/>
                                <thead>
                                    <row>
                                        <entry>Kafka Properties</entry>
                                        <entry>Description</entry>
                                    </row>
                                </thead>
                                <tbody>
                                    <row id="KPBrokerURI">
                                        <entry>Broker URI</entry>
                                        <entry>Connection string for the Kafka broker. Use the
                                            following format:
                                                <codeph>&lt;host>:&lt;port></codeph>.<p>To ensure a
                                                connection, enter a comma-separated list of
                                                additional broker URI.</p></entry>
                                    </row>
                                    <row id="KPRuntimeTopic">
                                        <entry>Runtime Topic Resolution <xref
                                                href="../Destinations/KProducer-RuntimeResolution.dita"
                                                  ><image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_ekt_x5g_cs"/>
                                            </xref></entry>
                                        <entry>Evaluates an expression at runtime to determine the
                                            topic to use for each record.</entry>
                                    </row>
                                    <row id="KPTopic">
                                        <entry>Topic</entry>
                                        <entry>Kafka topic to use. <p>Not available when using
                                                runtime topic resolution.</p></entry>
                                    </row>
                                    <row id="KPTopicEx">
                                        <entry>Topic Expression</entry>
                                        <entry>Expression used to determine where each record is
                                            written when using runtime topic resolution. Use an
                                            expression that evaluates to a topic name. </entry>
                                    </row>
                                    <row id="KPTopicWList">
                                        <entry>Topic White List</entry>
                                        <entry>List of valid topic names to write to when using
                                            runtime topic resolution. Use to avoid writing to
                                            invalid topics. Records that resolve to invalid topic
                                            names are passed the stage for error handling. <p>Use an
                                                asterisk (*) to allow writing to any topic name. By
                                                default, all topic names are valid.</p></entry>
                                    </row>
                                    <row id="KPPartStrategy">
                                        <entry>Partition Strategy </entry>
                                        <entry>Strategy to use to write to partitions:<ul
                                                id="ul_tq2_yr3_br">
                                                <li>Round Robin - Takes turns writing to different
                                                  partitions.</li>
                                                <li>Random - Writes to partitions randomly.</li>
                                                <li>Expression - Uses an expression to write data to
                                                  different partitions. </li>
                                            </ul></entry>
                                    </row>
                                    <row id="KPPartExpr">
                                        <entry>Partition Expression <xref
                                                href="../Destinations/KProducer-PartitionStrategy.dita#concept_qpm_xp4_4r">
                                                <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_as2_sc1_ft"/></xref></entry>
                                        <entry>Expression to use when using the expression partition
                                            strategy. <p>Define the expression to evaluate to the
                                                partition where you want each record written.
                                                Partition numbers start with 0.</p><p
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/EEditor"
                                            /></entry>
                                    </row>
                                    <row>
                                        <entry>Data Format</entry>
                                        <entry>Data format for output messages:<ul
                                                id="ul_wlj_353_br">
                                                <li>Text</li>
                                                <li>JSON</li>
                                                <li>Delimited</li>
                                                <li>SDC Record <xref
                                                  href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                                  <image href="../Graphics/icon_moreInfo.png"
                                                  scale="10" id="image_ucp_chr_br"/></xref></li>
                                                <li>Avro</li>
                                                <li>Binary</li>
                                            </ul></entry>
                                    </row>
                                    <row
                                        conref="ReusableTables.dita#concept_wfr_rnw_yq/D-CHARSET-other">
                                        <entry/>
                                    </row>
                                    <row id="KPOneMessPBatch">
                                        <entry>One Message per Batch</entry>
                                        <entry>For each batch, writes the records to each partition
                                            as a single message. </entry>
                                    </row>
                                    <row id="KPKConfigs">
                                        <entry>Kafka Configuration</entry>
                                        <entry>Additional Kafka properties to use. Click the
                                                <uicontrol>Add</uicontrol> icon and define the Kafka
                                            property name and value. <p>Use the property names and
                                                values as expected by Kafka. Do not use the
                                                broker.list property.</p><p>Several properties are
                                                defined by default. You can edit or delete the
                                                properties. </p></entry>
                                    </row>
                                </tbody>
                            </tgroup>
                        </table>
                    </p>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>RPCdest</uicontrol> - Used for Config
                        RPC Dest and pipeline error handling > Write to pipeline </draft-comment>
                </cmd>
            </step>
            <step id="RPCdest">
                <cmd>On the <wintitle>RPC</wintitle> tab, configure the following properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_pcc_mgx_dt">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.5*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>RPC Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-RPCconnect">
                                    <entry>RPC Connection  <xref
                                            href="../Destinations/RPCdest-Connections.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_bb2_k4b_ft"
                                        /></xref></entry>
                                    <entry>Connection information for the destination pipeline to
                                        continue processing data. Use the following format:
                                            <codeph>&lt;host>:&lt;port></codeph>. <p>Use a single
                                            RPC connection for each destination pipeline. Add
                                            additional connections as needed.</p><p>Use the port
                                            number when you configure the SDC RPC origin that
                                            receives the data.</p></entry>
                                </row>
                                <row id="row-RPCID">
                                    <entry>RPC ID</entry>
                                    <entry>User-defined ID to allow the destination to pass data to
                                        an SDC RPC origin. Use this ID in all SDC RPC origins to
                                        process data from the destination.</entry>
                                </row>
                                <row id="row-SSLenabled">
                                    <entry>TLS Enabled <xref
                                            href="../RPC_Pipelines/EnablingEncryption.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline" id="image_a5x_jzn_vs"
                                        /></xref></entry>
                                    <entry>Enables the secure transfer of data using TLS. <p>To use
                                            encryption, both the SDC RPC origin and SDC RPC
                                            destination must be enabled for TLS.</p></entry>
                                </row>
                                <row id="row-TrustStore">
                                    <entry>Truststore File</entry>
                                    <entry>Truststore file for TLS. Required if the keystore file is
                                        a self-signed certificate.<p>Must be stored in the <ph
                                                conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                            /> resources directory:
                                                <filepath>&lt;SDCinstalldir>/resources</filepath>.</p></entry>
                                </row>
                                <row id="row-TSpass">
                                    <entry>Truststore Password</entry>
                                    <entry>Password for the truststore file.</entry>
                                </row>
                                <row id="row-verifyHost">
                                    <entry>Verify Host in Server Certificate</entry>
                                    <entry>Verifies the host in the SDC RPC origin keystore
                                        file.</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
            <step>
                <cmd>
                    <draft-comment author="Loretta"><uicontrol>RPCdestAdv </uicontrol>- Used in RPC
                        destination &amp; pipeline error config - write to pipeline.</draft-comment>
                </cmd>
            </step>
            <step id="RPCadv">
                <cmd>On the <wintitle>Advanced</wintitle> tab, configure the following
                    properties:</cmd>
                <info>
                    <table frame="all" rowsep="1" colsep="1" id="table_mhd_nd1_ft">
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="3.5*"/>
                            <thead>
                                <row>
                                    <entry>Advanced Property</entry>
                                    <entry>Description</entry>
                                </row>
                            </thead>
                            <tbody>
                                <row id="row-RetriesBatch">
                                    <entry>Retries Per Batch</entry>
                                    <entry>Number of times the destination tries to write a batch to
                                        the SDC RPC origin. <p>When the destination cannot write the
                                            batch within the configured number of retries, it fails
                                            the batch.</p><p>Default is 3.</p></entry>
                                </row>
                                <row id="row-ConTimeout">
                                    <entry>Connection Timeout (ms)</entry>
                                    <entry>Milliseconds to establish a connection to the SDC RPC
                                        origin. <p>The destination retries the connection based on
                                            the Retries Per Batch property.</p><p>Default is 5000
                                            milliseconds.</p></entry>
                                </row>
                                <row id="row-ReadTimeout">
                                    <entry>Read Timeout (ms)</entry>
                                    <entry>Milliseconds to wait for the SDC RPC origin to read data
                                        from a batch. <p>The destination retries the write based on
                                            the Retries Per Batch property.</p><p>Default is 2000
                                            milliseconds.</p></entry>
                                </row>
                                <row id="row-UseCompression">
                                    <entry>Use Compression  <xref
                                            href="../Destinations/RPC-Compression.dita">
                                            <image href="../Graphics/icon_moreInfo.png" scale="10"
                                                placement="inline"
                                        /></xref></entry>
                                    <entry>Enables the destination to use compression to pass data
                                        to the SDC RPC origin. Enabled by default. </entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </info>
            </step>
        </steps>
    </taskbody>
</task>
