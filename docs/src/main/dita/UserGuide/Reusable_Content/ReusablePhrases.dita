<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_vhs_5tz_xp">
      <title>Reusable phrases for the book</title>
      <shortdesc>Use the following reusable phrases for conrefs in the book. <draft-comment
                  author="Loretta">Company name - not really using this</draft-comment></shortdesc>
      <conbody>
            <p>
                  <note>Do NOT use conrefs in headings - it causes problems in the webhelp
                        navigation panel.</note>
            </p>
            <p>Company name: <ph id="company">StreamSets</ph></p>
            <draft-comment author="Loretta">product name: trying to consistently use
                  this</draft-comment>
            <p>Full, init cap version of the product name: <ph id="pName-long">Data
                  Collector</ph></p>
            <p>
                  <draft-comment author="Loretta">KafkaSecure - used in KConsumer &amp; Producer
                        encryption areas. Original value: Kafka 0.9.0.0</draft-comment>
            </p>
            <p>Kafka version that supports SSL and SASL/Kerberos security: <ph id="KafkaSecure"
                        >Kafka 0.9.0.0</ph></p>
            <draft-comment author="Loretta">Using this in all DC Console topics:</draft-comment>
            <p>
                  <note id="Note-OptionDispay">Some icons and options might not display in the
                        console. The items that display are based on the task that you are
                        performing and roles assigned to your user account. </note>
            </p>
            <p>
                  <draft-comment author="Loretta">
                        <p>The following bullets are used in "Previewing a Single Stage" and
                              "Troubleshooting":</p>
                  </draft-comment>
            </p>
            <ul id="ul_EditPreview">
                  <li>The output data column for an origin.</li>
                  <li>The input data column for processors.</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following bullets are the types of origins
                        that can be reset / that remember where you left off. These are used
                        currently in "Starting a Pipeline" and "Resetting an
                        Origin":</draft-comment>
                  <ul id="ul_saveOffset">
                        <li>Directory</li>
                        <li>Kafka Consumer</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">The following bullets are CSV file types. Used in
                        "Configuring a Directory Origin", Directory-Data Formats and "Configuring
                        the Kafka Consumer" -- Make sure changes to this are copied to the Delimited
                        data format info later in this SAME FILE.</draft-comment>
            </p>
            <p>
                  <ul id="ul_delFileTypes">
                        <li><uicontrol>Default CSV</uicontrol> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>
                        <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>
                        <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel comma-separated
                              file.</li>
                        <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma separated file.</li>
                        <li><uicontrol>Tab-Separated Values</uicontrol> - File that includes
                              tab-separated values.</li>
                        <li><uicontrol>Custom</uicontrol> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">Use the following for invoking the expression editor. So
                  far, using in Config Expression Evaluator, Config Stream Selector, and Expression
                  Editor:</draft-comment>
            <p id="EEditor">Optionally, click <uicontrol>Ctrl + Space Bar</uicontrol> for help with
                  creating the expression. </p>
            <p>
                  <draft-comment author="Loretta">Using the following in the Configuring topic for
                        several processors (Expression Evaluator, Field Converter, Field Hasher, -
                        whichever ones allow wildcard use:</draft-comment>
            </p>
            <p id="wildcard">You can use the asterisk wildcard to represent array indices and map
                  elements. <xref
                        href="../Pipeline_Configuration/WildcardsArraysMaps.dita#concept_vqr_sqc_wr"
                              ><image href="../Graphics/icon_moreInfo.png" scale="10"/>
                  </xref>
            </p>
            <p>
                  <draft-comment author="Loretta">DataFormats-ALL - used in Kafka Consumer, MapR
                        Streams Consumer, possibly others.</draft-comment>
            </p>
            <p>
                  <ul id="DataFormats-ALL">
                        <li>Avro</li>
                        <li>Binary</li>
                        <li>Delimited</li>
                        <li>JSON</li>
                        <li>Log</li>
                        <li>Text</li>
                        <li>Protobuf</li>
                        <li>SDC Record <xref
                                    href="../Pipeline_Design/SDCRecordFormat.dita#concept_qkk_mwk_br">
                                    <image href="../Graphics/icon_moreInfo.png" scale="11"
                                          id="image_wjh_ycl_br"/></xref></li>
                        <li>XML</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">ORIGIN DATA FORMATS - from Directory. DLEntries
                        are conrefed by Kafka Consumer and Hadoop FS when possible. Also Amazon S3.
                        --- NOTE: Content copied to HTTP Client and altered. When making changes
                        here, check there as well.</draft-comment>
            </p>
            <p>
                  <dl id="ORIGIN-DFormats">
                        <dlentry id="OriginDF-TEXT">
                              <dt>Text</dt>
                              <dd>Generates a record for each line of text. </dd>
                              <dd>When a line exceeds the maximum line length defined for the
                                    origin, the origin truncates the line. The origin adds a boolean
                                    field named Truncated to indicate if the line was
                                    truncated.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSON">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage. </dd>
                        </dlentry>
                        <dlentry id="OriginDF-JSONFILE">
                              <dt>JSON</dt>
                              <dd>Generates a record for each JSON object. You can process JSON
                                    files that include multiple JSON objects or a single JSON
                                    array.</dd>
                              <dd>When an object exceeds the maximum object length defined for the
                                    origin, the origin cannot continue processing data in the file.
                                    Records already processed from the file are passed to the
                                    pipeline. The behavior of the origin is then based on the error
                                    handling configured for the stage:<ul id="ul_sjn_fxf_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="OriginDF-DELIM">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_c12_1k2_gt">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. </dd>
                              <dd>When a record exceeds the maximum record length defined for the
                                    origin, the origin processes the object based on the error
                                    handling configured for the stage.</dd>
                              <dd>For more information about the root field types, see <xref
                                          href="../Pipeline_Design/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-DELIMFILE">
                              <dt>Delimited</dt>
                              <dd>Generates a record for each delimited line. You can use the
                                    following delimited format types:<ul id="ul_hbd_f2p_ht">
                                          <li><uicontrol>Default CSV</uicontrol> - File that
                                                includes comma-separated values. Ignores empty lines
                                                in the file.</li>
                                          <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated
                                                file that strictly follows RFC4180 guidelines.</li>
                                          <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel
                                                comma-separated file.</li>
                                          <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma
                                                separated file.</li>
                                          <li><uicontrol>Tab-Separated Values</uicontrol> - File
                                                that includes tab-separated values.</li>
                                          <li><uicontrol>Custom</uicontrol> - File that uses
                                                user-defined delimiter, escape, and quote
                                                characters.</li>
                                    </ul></dd>
                              <dd>You can use a list or list-map root field type for delimited data,
                                    optionally including the header information when available. </dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_it3_g2p_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                              <dd>For more information about the root field types, see <xref
                                          href="../Pipeline_Design/DelimitedDataRootFieldTypes.dita#concept_zcg_bm4_fs"
                                    />.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-XML">
                              <dt>XML</dt>
                              <dd>Generates records based on the location of the XML element that
                                    you define as the record delimiter. If you do not define a
                                    delimiter element, the origin treats the XML file as a single
                                    record.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin skips the record and continues processing with the next
                                    record. It sends the skipped record to the pipeline for error
                                    handling. </dd>
                        </dlentry>
                        <dlentry id="OriginDF-XMLFILE">
                              <dt>XML</dt>
                              <dd>Generates records based on the location of the XML element that
                                    you define as the record delimiter. If you do not define a
                                    delimiter element, the origin treats the XML file as a single
                                    record.</dd>
                              <dd>When a record exceeds the user-defined maximum record length, the
                                    origin cannot continue processing data in the file. Records
                                    already processed from the file are passed to the pipeline. The
                                    behavior of the origin is then based on the error handling
                                    configured for the stage:<ul id="ul_rnb_qdp_ht">
                                          <li>Discard - The origin continues processing with the
                                                next file, leaving the partially-processed file in
                                                the directory. </li>
                                          <li>To Error - The origin continues processing with the
                                                next file. If a post-processing error directory is
                                                configured for the stage, the origin moves the
                                                partially-processed file to the error directory.
                                                Otherwise, it leaves the file in the directory.</li>
                                          <li>Stop Pipeline - The origin stops the pipeline. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="OriginDF-SDC">
                              <dt>SDC Record</dt>
                              <dd>Generates a record for every record. Use to process records
                                    generated by a <ph
                                          conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                                    /> pipeline using the SDC Record data format.</dd>
                              <dd>For error records, the origin  provides the original record as
                                    read from the origin in the original pipeline, as well as error
                                    information that you can use to correct the record. </dd>
                              <dd>When processing error records, the origin expects the error file
                                    names and contents as generated by the original pipeline.</dd>
                        </dlentry>
                        <dlentry id="OriginDF-LOG">
                              <dt>Log</dt>
                              <dd>Generates a record for every log line. </dd>
                              <dd>When a line exceeds the user-defined maximum line length, the
                                    origin truncates longer lines. </dd>
                              <dd>You can include the processed log line as a field in the record.
                                    If the log line is truncated, and you request the log line in
                                    the record, the origin includes the truncated line.</dd>
                              <dd>You can define the log format or type to be read.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO - Used by Kafka Consumer and Hadoop FS, and
                        Kinesis Consumer</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-AVRO">
                              <dt>Avro</dt>
                              <dd>Generates a record for every message. </dd>
                              <dd>To ensure proper data processing, indicate if the message includes
                                    an Avro schema. </dd>
                              <dd>You can use the schema associated with the message or provide an
                                    alternate schema definition. Providing an alternate schema can
                                    improve performance.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO-Files - used by Directory and Amazon
                        S3.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="OriginDF-AVROfiles">
                              <dt>Avro</dt>
                              <dd>Generates a record for every record. </dd>
                              <dd>The origin assumes each file includes an Avro schema definition.
                                    But you can optionally provide an alternate schema. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">AVRO-HadoopFS - used by Hadoop FS origin -
                        MapReduce doesn't allow getting the schema from the files.</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="AVRO-HadoopFS">
                              <dt>Avro</dt>
                              <dd>Generates a record for every record. </dd>
                              <dd>The origin uses the specified schema definition to process Avro
                                    data.</dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-Binary - used by Kafka Consumer and
                        Kinesis Consumer</draft-comment>
            </p>
            <p>
                  <dl>
                        <dlentry id="OriginDF-Binary">
                              <dt>Binary</dt>
                              <dd>Generates a record with a single byte array field at the root of
                                    the record. </dd>
                              <dd>When the data exceeds the user-defined maximum data size, the
                                    origin cannot process the data. Because the record is not
                                    created, the origin cannot pass the record to the pipeline to be
                                    written as an error record. Instead, the origin generates a
                                    stage error. </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">OriginDF-ProtoMessage -  JMS Consumer, Kinesis
                        Consumer, Kafka Consumer, RabbitMQ</draft-comment>
                  <dl>
                        <dlentry id="OriginDF-ProtoMessage">
                              <dt>Protobuf</dt>
                              <dd>Generates a record for every protobuf message. By default, the
                                    origin assumes messages contain multiple protobuf messages.</dd>
                              <dd>Protobuf messages must match the specified message type and be
                                    described in the descriptor file. </dd>
                              <dd>When the data for a record exceeds 1 MB, the origin cannot
                                    continue processing data in the message. The origin handles the
                                    message based on the stage error handling property and continues
                                    reading the next message.  </dd>
                              <dd>For information about generating the descriptor file, see <xref
                                          href="../Pipeline_Design/Protobuf-Prerequisites.dita"
                                    />.</dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta">OriginDF-ProtoFile - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="OriginDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Generates a record for every protobuf message. </dd>
                        <dd>Protobuf messages must match the specified message type and be described
                              in the descriptor file. </dd>
                        <dd>When the data for a record exceeds 1 MB, the origin cannot continue
                              processing data in the file. The origin handles the file based on file
                              error handling properties and continues reading the next file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta">DESTINATION DATA FORMATS. Used by Kafka and Flume.
                        They also have additional data formats specific to each destination. Hadoop
                        FS and Local FS conref individual data formats</draft-comment>
            </p>
            <p>
                  <draft-comment author="Loretta"><b>DESTDataF-Text</b> and others in this list are
                        used by Kafka, Flume, Hadoop FS, Local FS. </draft-comment>
                  <dl id="DESTDataFormats">
                        <dlentry id="DESTDataF-Text">
                              <dt>Text</dt>
                              <dd>The destination writes a single text field of a record. When you
                                    configure the stage, you select the field to use. When
                                    necessary, merge record data into the field earlier in the
                                    pipeline. </dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-JSON">
                              <dt>JSON</dt>
                              <dd>The destination writes records as JSON data. You can use one of
                                    the following formats:<ul id="ul_dd1_5y1_wr">
                                          <li>Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>
                                          <li>Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-Delim">
                              <dt>Delimited</dt>
                              <dd>The destination writes records as delimited data. When you use
                                    this data format, you must ensure that the data uses the
                                    following data structure:<ul id="ul_tr1_ms1_wr">
                                          <li>A root field that contains an array of maps.</li>
                                          <li>Each map includes a <term>value</term> element and an
                                                optional <term>header</term> element.</li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry>
                              <dt/>
                              <dd>
                                    <draft-comment author="Loretta"><b>AvroFile</b> used by Local
                                          and Hadoop FS, S3, and other file destinations.
                                                <b>AvroMess</b> used by Kafka and Kinesis Producer
                                          and other message destinations. When making changes to
                                          these, add updates to the Flume data formats topic when
                                          necessary. </draft-comment>
                              </dd>
                        </dlentry>
                        <dlentry id="DEST-DF-AvroFILE">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema that you
                                    provide. The schema definition is included in each file.</dd>
                              <dd>You can also compress data with an Avro compression codec. When
                                    using Avro compression, do not use other compression available
                                    in the destination. </dd>
                        </dlentry>
                        <dlentry id="D-DF-AvroMess">
                              <dt>Avro</dt>
                              <dd>The destination writes records based on the Avro schema that you
                                    provide. </dd>
                              <dd>You can optionally include the schema definition as part of the
                                    message. Omitting the schema definition can improve performance,
                                    but requires the appropriate schema management to avoid losing
                                    track of the schema associated with the data.</dd>
                              <dd>You can also compress data with an Avro compression codec. When
                                    using Avro compression, do not use other compression available
                                    in the destination. </dd>
                        </dlentry>
                        <dlentry id="DEST-DataF-SDC">
                              <dt>SDC Record</dt>
                              <dd>The destination writes records in the SDC Record data format.
                              </dd>
                        </dlentry>
                  </dl>
            </p>
            <draft-comment author="Loretta"><b>DESTDataFormat-Binary</b> - the following is just for
                  Kafka Producer and Amazon S3.</draft-comment>
            <dl>
                  <dlentry id="DESTDataFormat-Binary">
                        <dt>Binary</dt>
                        <dd>The destination writes binary data from a single field in the record.
                        </dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DestDF-ProtoMess</b> - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoMess">
                        <dt>Protobuf</dt>
                        <dd>Writes one record in a message. Uses the user-defined message type and
                              the definition of the message type in the descriptor file to generate
                              the message. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <draft-comment author="Loretta"><b>DestDF-ProtoFile</b> - Amazon S3, Directory? Hadoop
                  FS?</draft-comment>
            <dl>
                  <dlentry id="DestDF-ProtoFile">
                        <dt>Protobuf</dt>
                        <dd>Writes a batch of messages in each file. </dd>
                        <dd>Uses the user-defined message type and the definition of the message
                              type in the descriptor file to generate the messages in the file. </dd>
                        <dd>For information about generating the descriptor file, see <xref
                                    href="../Pipeline_Design/Protobuf-Prerequisites.dita"/>.</dd>
                  </dlentry>
            </dl>
            <p>
                  <draft-comment author="Loretta">The following paragraphs are used for Expressions
                        (Pipeline Config chapter) and Expression Language (appendix).
                  </draft-comment>
            </p>
            <p id="EXP-p1">Use the expression language to configure expressions and conditions in
                  processors, such as the Expression Evaluator or Stream Selector. Some destination
                  properties also require the expression language, such as the directory template
                  for Hadoop FS. </p>
            <p id="EXP-p2">Optionally, you can use the expression language to define any stage or
                  pipeline property that represents a numeric or string value. This allows you to
                  use constants or runtime properties throughout the pipeline. You can use
                  expression completion to determine where you can use an expression and the
                  expression elements that you can use in that location. </p>
            <p id="EXP-p3">You can use the following elements in an expression:<ul
                        id="ul_w34_vxl_2s">
                        <li>Constants</li>
                        <li>Field names</li>
                        <li>Functions</li>
                        <li>Literals</li>
                        <li>Operators</li>
                        <li>Runtime properties</li>
                        <li>Runtime resources</li>
                  </ul></p>
            <draft-comment author="Loretta">The following bullets are used in Troubleshooting >
                  Accessing Error Message and Tutorial > Run the Extended Pipeline</draft-comment>
            <ul id="ul_ErrorInformation">
                  <li>errorStage - The stage in the pipeline that generated the error.</li>
                  <li>errorCode - The error code associated with the error message.</li>
                  <li>errorMessage - The text of the error message. </li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following is used in the Tutorial chapter, in
                        tables in the Creating a Pipeline... topic and the Write to the Destination
                        topic.</draft-comment>
            </p>
            <p id="FilePrefix">By default, the files are prefixed with "SDC" and an expression that
                  returns the <ph conref="#concept_vhs_5tz_xp/pName-long"/> ID, but that's more than
                  we need here. </p>
            <draft-comment author="Loretta">The following is used in HDFS origin and destination
                  overviews:</draft-comment>
            <p id="HDFS_user_props">When necessary, you can enable Kerberos authentication or use an
                  HDFS user to connect to HDFS. You can also use HDFS configuration files and add
                  other HDFS configuration properties as needed. </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by Hadoop FS
                  origin - used in Hadoop Properties and Configuring Hadoop FS
                  origin.</draft-comment>
            <p>
                  <ul id="ul-HDFSfiles_HDFSorigin">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                        <li>yarn-site.xml</li>
                        <li>mapred-site.xml</li>
                  </ul>
                  <draft-comment author="Loretta" id="ul_">list of HDFS configuration files used by
                        Hadoop FS destination - used in Hadoop Properties and Configuring Hadoop FS
                        destination.</draft-comment>
                  <ul id="HDFSfiles_HDFSdest">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                  </ul>
            </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by HBase
                  destination - used in Hadoop Properties and Configuring HBase
                  destination.</draft-comment>
            <ul id="HDFSfiles_HBasedest">
                  <li>hbase-site.xml</li>
            </ul>
            <draft-comment author="Loretta">List of config files used by Hive Streaming - used in
                  Hive Properties &amp; Configuring Hive Streaming dest.</draft-comment>
            <ul id="HiveStreamingFiles">
                  <li>core-site.xml</li>
                  <li>hdfs-site.xml</li>
                  <li>hive-site.xml</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">DRIFT-IgnoreWMissing - Using this in the Data
                        Drift Functions topic:</draft-comment>
            </p>
            <p id="DRIFT-ignoreWMissing">Use the ignoreWhenMissing flag to determine the behavior
                  when the field is missing. When set to "true", a missing field causes no errors.
                  When set to "false", a missing field generates an alert for the record missing the
                  field, and for the next record that includes the field.</p>
            <p>
                  <draft-comment author="Loretta">ul-KERBprops - Kerberos properties in Data
                        Collector config file:</draft-comment>
            </p>
            <p>
                  <ul id="ul-KERBprops">
                        <li>kerberos.client.enabled</li>
                        <li>kerberos.client.principal</li>
                        <li>kerberos.client.keytab</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">Kafka-SSL - using this ordered list for KConsumer
                        and KProducer</draft-comment>
            </p>
            <ol id="KAFKA-SSL-list">
                  <li>To use SSL to connect, first make sure Kafka is configured for SSL as
                        described in the Kafka documentation: <xref
                              href="http://kafka.apache.org/documentation.html#security_ssl"
                              format="html" scope="external"/>. </li>
                  <li id="KAFKASetStage">On the <uicontrol>General</uicontrol> tab of the stage, set
                        the <uicontrol>Stage Library</uicontrol> property to <uicontrol>Apache Kafka
                              0.9.0.0</uicontrol>.</li>
                  <li>On the <wintitle>Kafka</wintitle> tab, add the
                              <uicontrol>security.protocol</uicontrol> Kafka configuration property
                        and set it to <uicontrol>SSL</uicontrol>.</li>
                  <li id="KafkaSSLprops">Then, add the following SSL Kafka configuration
                              properties:<ul id="ul_hkf_ydb_t5">
                              <li>ssl.truststore.location</li>
                              <li>ssl.truststore.password</li>
                        </ul><p>When the Kafka broker requires client authentication - when the
                              ssl.client.auth broker property is set to "required" - add and
                              configure the following properties: <ul id="ul_y5b_rml_55">
                                    <li>ssl.keystore.location</li>
                                    <li>ssl.keystore.password</li>
                                    <li>ssl.key.password</li>
                              </ul></p><p>Some brokers might require adding the following properties
                              as well:<ul>
                                    <li>ssl.enabled.protocols</li>
                                    <li>ssl.truststore.type</li>
                                    <li>ssl.keystore.type</li>
                              </ul></p><p>For details about these properties, see the Kafka
                              documentation.</p></li>
            </ol>
            <p>
                  <draft-comment author="Loretta">Kafka-Kerberos - using this in KConsumer and
                        KProducer</draft-comment>
            </p>
            <p>
                  <ol id="Kafka-Kerberos">
                        <li>To use Kerberos, first make sure Kafka is configured for Kerberos as
                              described in the Kafka documentation: <xref
                                    href="http://kafka.apache.org/documentation.html#security_sasl"
                                    format="html" scope="external"/>.</li>
                        <li id="KafkaKERBsdcprops">In the <ph
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> configuration file, <codeph>$SDC_CONF/sdc.properties</codeph>, make sure the following Kerberos properties are
                                    configured:<ul
                                    conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul-KERBprops"
                                    id="ul_gyj_jpf_55">
                                    <li/>
                              </ul></li>
                        <li conref="#concept_vhs_5tz_xp/KAFKASetStage"/>
                        <li>On the <wintitle>Kafka</wintitle> tab, add the
                                    <uicontrol>security.protocol</uicontrol> Kafka configuration
                              property, and set it to <uicontrol>SASL_PLAINTEXT</uicontrol>.</li>
                        <li id="KafkaKERBservicename">Then, add the
                                    <uicontrol>sasl.kerberos.service.name</uicontrol> configuration
                              property, and set it to the Kerberos principal name that Kafka runs
                              as. </li>
                  </ol>
            </p>
            <p>
                  <draft-comment author="Loretta">KAFKA-SSLandKerberos - used in KConsumer and
                        KProducer</draft-comment>
            </p>
            <p>
                  <ol id="KAFKA-SSLandKerberos">
                        <li>Make sure Kafka is configured to use SSL and Kerberos (SASL) as
                              described in the following Kafka documentation:<ul id="ul_gx4_1sk_55">
                                    <li><xref
                                                href="http://kafka.apache.org/documentation.html#security_ssl"
                                                format="html" scope="external"/></li>
                                    <li><xref
                                                href="http://kafka.apache.org/documentation.html#security_sasl"
                                                format="html" scope="external"/></li>
                              </ul></li>
                        <li conref="#concept_vhs_5tz_xp/KafkaKERBsdcprops"/>
                        <li conref="#concept_vhs_5tz_xp/KAFKASetStage"/>
                        <li>On the <wintitle>Kafka</wintitle> tab, add the
                                    <uicontrol>security.protocol</uicontrol> property and set it to
                                    <uicontrol>SASL_SSL</uicontrol>.</li>
                        <li conref="#concept_vhs_5tz_xp/KafkaKERBservicename"/>
                        <li conref="#concept_vhs_5tz_xp/KafkaSSLprops"/>
                  </ol>
            </p>
            <draft-comment author="Loretta">UseDefaults: Using this phrase throughout the tutorial
                  as a disclaimer:</draft-comment>
            <p id="UseDefaults">Use the defaults for properties that aren't listed:</p>
            <p>
                  <draft-comment author="Loretta">MapR-Prereq - Using in all MapR stages to make
                        sure they configure SDC.</draft-comment>
            </p>
            <p id="MapRPrereq">Before you use any MapR stage in a pipeline, you must perform
                  additional steps to enable the <ph conref="#concept_vhs_5tz_xp/pName-long"/> to
                  process MapR data. For more information, see <xref
                        href="../Install_Config/MapR-Prerequisites.dita#concept_jgs_qpg_2v"/>.
</p>
      </conbody>
</concept>
