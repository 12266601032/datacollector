<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_vhs_5tz_xp">
      <title>Reusable phrases for the book</title>
      <shortdesc>Use the following reusable phrases for conrefs in the book:<draft-comment
                  author="Loretta">Company name - not really using this</draft-comment></shortdesc>
      <conbody>
            <p>Company name: <ph id="company">StreamSets</ph></p>
            <draft-comment author="Loretta">product name: trying to consistently use
                  this</draft-comment>
            <p>Full, init cap version of the product name: <ph id="pName-long">Data
                  Collector</ph></p>
            <draft-comment author="Loretta">Using this in all DC Console topics:</draft-comment>
            <p>
                  <note id="Note-OptionDispay">Some icons and options might not display in the
                        console. The items that display are based on the task that you are
                        performing and roles assigned to your user account. </note>
            </p>
            <p>
                  <draft-comment author="Loretta">
                        <p>The following bullets are used in "Previewing a Single Stage" and
                              "Troubleshooting":</p>
                  </draft-comment>
            </p>
            <ul id="ul_EditPreview">
                  <li>The output data column for an origin.</li>
                  <li>The input data column for processors.</li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following bullets are the types of origins
                        that can be reset / that remember where you left off. These are used
                        currently in "Starting a Pipeline" and "Resetting an
                        Origin":</draft-comment>
                  <ul id="ul_saveOffset">
                        <li>Directory</li>
                        <li>Kafka Consumer</li>
                  </ul>
            </p>
            <p>
                  <draft-comment author="Loretta">The following bullets are CSV file types. Used in
                        "Configuring a Directory Origin", Directory-Data Formats and "Configuring
                        the Kafka Consumer"</draft-comment>
            </p>
            <p>
                  <ul id="ul_delFileTypes">
                        <li><uicontrol>Default CSV</uicontrol> - File that includes comma-separated
                              values. Ignores empty lines in the file.</li>
                        <li><uicontrol>RFC4180 CSV</uicontrol> - Comma-separated file that strictly
                              follows RFC4180 guidelines.</li>
                        <li><uicontrol>MS Excel CSV</uicontrol> - Microsoft Excel comma-separated
                              file.</li>
                        <li><uicontrol>MySQL CSV</uicontrol> - MySQL comma separated file.</li>
                        <li><uicontrol>Tab-Separated Values</uicontrol> - File that includes
                              tab-separated values.</li>
                        <li><uicontrol>Custom</uicontrol> - File that uses user-defined delimiter,
                              escape, and quote characters.</li>
                  </ul>
            </p>
            <draft-comment author="Loretta">Use the following for invoking the expression editor. So
                  far, using in Config Expression Evaluator, Config Stream Selector, and Expression
                  Editor:</draft-comment>
            <p id="EEditor">Optionally, click <uicontrol>Ctrl + Space Bar</uicontrol> for help with
                  creating the expression. </p>
            <p>
                  <draft-comment author="Loretta">Using the following in the Configuring topic for
                        several processors (Expression Evaluator, Field Converter, Field Hasher, -
                        whichever ones allow wildcard use:</draft-comment>
            </p>
            <p id="wildcard">You can use the asterisk wildcard to represent array indices and map
                  elements. <xref
                        href="../Pipeline_Configuration/WildcardsArraysMaps.dita#concept_vqr_sqc_wr"
                              ><image href="../Graphics/icon_moreInfo.png" scale="10"/>
                  </xref>
            </p>
            <p>
                  <draft-comment author="Loretta">The following list is destination data formats.
                        Used by Kafka and Flume. They also have an individual Avro description added
                        to their list in the local file. Copying all but SDC Record to Hadoop FS
                        destination.</draft-comment>
                  <dl id="DataFormats-dest">
                        <dlentry>
                              <dt>Text</dt>
                              <dd>The destination writes a single text field of a record. When you
                                    configure the stage, you select the field to use. When
                                    necessary, merge record data into the field earlier in the
                                    pipeline. </dd>
                        </dlentry>
                        <dlentry>
                              <dt>JSON</dt>
                              <dd>The destination writes records as JSON data. You can use one of
                                    the following formats:<ul id="ul_dd1_5y1_wr">
                                          <li>Array - Each file includes a single array. In the
                                                array, each element is a JSON representation of each
                                                record.</li>
                                          <li>Multiple objects - Each file includes multiple JSON
                                                objects. Each object is a JSON representation of a
                                                record. </li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry>
                              <dt>Delimited</dt>
                              <dd>The destination writes records as delimited data. When you use
                                    this data format, you must ensure that the data uses the
                                    following data structure:<ul id="ul_tr1_ms1_wr">
                                          <li>A root field that contains an array of maps.</li>
                                          <li>Each map includes a <term>value</term> element and an
                                                optional <term>header</term> element.</li>
                                    </ul></dd>
                        </dlentry>
                        <dlentry>
                              <dt>SDC Record</dt>
                              <dd>The destination writes records in the SDC Record data format.
                              </dd>
                        </dlentry>
                  </dl>
            </p>
            <p>
                  <draft-comment author="Loretta">The following paragraphs are used for Expressions
                        (Pipeline Config chapter) and Expression Language (appendix).
                  </draft-comment>
            </p>
            <p id="EXP-p1">Use the expression language to configure expressions and conditions in
                  processors, such as the Expression Evaluator or Stream Selector. Some destination
                  properties also require the expression language, such as the directory template
                  for Hadoop FS. </p>
            <p id="EXP-p2">Optionally, you can use the expression language to define any stage or
                  pipeline property that represents a numeric or string value. This allows you to
                  use constants or runtime properties throughout the pipeline. You can use
                  expression completion determine where you can use an expression and the expression
                  elements that you can use in that location. </p>
            <p id="EXP-p3">You can use the following elements in an expression:<ul
                        id="ul_w34_vxl_2s">
                        <li>Constants</li>
                        <li>Field names</li>
                        <li>Functions</li>
                        <li>Literals</li>
                        <li>Operators</li>
                        <li>Runtime properties</li>
                        <li>Runtime resources</li>
                  </ul></p>
            <draft-comment author="Loretta">The following bullets are used in Troubleshooting >
                  Accessing Error Message and Tutorial > Run the Extended Pipeline</draft-comment>
            <ul id="ul_ErrorInformation">
                  <li>errorStage - The stage in the pipeline that generated the error.</li>
                  <li>errorCode - The error code associated with the error message.</li>
                  <li>errorMessage - The text of the error message. <draft-comment author="Loretta"
                              >Dev: I am listing the subset of key info - are there other ones I
                              should list here as particularly useful?</draft-comment></li>
            </ul>
            <p>
                  <draft-comment author="Loretta">The following is used in the Tutorial chapter, in
                        tables in the Creating a Pipeline... topic and the Write to the Destination
                        topic.</draft-comment>
            </p>
            <p id="FilePrefix">By default, the files are prefixed with "SDC" and an expression that
                  returns the <ph conref="#concept_vhs_5tz_xp/pName-long"/> ID, but that's more than
                  we need here. </p>
            <draft-comment author="Loretta">The following is used in HDFS origin and destination
                  overviews:</draft-comment>
            <p id="HDFS_user_props">When necessary, you can enable Kerberos authentication or use an
                  HDFS user to connect to HDFS. You can also use HDFS configuration files and add
                  other HDFS configuration properties as needed. </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by Hadoop FS
                  origin - used in Hadoop Properties and Configuring Hadoop FS
                  origin.</draft-comment>
            <p>
                  <ul id="ul-HDFSfiles_HDFSorigin">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                        <li>yarn-site.xml</li>
                        <li>mapred-site.xml</li>
                  </ul>
                  <draft-comment author="Loretta" id="ul_">list of HDFS configuration files used by
                        Hadoop FS destination - used in Hadoop Properties and Configuring Hadoop FS
                        destination.</draft-comment>
                  <ul id="HDFSfiles_HDFSdest">
                        <li>core-site.xml</li>
                        <li>hdfs-site.xml </li>
                  </ul>
            </p>
            <draft-comment author="Loretta">list of HDFS configuration files used by HBase
                  destination - used in Hadoop Properties and Configuring HBase
                  destination.</draft-comment>
            <ul id="HDFSfiles_HBasedest">
                  <li>hbase-site.xml</li>
            </ul>
      </conbody>
</concept>
