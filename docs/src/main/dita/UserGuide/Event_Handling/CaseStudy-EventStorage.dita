<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_ocb_nnl_px">
 <title>Case Study: Event Storage </title>
 <conbody>
  <p><indexterm>event handling case studies<indexterm>event storage</indexterm></indexterm>Store
            event records to preserve an audit trail of the events that occur. You can store event
            records from any event-generating stage. For this case study, say you want to keep a log
            of the objects written to Amazon S3 by the following pipeline: </p>
        <p><image href="../Graphics/Event-Storage.png" id="image_csj_gwl_px" scale="60"/></p>
        <p>To do this, you simply:<ol id="ol_xgc_3wl_px">
                <li>Configure Amazon S3 to generate events.<p>On the <wintitle>General</wintitle>
                        tab, select the <uicontrol>Produce Events</uicontrol> property </p><p>Now
                        the event output stream becomes available and the destination generates an
                        event each time it completes writing to an object:</p><p><image
                            href="../Graphics/Event-Storage-S3.png" id="image_zyg_cxl_px" scale="60"
                        /></p></li>
                <li>You can write the event records to any destination, but let's assume you want to
                    write them to Amazon S3 as well: <p><image
                            href="../Graphics/Event-Storage-S3-S3.png" id="image_orv_fyl_px"
                            scale="55"/></p><p>You could be done right there, but you want to
                        include the time of the event in the record, so you know exactly when the
                        Amazon S3 destination stopped writing to an object. </p></li>
                <li> All event records include the event creation time in the
                    sdc.event.creation_timestamp record header attribute, so you can add an
                    Expression Evaluator to the pipeline and use the following expression to include
                    the creation time in the
                        record:<codeblock>${record:attribute('sdc.event.creation_timestamp')}</codeblock><p>The
                        resulting pipeline looks like this: </p><p><image
                            href="../Graphics/Event-Storage-EEval.png" id="image_sqc_3bq_tx"
                            scale="65"/></p><p>Note that event creation time is expressed as an
                        epoch or Unix timestamp, such as 1477698601031. And record header attributes
                        provide data as strings.</p><p>
                        <note type="tip">When writing to database or other destinations that
                            recognize non-string data types, you can use the Field Type Converter to
                            convert the string timestamp to Long to enable downstream processing.
                                <p>To convert the string timestamp to a human-readable datetime, use
                                a set of Field Type Converters: One to convert the string to Long
                                and one to convert the Long to the datetime format of your choice.
                                When writing to a file system, you might add an additional Field
                                Type Converter to convert the datetime back to a string.</p></note>
                    </p></li>
            </ol></p>
 </conbody>
</concept>
