<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cph_5h4_lx">
    <title>Event Handling</title>
    <conbody>
        <p><indexterm>event handling<indexterm>overview</indexterm></indexterm><term>Event
                handling</term> allows you to keep a record of events that occur as a pipeline runs,
            such as when an origin starts and completes reading a file. </p>
        <p>Event handling also enables you to perform event-driven, pipeline-related tasks in
            external systems, such as running a MapReduce job after a file is written to HDFS. </p>
        <p>To implement event handling, you can add an event stream to any pipeline that includes an
            event-generating stage. </p>
        <p>Event streams can include the following conceptual components:<dl>
                <dlentry>
                    <dt>event generation</dt>
                    <dd>Events are generated by an event-generating stage when a specific action
                        takes place. The action that generates an event is related to how the stage
                        processes data and can differ from stage to stage. </dd>
                    <dd>For example, the Hive Metastore destination updates Hive metadata, so the
                        destination generates an event each time it changes Hive table structures.
                        In contrast, the Hadoop FS destination writes files to HDFS, so it generates
                        events each time it closes a file. </dd>
                    <dd>When an event occurs, a stage generates an <term>event record</term> that
                        passes to the pipeline through an event output stream. Event streams cannot
                        be merged with data streams.</dd>
                </dlentry>
                <dlentry>
                    <dt>task execution</dt>
                    <dd>To use events to trigger a task, connect the event stream to an
                            <term>executor</term>. Executor stages perform tasks instead of
                        processing data.</dd>
                    <dd> Each time an executor receives an event record, it performs the specified
                        task.</dd>
                    <dd>For example, the Hive Query executor runs a user-defined query each time it
                        receives an event. Similarly, the MapReduce executor triggers a MapReduce
                        job when it receives an event. </dd>
                </dlentry>
                <dlentry>
                    <dt>event storage</dt>
                    <dd>To store event information, connect the event stream to a destination. You
                        can use processors to enrich the event record before passing it to the
                        destination. </dd>
                    <dd>Once written to a destination system, you can treat the event record like
                        any standard <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> record.</dd>
                </dlentry>
            </dl></p>
    </conbody>
</concept>
