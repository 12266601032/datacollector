<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_jx4_zym_vs">
 <title>Data Formats</title>
 <conbody>
  <p><indexterm>data formats<indexterm>for Hadoop FS</indexterm></indexterm><indexterm>Hadoop FS
     origin<indexterm>data formats</indexterm></indexterm>The Hadoop FS origin processes data
   differently based on the data format that you select. Hadoop FS processes the following types of
    data:<draft-comment author="Loretta">copied and updated from Directory, removing invalid data
    formats</draft-comment><dl>
    <dlentry>
     <dt>Text</dt>
     <dd>Generates a record for each line in the file. </dd>
     <dd>When a line exceeds the maximum line length defined for the origin, Hadoop FS truncates the
      line.</dd>
    </dlentry>
    <dlentry>
     <dt>JSON</dt>
     <dd>Generates a record for each JSON object. You can use JSON files that include multiple JSON
      objects or a single JSON array.</dd>
     <dd>When an object exceeds the maximum object length defined for the origin, Hadoop FS
      processes the object based on the error handling configured for the origin. </dd>
    </dlentry>
    <dlentry>
     <dt>Log</dt>
     <dd>Generates a record for every log line. </dd>
     <dd>When a line exceeds the maximum line length defined for the origin, Hadoop FS truncates
      longer lines. </dd>
     <dd>You can include the processed log line as a field in the record. If the log line is
      truncated, and you request the log line in the record, Hadoop FS includes the truncated
      line.</dd>
     <dd>You can define the log format or type to be read.</dd>
    </dlentry>
   </dl></p>
 </conbody>
</concept>
