<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_xgs_nlc_wr">
 <title>Data Formats</title>
 <conbody>
  <p><indexterm>data formats<indexterm>Kafka Consumer</indexterm></indexterm><indexterm>Kafka
        Consumer origin<indexterm>data formats</indexterm></indexterm>The Kafka Consumer origin
      processes data differently based on the data format. Kafka Consumer can process the following
      types of data:<draft-comment author="Loretta">Doc reminder: Updates to this section should be
        ported over to Directory-Data Formats - the origins handle data formats similarly. 4/28/15
        Also to JMS - but be careful of the differences. 8/24/15.</draft-comment></p>
  <dl>
   <dlentry>
    <dt>Text</dt>
    <dd>Generates a record for each line of text. </dd>
    <dd>When a line exceeds the maximum line length defined for the origin, the origin truncates the
          line. The origin adds a boolean field named Truncated to indicate if the line was
          truncated.</dd>
   </dlentry>
   <dlentry>
    <dt>JSON</dt>
    <dd>Generates a record for each JSON object. You can use JSON files that include multiple JSON
     objects or a single JSON array.</dd>
    <dd>When an object exceeds the maximum object length defined for the origin, Kafka Consumer
     processes the object based on the error handling configured for the origin. </dd>
   </dlentry>
   <dlentry>
    <dt>Delimited</dt>
    <dd>Generates a record for each delimited line. You can use the following delimited format
            types:<ul id="ul_a51_wzk_5q">
            <li>Default CSV (ignores empty lines)</li>
            <li>RFC4180 CSV</li>
            <li>Microsoft Excel CSV</li>
            <li>MySQL CSV</li>
            <li>Tab-separated values</li>
          </ul></dd>
    <dd>When a record exceeds the maximum record length defined for the origin, Kafka Consumer
     processes the object based on the error handling configured for the origin. </dd>
    <dd>The resulting record is a List with header and value maps. the origin can convert a file to
          a map based on the header line of the file. If the file does not include a header, the
          origin creates the map based on column number.</dd>
        <dd>For more information about the delimited data record structure, see <xref
            href="../Pipeline_Design/DelimitedDataRecordStructure.dita#concept_zcg_bm4_fs"
          />.</dd>
   </dlentry>
   <dlentry>
    <dt>XML</dt>
    <dd>Generates records based on the location of the XML element that you define as the record
     delimiter. If you do not define a delimiter element, Kafka Consumer treats the XML file as a
     single record.</dd>
    <dd>When a record exceeds the maximum record length defined for the origin, the origin skips the
          record and continues processing with the next record. It sends the skipped record to the
          pipeline for error handling.</dd>
   </dlentry>
   <dlentry>
    <dt>SDC Records</dt>
    <dd>Generates a record for every record. Use to process records generated by a <ph
      conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> pipeline
     using the SDC Record data format.</dd>
    <dd>For error records, Kafka Consumer provides the original record as read from the origin in
     the original pipeline, as well as error information that you can use to correct the record. </dd>
    <dd>When processing error records, the origin expects the error file names and contents as
          generated by the original pipeline.</dd>
   </dlentry>
   <dlentry>
        <dt>Log</dt>
        <dd>Generates a record for every log line. </dd>
        <dd>When a line exceeds the maximum line length defined for the origin, Kafka Consumer
          truncates longer lines. </dd>
        <dd>You can include the processed log line as a field in the record. If the log line is
          truncated, and you request the log line in the record, the origin includes the truncated
          line.</dd>
        <dd>You can define the log format or type to be read.</dd>
      </dlentry>
      <dlentry>
        <dt>Avro</dt>
        <dd>Generates a record for every message. </dd>
        <dd>To ensure proper data processing, indicate if the message includes an Avro schema. </dd>
        <dd>You can use the schema associated with the message or provide an alternate schema
          definition. Providing an alternate schema can improve performance.</dd>
      </dlentry>
  </dl>
 </conbody>
</concept>
