<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_xgs_nlc_wr">
 <title>Data Formats</title>
 <conbody>
  <p><indexterm>data formats<indexterm>for Kafka Consumer</indexterm></indexterm><indexterm>Kafka
        Consumer origin<indexterm>data formats</indexterm></indexterm>The Kafka Consumer origin
      processes data differently based on the type of data in the file. Kafka Consumer can process
      the following types of data:<draft-comment author="Loretta">Doc reminder: Updates to this
        section should be ported over to Directory-Data Formats - the origins handle data formats
        similarly. 4/28/15</draft-comment></p>
  <dl>
   <dlentry>
    <dt>Text</dt>
    <dd>Generates a record for each line of text. </dd>
    <dd>When a line exceeds the maximum line length defined for the origin, Kafka Consumer truncates
     the line.</dd>
   </dlentry>
   <dlentry>
    <dt>JSON</dt>
    <dd>Generates a record for each JSON object. You can use JSON files that include multiple JSON
     objects or a single JSON array.</dd>
    <dd>When an object exceeds the maximum object length defined for the origin, Kafka Consumer
     processes the object based on the error handling configured for the origin. </dd>
   </dlentry>
   <dlentry>
    <dt>Delimited</dt>
    <dd>Generates a record for each delimited line. You can use the following delimited format
            types:<ul id="ul_a51_wzk_5q">
            <li>Default CSV (ignores empty lines)</li>
            <li>RFC4180 CSV</li>
            <li>Microsoft Excel CSV</li>
            <li>MySQL CSV</li>
            <li>Tab-separated values</li>
          </ul></dd>
    <dd>When a record exceeds the maximum record length defined for the origin, Kafka Consumer
     processes the object based on the error handling configured for the origin. </dd>
    <dd>The resulting record is a List with header and value maps. Kafka Consumer can convert a file
          to a map based on the header line of the file. If the file does not include a header,
          Kafka Consumer creates the map based on column number.</dd>
        <dd>For more information about the delimited data record structure, see <xref
            href="../Pipeline_Configuration/DelimitedDataRecordStructure.dita#concept_zcg_bm4_fs"
          />.</dd>
   </dlentry>
   <dlentry>
    <dt>XML</dt>
    <dd>Generates records based on the location of the XML element that you define as the record
     delimiter. If you do not define a delimiter element, Kafka Consumer treats the XML file as a
     single record.</dd>
    <dd>When a record exceeds the maximum record length defined for the origin, Kafka Consumer skips
     the record and continues processing with the next record. It sends the skipped record to the
     pipeline for error handling.</dd>
   </dlentry>
   <dlentry>
    <dt>SDC Records</dt>
    <dd>Generates a record for every record. Use to process records generated by a <ph
      conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/> pipeline
     using the SDC Record data format.</dd>
    <dd>For error records, Kafka Consumer provides the original record as read from the origin in
     the original pipeline, as well as error information that you can use to correct the record. </dd>
    <dd>When processing error records, Kafka Consumer expects the error file names and contents as
     generated by the original pipeline.</dd>
   </dlentry>
   <dlentry>
        <dt>Log</dt>
        <dd>Generates a record for every log line. </dd>
        <dd>When a line exceeds the maximum line length defined for the origin, Kafka Consumer
          truncates longer lines. </dd>
        <dd>You can include the processed log line as a field in the record. If the log line is
          truncated, and you request the log line in the record, Kafka Consumer includes the
          truncated line.</dd>
        <dd>You can define the log format or type to be read.</dd>
      </dlentry>
      <dlentry>
        <dt>Avro</dt>
        <dd>Generates a record for every message. </dd>
        <dd>To ensure proper data processing, you must indicate if the message includes an Avro
          schema. </dd>
        <dd>You use schema associated with the message or provide an alternate schema definition to
          use. Providing an alternate schema can improve performance.</dd>
      </dlentry>
  </dl>
 </conbody>
</concept>
