<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_hpr_twm_jq">
 <title>Origins</title>
 <shortdesc>An origin stage represents the source for the pipeline. You can use a single origin
    stage in a pipeline.</shortdesc>
 <conbody>
  <p><indexterm>origins<indexterm>overview</indexterm></indexterm>You can use different origins
      based on the execution mode of the pipeline. </p>
    <p>In standalone pipelines, you can use the following origins: <ul id="ul_mxz_jxm_jq">
        <li>Directory - Reads fully-written files from a directory. </li>
        <li>File Tail - Reads lines of data from an active file after reading related archived files
          in the directory. </li>
        <li>HTTP Client - Reads data from a streaming HTTP resource URL.</li>
        <li>JDBC Consumer - Reads database data through a JDBC connection.</li>
        <li>JMS Consumer - Reads messages from JMS. </li>
        <li>Kafka Consumer - Reads messages from Kafka.</li>
        <li>Kinesis Consumer - Reads data from Kinesis.</li>
        <li>MongoDB - Reads documents from MongoDB.</li>
        <li>Omniture - Reads web usage reports from the Omniture reporting API.</li>
        <li>RPC - Reads data from an RPC destination in an RPC origin pipeline.</li>
        <li>UDP Source - Reads messages from one or more UDP ports. </li>
      </ul></p>
    <p>In cluster pipelines, you can use the following origins:<ul id="ul_unr_xhb_ws">
        <li>Hadoop FS - Reads data from the Hadoop Distributed File System (HDFS). </li>
        <li>Kafka Consumer - Reads messages from Kafka. Use the cluster version of the origin.</li>
      </ul></p>
 </conbody>
</concept>
