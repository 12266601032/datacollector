<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_gz5_dqw_yq">
 <title>Data Formats</title>
 <conbody>
    <p><indexterm>data formats<indexterm>Directory</indexterm></indexterm><indexterm>Directory
          origin<indexterm>data formats</indexterm></indexterm>The Directory origin processes data
      differently based on the data format. Directory processes the following types of data:</p>
    <draft-comment author="Loretta">Doc reminder: Updates to this section should be ported over to
      KConsumer-Data Formats and Hadoop FS-Data Formats- the origins handle data formats the same
      way. 4/28/15 -- except for Avro - that's different on purpose. 6/18-15.</draft-comment>
  <dl>
   <dlentry>
    <dt>Text</dt>
    <dd>Generates a record for each line in the file. </dd>
    <dd>When a line exceeds the maximum line length defined for the origin, the origin truncates the
          line. The origin adds a boolean field named Truncated to indicate if the line was
          truncated.</dd>
   </dlentry>
   <dlentry>
    <dt>JSON</dt>
    <dd>Generates a record for each JSON object. You can use JSON files that include multiple JSON
          objects or a single JSON array.</dd>
    <dd>When an object exceeds the maximum object length defined for the origin, the origin
          processes the object based on the error handling configured for the origin. </dd>
   </dlentry>
   <dlentry>
    <dt>Delimited</dt>
    <dd>Generates a record for each delimited line. You can use the following delimited format
            types:<ul
            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/ul_delFileTypes"
            id="ul_usk_hlf_qs">
            <li/>
          </ul></dd>
        <dd>When a record exceeds the maximum record length defined for the origin, Directory
          processes the object based on the error handling configured for the origin. </dd>
    <dd>The resulting record is a List with header and value maps. Directory can convert a file to a
          map based on the header line of the file. If the file does not include a header, the
          origin creates the map based on column number.</dd>
        <dd>For more information about the delimited data record structure, see <xref
            href="../Pipeline_Design/DelimitedDataRecordStructure.dita#concept_zcg_bm4_fs"
          />.</dd>
   </dlentry>
   <dlentry>
    <dt>XML</dt>
    <dd>Generates records based on the location of the XML element that you define as the record
          delimiter. If you do not define a delimiter element, Directory treats the XML file as a
          single record.</dd>
        <dd>When a record exceeds the maximum record length defined for the origin, the origin skips
          the record and continues processing with the next record. It sends the skipped record to
          the pipeline for error handling.</dd>
   </dlentry>
      <dlentry>
        <dt>SDC Records</dt>
        <dd>Generates a record for every record. Use to process records generated by a <ph
            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"/>
          pipeline using the SDC Record data format.</dd>
        <dd>For error records, Directory provides the original record as read from the origin in the
          original pipeline, as well as error information that you can use to correct the record. </dd>
        <dd>When processing error records, the origin expects the error file names and contents as
          generated by the original pipeline.</dd>
      </dlentry>
      <dlentry>
        <dt>Log</dt>
        <dd>Generates a record for every log line. </dd>
        <dd>When a line exceeds the maximum line length defined for the origin, Directory truncates
          longer lines. </dd>
        <dd>You can include the processed log line as a field in the record. If the log line is
          truncated, and you request the log line in the record, the origin includes the truncated
          line.</dd>
        <dd>You can define the log format or type to be read.</dd>
      </dlentry>
      <dlentry>
        <dt>Avro</dt>
        <dd>Generates a record for every record. </dd>
        <dd>Directory assumes each file includes an Avro schema definition. But you can optionally
          provide an alternate schema. </dd>
      </dlentry>
  </dl>
 </conbody>
</concept>
