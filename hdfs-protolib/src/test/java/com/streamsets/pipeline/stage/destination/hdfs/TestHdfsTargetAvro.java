/**
 * (c) 2015 StreamSets, Inc. All rights reserved. May not
 * be copied, modified, or distributed in whole or part without
 * written consent of StreamSets, Inc.
 */
package com.streamsets.pipeline.stage.destination.hdfs;

import com.streamsets.pipeline.api.OnRecordError;
import com.streamsets.pipeline.api.Record;
import com.streamsets.pipeline.config.DataFormat;
import com.streamsets.pipeline.lib.util.SdcAvroTestUtil;
import com.streamsets.pipeline.sdk.TargetRunner;
import org.apache.avro.Schema;
import org.apache.avro.file.DataFileReader;
import org.apache.avro.generic.GenericDatumReader;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.DatumReader;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

import java.io.File;
import java.io.FilenameFilter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.UUID;

public class TestHdfsTargetAvro {

  private static String testDir;

  @Before
  public void setUpClass() {
    File dir = new File("target", UUID.randomUUID().toString()).getAbsoluteFile();
    Assert.assertTrue(dir.mkdirs());
    testDir = dir.getAbsolutePath();
  }
  private String getTestDir() {
    return testDir;
  }

  @Test
  public void testHdfsTargetAvro() throws Exception {

    String dirPathTemplate = getTestDir() + "/hdfs/";

    TargetRunner runner = new TargetRunner.Builder(HdfsDTarget.class)
      .setOnRecordError(OnRecordError.STOP_PIPELINE)
      .addConfiguration("hdfsUri", "file:///")
      .addConfiguration("hdfsKerberos", false)
      .addConfiguration("hdfsConfDir", null)
      .addConfiguration("hdfsConfigs", new HashMap<>())
      .addConfiguration("uniquePrefix", "foo")
      .addConfiguration("dirPathTemplate", dirPathTemplate)
      .addConfiguration("timeZoneID", "UTC")
      .addConfiguration("fileType", HdfsFileType.TEXT)
      .addConfiguration("keyEl", "${uuid()}")
      .addConfiguration("compression", CompressionMode.NONE)
      .addConfiguration("seqFileCompressionType", HdfsSequenceFileCompressionType.BLOCK)
      .addConfiguration("maxRecordsPerFile", 3)
      .addConfiguration("maxFileSize", 0)
      .addConfiguration("timeDriver", "${time:now()}")
      .addConfiguration("lateRecordsLimit", "${30 * MINUTES}")
      .addConfiguration("lateRecordsAction", LateRecordsAction.SEND_TO_ERROR)
      .addConfiguration("lateRecordsDirPathTemplate", "")
      .addConfiguration("dataFormat", DataFormat.AVRO)
      .addConfiguration("csvFileFormat", null)
      .addConfiguration("csvReplaceNewLines", false)
      .addConfiguration("charset", "UTF-8")
      .addConfiguration("avroSchema", SdcAvroTestUtil.AVRO_SCHEMA1)
      .build();
    runner.runInit();

    List<Record> records = SdcAvroTestUtil.getRecords1();

    runner.runWrite(records);
    runner.runDestroy();

    //read the avro data file generated by and verify the records
    File dir = new File(dirPathTemplate);
    Assert.assertTrue(dir.exists());
    Assert.assertTrue(dir.isDirectory());

    String[] files = dir.list(new FilenameFilter() {
      @Override
      public boolean accept(File dir, String name) {
        if(name.startsWith("foo")) {
          return true;
        }
        return false;
      }
    });
    Assert.assertEquals(1, files.length);

    File avroDataFile = new File(dirPathTemplate, files[0]);
    Schema schema = new Schema.Parser().parse(SdcAvroTestUtil.AVRO_SCHEMA1);
    DatumReader<GenericRecord> datumReader = new GenericDatumReader<>(schema);
    DataFileReader<GenericRecord> dataFileReader = new DataFileReader<>(avroDataFile, datumReader);
    int actualRecordCount = 0;
    List<GenericRecord> genericRecords = new ArrayList<>();
    while(dataFileReader.hasNext()) {
      genericRecords.add(dataFileReader.next());
      actualRecordCount++;
    }
    Assert.assertEquals(3, actualRecordCount);

    SdcAvroTestUtil.compare1(genericRecords);

  }

}
